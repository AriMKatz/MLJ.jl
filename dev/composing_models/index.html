<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Composing Models · MLJ</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MLJ</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Getting Started</a></li><li><a class="toctext" href="../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="toctext" href="../model_search/">Model Search</a></li><li><a class="toctext" href="../machines/">Machines</a></li><li><a class="toctext" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="toctext" href="../performance_measures/">Performance Measures</a></li><li><a class="toctext" href="../tuning_models/">Tuning Models</a></li><li><a class="toctext" href="../built_in_transformers/">Built-in Transformers</a></li><li class="current"><a class="toctext" href>Composing Models</a><ul class="internal"><li><a class="toctext" href="#Linear-pipelines-1">Linear pipelines</a></li><li><a class="toctext" href="#Homogeneous-Ensembles-1">Homogeneous Ensembles</a></li><li><a class="toctext" href="#Learning-Networks-1">Learning Networks</a></li><li><a class="toctext" href="#Exporting-a-learning-network-as-a-stand-alone-model-1">Exporting a learning network as a stand-alone model</a></li><li><a class="toctext" href="#Static-operations-on-nodes-1">Static operations on nodes</a></li><li><a class="toctext" href="#The-learning-network-API-1">The learning network API</a></li></ul></li><li><a class="toctext" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="toctext" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="toctext" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="toctext" href="../benchmarking/">Benchmarking</a></li><li><a class="toctext" href="../working_with_tasks/">Working with Tasks</a></li><li><a class="toctext" href="../internals/">Internals</a></li><li><a class="toctext" href="../glossary/">Glossary</a></li><li><a class="toctext" href="../api/">API</a></li><li><a class="toctext" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="toctext" href="../NEWS/">MLJ News</a></li><li><a class="toctext" href="../frequently_asked_questions/">FAQ</a></li><li><a class="toctext" href="../julia_blogpost/">Julia BlogPost</a></li><li><a class="toctext" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Composing Models</a></li></ul><a class="edit-page" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/composing_models.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Composing Models</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Composing-Models-1" href="#Composing-Models-1">Composing Models</a></h1><p>MLJ has a flexible interface for composing multiple machine learning elements to form a <em>learning network</em>, whose complexity can extend beyond the &quot;pipelines&quot; of other machine learning toolboxes. However, MLJ does provide special syntax for common use cases, which are described first below. A description of the general framework begins at <a href="#Learning-Networks-1">Learning Networks</a>.</p><h2><a class="nav-anchor" id="Linear-pipelines-1" href="#Linear-pipelines-1">Linear pipelines</a></h2><p>In MLJ a <em>pipeline</em> is a composite model in which models are chained together in a linear (non-branching) chain. Pipelines can include learned or static target transformations, if one of the models is supervised.</p><p>To illustrate basic construction of a pipeline, consider the following toy data:</p><div></div><pre><code class="language-julia">using MLJ
X = (age    = [23, 45, 34, 25, 67],
     gender = categorical([&#39;m&#39;, &#39;m&#39;, &#39;f&#39;, &#39;m&#39;, &#39;f&#39;]));
height = [67.0, 81.5, 55.6, 90.0, 61.1];</code></pre><pre><code class="language-none">5-element Array{Float64,1}:
 67.0
 81.5
 55.6
 90.0
 61.1</code></pre><p>The code below creates a new pipeline model type called <code>MyPipe</code> for performing the following operations:</p><ul><li>standardize the target variable <code>:height</code> to have mean zero and standard deviation one</li><li>coerce the <code>:age</code> field to have <code>Continuous</code> scitype</li><li>one-hot encode the categorical feature <code>:gender</code></li><li>train a K-nearest neighbor model on the transformed inputs and transformed target</li><li>restore the predictions of the KNN model to the original <code>:height</code> scale (i.e., invert the standardization)</li></ul><p>The code also creates an instance of the new pipeline model type, called <code>pipe</code>, whose hyperparameters <code>hot</code>, <code>knn</code>, and <code>stand</code> are the component model instances specified in the macro expression:</p><pre><code class="language-julia">julia&gt; pipe = @pipeline MyPipe(X -&gt; coerce(X, :age=&gt;Continuous),
                               hot = OneHotEncoder(),
                               knn = KNNRegressor(K=3),
                               target = UnivariateStandardizer())

MyPipe(hot = OneHotEncoder(features = Symbol[],
                           drop_last = false,
                           ordered_factor = true,),
       knn = KNNRegressor(K = 3,
                          metric = MLJModels.KNN.euclidean,
                          kernel = MLJModels.KNN.reciprocal,),
       target = UnivariateStandardizer(),) @ 1…39</code></pre><p>We can, for example, evaluate the pipeline like we would any other model:</p><pre><code class="language-julia">julia&gt; pipe.knn.K = 2
julia&gt; pipe.hot.drop_last = true
julia&gt; evaluate(pipe, X, height, resampling=Holdout(), measure=rms, verbosity=2)

[ Info: Training Machine{MyPipe} @ 4…44.
[ Info: Training NodalMachine{OneHotEncoder} @ 1…16.
[ Info: Spawning 1 sub-features to one-hot encode feature :gender.
[ Info: Training NodalMachine{UnivariateStandardizer} @ 5…65.
[ Info: Training NodalMachine{KNNRegressor} @ 1…49.
(measure = MLJBase.RMS[rms],
 measurement = [10.0336],
 per_fold = Array{Float64,1}[[10.0336]],
 per_observation = Missing[missing],)</code></pre><p>For important details on including target transformations, see below.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.@pipeline" href="#MLJ.@pipeline"><code>MLJ.@pipeline</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-julia">@pipeline NewPipeType(fld1=model1, fld2=model2, ...)
@pipeline NewPipeType(fld1=model1, fld2=model2, ...) is_probabilistic=false</code></pre><p>Create a new pipeline model type <code>NewPipeType</code> that composes the types of the specified models <code>model1</code>, <code>model2</code>, ... . The models are composed in the specified order, meaning the input(s) of the pipeline goes to <code>model1</code>, whose output is sent to <code>model2</code>, and so forth. </p><p>At most one of the models may be a supervised model, in which case <code>NewPipeType</code> is supervised. Otherwise it is unsupervised.</p><p>The new model type <code>NewPipeType</code> has hyperparameters (fields) named <code>:fld1</code>, <code>:fld2</code>, ..., whose default values for an automatically generated keyword constructor are deep copies of <code>model1</code>, <code>model2</code>, ... .</p><p><em>Important.</em> If the overall pipeline is supervised and makes probabilistic predictions, then one must declare <code>is_probabilistic=true</code>. In the deterministic case the keyword argument can be omitted.</p><p>Static (unlearned) transformations - that is, ordinary functions - may also be inserted in the pipeline as shown in the following example (the classifier is probabilistic but the pipeline itself is deterministic):</p><pre><code class="language-none">@pipeline MyPipe(X -&gt; coerce(X, :age=&gt;Continuous), 
                 hot=OneHotEncoder(),
                 cnst=ConstantClassifier(),
                 yhat -&gt; mode.(yhat))</code></pre><p><strong>Return value</strong></p><p>An instance of the new type, with default hyperparameters (see above), is returned.</p><p><strong>Target transformation and inverse transformation</strong></p><p>A learned target transformation (such as standardization) can also be specified, using the keyword <code>target</code>, provided the transformer provides an <code>inverse_transform</code> method:</p><pre><code class="language-none">@load KNNRegressor
@pipeline MyPipe(hot=OneHotEncoder(), 
                 knn=KNNRegressor(),
                 target=UnivariateTransformer())</code></pre><p>A static transformation can be specified instead, but then an <code>inverse</code> must also be given:</p><pre><code class="language-none">@load KNNRegressor
@pipeline MyPipe(hot=OneHotEncoder(),
                 knn=KNNRegressor(),
                 target = v -&gt; log.(v),
                 inverse = v -&gt; exp.(v))</code></pre><p><em>Important.</em> While the supervised model in a pipeline providing a  target transformation can appear anywhere in the pipeline (as in  <code>ConstantClassifier</code> example above), the inverse operation is always  performed on the output of the <em>final</em> model or static  transformation in the pipeline.</p><p>See also: <a href="@ref">@from_network</a></p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/23bb4b5baf4089878267c11f6be2384c54acf037/src/pipelines.jl#L227-L291">source</a></section><h2><a class="nav-anchor" id="Homogeneous-Ensembles-1" href="#Homogeneous-Ensembles-1">Homogeneous Ensembles</a></h2><p>For performance reasons, creating a large ensemble of models sharing a common set of hyperparameters is achieved in MLJ through a model wrapper, rather than through the learning networks API. See the separate <a href="../homogeneous_ensembles/">Homogeneous Ensembles</a> section for details.</p><h2><a class="nav-anchor" id="Learning-Networks-1" href="#Learning-Networks-1">Learning Networks</a></h2><p>Hand-crafting a learning network, as outlined below, is a relatively advanced MLJ feature, assuming familiarity with the basics outlined in <a href="../">Getting Started</a>. The syntax for building a learning network is essentially an extension of the basic syntax but with data containers replaced with nodes (&quot;dynamic data&quot;).</p><p>In MLJ, a <em>learning network</em> is a directed acyclic graph whose nodes apply an operation, such as <code>predict</code> or <code>transform</code>, using a fixed machine (requiring training) - or which, alternatively, applies a regular (untrained) mathematical operation, such as <code>+</code>, <code>log</code> or <code>vcat</code>, to its input(s). In practice, a learning network works with fixed sources for its training/evaluation data, but can be built and tested in stages. By contrast, an <em>exported learning network</em> is a learning network exported as a stand-alone, re-usable <code>Model</code> object, to which all the MLJ <code>Model</code> meta-algorithms can be applied (ensembling, systematic tuning, etc).</p><p>As we shall see, exporting a learning network as a reusable model, is quite simple. While one can entirely skip the build-and-train steps, experimenting with raw learning networks may be the best way to understand how the stand-alone models work under the hood.</p><p>In MLJ learning networks treat the flow of information during training and predicting separately. Also, different nodes may use the same paramaters (fitresult) learned during the training of some model (that is, point to a common <em>nodal machine</em>; see below). For these reasons, simple examples may appear more slightly more complicated than in other frameworks. However, in more sophisticated applications, the extra flexibility is essential.</p><h3><a class="nav-anchor" id="Building-a-simple-learning-network-1" href="#Building-a-simple-learning-network-1">Building a simple learning network</a></h3><p><img src="../wrapped_ridge.png" alt/></p><p>The diagram above depicts a learning network which standardizes the input data <code>X</code>, learns an optimal Box-Cox transformation for the target <code>y</code>, predicts new target values using ridge regression, and then inverse-transforms those predictions, for later comparison with the original test data. The machines are labeled in yellow.</p><p>For testing purposes, we&#39;ll use a small synthetic data set:</p><pre><code class="language-julia">using Statistics, DataFrames

@load RidgeRegressor pkg=MultivariateStats

x1 = rand(300)
x2 = rand(300)
x3 = rand(300)
y = exp.(x1 - x2 -2x3 + 0.1*rand(300))
X = DataFrame(x1=x1, x2=x2, x3=x3)

Xs = source(X)
ys = source(y, kind=:target)</code></pre><pre><code class="language-julia">Source @ 3…40</code></pre><p><em>Note.</em> Once can wrap the source nodes around <code>nothing</code> instead of actual data. One can still export the resulting network as a stand-alone model (see later) but will be unable to fit or call on network nodes as described below.</p><p>We label nodes we will construct according to their outputs in the diagram. Notice that the nodes <code>z</code> and <code>yhat</code> use the same machine, namely <code>box</code>, for different operations.</p><p>To construct the <code>W</code> node we first need to define the machine <code>stand</code> that it will use to transform inputs.</p><pre><code class="language-julia">stand_model = Standardizer()
stand = machine(stand_model, Xs)</code></pre><pre><code class="language-julia">NodalMachine @ 6…82 = machine(Standardizer{} @ 1…82, 3…40)</code></pre><p>Because <code>Xs</code> is a node, instead of concrete data, we can call <code>transform</code> on the machine without first training it, and the result is the new node <code>W</code>, instead of concrete transformed data:</p><pre><code class="language-julia">W = transform(stand, Xs)</code></pre><pre><code class="language-julia">Node @ 1…67 = transform(6…82, 3…40)</code></pre><p>To get actual transformed data we <em>call</em> the node appropriately, which will require we first train the node. Training a node, rather than a machine, triggers training of <em>all</em> necessary machines in the network.</p><pre><code class="language-julia">test, train = partition(eachindex(y), 0.8)
fit!(W, rows=train)
W()           # transform all data
W(rows=test ) # transform only test data
W(X[3:4,:])   # transform any data, new or old</code></pre><pre><code class="language-julia">2×3 DataFrame
│ Row │ x1        │ x2       │ x3        │
│     │ Float64   │ Float64  │ Float64   │
├─────┼───────────┼──────────┼───────────┤
│ 1   │ -0.516373 │ 0.675257 │ 1.27734   │
│ 2   │ 0.63249   │ -1.70306 │ 0.0479891 │</code></pre><p>If you like, you can think of <code>W</code> (and the other nodes we will define) as &quot;dynamic data&quot;: <code>W</code> is <em>data</em>, in the sense that it an be called (&quot;indexed&quot;) on rows, but <em>dynamic</em>, in the sense the result depends on the outcome of training events.</p><p>The other nodes of our network are defined similarly:</p><pre><code class="language-julia">box_model = UnivariateBoxCoxTransformer()  # for making data look normally-distributed
box = machine(box_model, ys)
z = transform(box, ys)

ridge_model = RidgeRegressor(lambda=0.1)
ridge =machine(ridge_model, W, z)
zhat = predict(ridge, W)

yhat = inverse_transform(box, zhat)</code></pre><pre><code class="language-julia">Node @ 1…07 = inverse_transform(1…09, predict(2…66, transform(6…82, 3…40)))</code></pre><p>We are ready to train and evaluate the completed network. Notice that the standardizer, <code>stand</code>, is <em>not</em> retrained, as MLJ remembers that it was trained earlier:</p><pre><code class="language-julia">fit!(yhat, rows=train)</code></pre><pre><code class="language-julia">[ Info: Not retraining NodalMachine{Standardizer} @ 6…82. It is up-to-date.
[ Info: Training NodalMachine{UnivariateBoxCoxTransformer} @ 1…09.
[ Info: Training NodalMachine{RidgeRegressor} @ 2…66.
Node @ 1…07 = inverse_transform(1…09, predict(2…66, transform(6…82, 3…40)))</code></pre><pre><code class="language-julia">rms(y[test], yhat(rows=test)) # evaluate</code></pre><pre><code class="language-julia">0.022837595088079567</code></pre><p>We can change a hyperparameters and retrain:</p><pre><code class="language-julia">ridge_model.lambda = 0.01
fit!(yhat, rows=train)</code></pre><pre><code class="language-julia">[ Info: Not retraining NodalMachine{UnivariateBoxCoxTransformer} @ 1…09. It is up-to-date.
[ Info: Not retraining NodalMachine{Standardizer} @ 6…82. It is up-to-date.
[ Info: Updating NodalMachine{RidgeRegressor} @ 2…66.
Node @ 1…07 = inverse_transform(1…09, predict(2…66, transform(6…82, 3…40)))</code></pre><p>And re-evaluate:</p><pre><code class="language-julia">rms(y[test], yhat(rows=test))
0.039410306910269116</code></pre><blockquote><p><strong>Notable feature.</strong> The machine, <code>ridge::NodalMachine{RidgeRegressor}</code>, is retrained, because its underlying model has been mutated. However, since the outcome of this training has no effect on the training inputs of the machines <code>stand</code> and <code>box</code>, these transformers are left untouched. (During construction, each node and machine in a learning network determines and records all machines on which it depends.) This behavior, which extends to exported learning networks, means we can tune our wrapped regressor (using a holdout set) without re-computing transformations each time the hyperparameter is changed.</p></blockquote><h2><a class="nav-anchor" id="Exporting-a-learning-network-as-a-stand-alone-model-1" href="#Exporting-a-learning-network-as-a-stand-alone-model-1">Exporting a learning network as a stand-alone model</a></h2><p>Having satisfied that our learning network works on the synthetic data, we are ready to export it as a stand-alone model.</p><h3><a class="nav-anchor" id="Method-I:-The-@from_network-macro-1" href="#Method-I:-The-@from_network-macro-1">Method I: The @from_network  macro</a></h3><p>The following call simultaneously defines a new model subtype <code>WrappedRidgeI &lt;: Supervised</code> and creates an instance of this type <code>wrapped_modelI</code>:</p><pre><code class="language-julia">wrapped_ridgeI = @from_network WrappedRidgeI(ridge=ridge_model) &lt;= yhat</code></pre><p>Any MLJ work-flow can be applied to this composite model:</p><pre><code class="language-julia">julia&gt; params(wrapped_ridgeI)</code></pre><pre><code class="language-julia">(ridge = (lambda = 0.01,),)</code></pre><pre><code class="language-julia">X, y = @load_boston
evaluate(wrapped_ridgeI, X, y, resampling=CV(), measure=rms, verbosity=0)</code></pre><p><em>Notes:</em></p><ul><li><p>A deep copy of the original learning network <code>ridge_model</code> has become the default value for the field <code>ridge</code> of the new <code>WrappedRidgeI</code> struct.</p></li><li><p>It is important to have labeled the target source, as in <code>ys = source(y, kind=:target)</code>, to ensure the network is exported as a <em>supervised</em> model.</p></li><li><p>One can can also use the <code>@from_network</code> to export unsupervised learning networks and the syntax is the same. For example:</p></li></ul><pre><code class="language-julia">langs_composite = @from_network LangsComposite(pca=network_pca) &lt;= Xout</code></pre><ul><li>For a supervised network making <em>probabilistic</em> predictions, one must add</li></ul><p><code>is_probabilistic=true</code> to the end of the <code>@from network</code> call. For example:</p><pre><code class="language-julia">petes_composite = @from_network PetesComposite(tree_classifier=network_tree) probabilistic=true</code></pre><h3><a class="nav-anchor" id="Method-II:-Finer-control-1" href="#Method-II:-Finer-control-1">Method II: Finer control</a></h3><p>In Method I above, only models appearing in the network will appear as hyperparameters of the exported composite model. There is a second more flexible method for exporting the network, which allows finer control over the exported <code>Model</code> struct (see the example under <a href="#Static-operations-on-nodes-1">Static operations on nodes</a> below) and which also avoids macros. The two steps required are:</p><ul><li><p>Define a new <code>mutable struct</code> model type.</p></li><li><p>Wrap the learning network code in a model <code>fit</code> method.</p></li></ul><p>All learning networks that make deterministic (respectively, probabilistic) predictions export to models of subtype <code>DeterministicNetwork</code> (respectively, <code>ProbabilisticNetwork</code>), Unsupervised learning networks export to <code>UnsupervisedNetwork</code> model subtypes.</p><pre><code class="language-julia">mutable struct WrappedRidgeII &lt;: DeterministicNetwork
    ridge_model
end

# keyword constructor
WrappedRidgeII(; ridge=RidgeRegressor()) = WrappedRidgeII(ridge); </code></pre><p>We now simply cut and paste the code defining the learning network into a model <code>fit</code> method (as opposed to machine <code>fit!</code> methods, which internally dispatch model <code>fit</code> methods on the data bound to the machine):</p><pre><code class="language-julia">function MLJ.fit(model::WrappedRidgeII, verbosity::Integer, X, y)
    Xs = source(X)
    ys = source(y, kind=:target)

    stand_model = Standardizer()
    stand = machine(stand_model, Xs)
    W = transform(stand, Xs)

    box_model = UnivariateBoxCoxTransformer()  # for making data look normally-distributed
    box = machine(box_model, ys)
    z = transform(box, ys)

    ridge_model = model.ridge_model ###
    ridge =machine(ridge_model, W, z)
    zhat = predict(ridge, W)

    yhat = inverse_transform(box, zhat)
    fit!(yhat, verbosity=0)

    return fitresults(yhat)
end</code></pre><p>The line marked <code>###</code>, where the new exported model&#39;s hyperparameter <code>ridge</code> is spliced into the network, is the only modification. This completes the export process.</p><blockquote><p><strong>What&#39;s going on here?</strong> MLJ&#39;s machine interface is built atop a more primitive <em><a href="../simple_user_defined_models/">model</a></em> interface, implemented for each algorithm. Each supervised model type (eg, <code>RidgeRegressor</code>) requires model <code>fit</code> and <code>predict</code> methods, which are called by the corresponding <em>machine</em> <code>fit!</code> and <code>predict</code> methods. We don&#39;t need to define a  model <code>predict</code> method here because MLJ provides a fallback which simply calls the terminating node of the network built in <code>fit</code> on the data supplied. The expression <code>fitresults(yhat)</code> bundles the terminal node <code>yhat</code> with reports (one for each machine in the network) and moves training data out to a bundled cache object. This ensures machines wrapping exported model instances do not contain actual training data in their <code>fitresult</code> fields.  </p></blockquote><pre><code class="language-julia">X, y = @load_boston
wrapped_ridgeII = WrappedRidgeII()
evaluate(wrapped_ridgeII, X, y, resampling=CV(), measure=rms, verbosity=0)</code></pre><pre><code class="language-julia">6-element Array{Float64,1}:
 3.0225867093289347
 4.755707358891049
 5.011312664189936
 4.226827668908119
 8.93385968738185  
 3.4788524973220545</code></pre><p>Another example of an exported learning network is given in the next subsection.</p><h2><a class="nav-anchor" id="Static-operations-on-nodes-1" href="#Static-operations-on-nodes-1">Static operations on nodes</a></h2><p>Continuing to view nodes as &quot;dynamic data&quot;, we can, in addition to applying &quot;dynamic&quot; operations like <code>predict</code> and <code>transform</code> to nodes, overload ordinary &quot;static&quot; (unlearned) operations as well. Common operations, like addition, scalar multiplication, <code>exp</code>, <code>log</code>, <code>vcat</code>, <code>hcat</code>, tabularization (<code>MLJ.table</code>) and matrixification (<code>MLJ.matrix</code>) work out-of-the box.</p><p>As a demonstration, consider the code below defining a composite model <code>blended_model</code> (subtype of <code>KNNRidgeBlend</code>) that: (i) One-hot encodes the input table <code>X</code>; (ii) Log transforms the continuous target <code>y</code>; (iii) Fits specified K-nearest neighbour and ridge regressor models to the data; (iv) Computes a weighted average of individual model predictions; and (v) Inverse transforms (exponentiates) the blended predictions. We include the weighting as a hyperparameter of the new model, which would not be possible using the <code>@from_network</code> macro.</p><p>Note, in particular, the lines defining <code>zhat</code> and <code>yhat</code>, which combine several static node operations.</p><pre><code class="language-julia">
@load RidgeRegressor pkg=MultivariateStats

mutable struct KNNRidgeBlend &lt;:DeterministicNetwork

    knn_model
    ridge_model
    weights::Tuple{Float64, Float64}

end

function MLJ.fit(model::KNNRidgeBlend, verbosity::Integer, X, y)

    Xs = source(X)
    ys = source(y, kind=:target)

    hot = machine(OneHotEncoder(), Xs)

    # W, z, zhat and yhat are nodes in the network:

    W = transform(hot, Xs) # one-hot encode the input
    z = log(ys) # transform the target

    ridge_model = model.ridge_model
    knn_model = model.knn_model

    ridge = machine(ridge_model, W, z)
    knn = machine(knn_model, W, z)

    # average the predictions of the KNN and ridge models
    zhat = model.weights[1]*predict(ridge, W) + weights[2]*predict(knn, W)

    # inverse the target transformation
    yhat = exp(zhat)

    fit!(yhat, verbosity=0)

    return fitresults(Xs, ys, yhat)
end
</code></pre><pre><code class="language-julia">using CSV
X, y = load_reduced_ames()()
knn_model = KNNRegressor(K=2)
ridge_model = RidgeRegressor(lambda=0.1)
weights = (0.9, 0.1)
blended_model = KNNRidgeBlend(knn_model, ridge_model, weights)
evaluate(blended_model, X, y, resampling=Holdout(fraction_train=0.7), measure=rmsl)</code></pre><pre><code class="language-julia">julia&gt; evaluate!(mach, resampling=Holdout(fraction_train=0.7), measure=rmsl)
┌ Info: Evaluating using a holdout set.
│ fraction_train=0.7
│ shuffle=false
│ measure=MLJ.rmsl
│ operation=StatsBase.predict
└ Resampling from all rows.
mach = NodalMachine{OneHotEncoder} @ 1…14
mach = NodalMachine{RidgeRegressor} @ 1…87
mach = NodalMachine{KNNRegressor} @ 1…02
0.13108966715886725</code></pre><p>A <code>node</code> method allows us to overload a given function to node arguments.  Here are some examples taken from MLJ source (at work in the example above):</p><pre><code class="language-julia">Base.log(v::Vector{&lt;:Number}) = log.(v)
Base.log(X::AbstractNode) = node(log, X)

import Base.+
+(y1::AbstractNode, y2::AbstractNode) = node(+, y1, y2)
+(y1, y2::AbstractNode) = node(+, y1, y2)
+(y1::AbstractNode, y2) = node(+, y1, y2)</code></pre><p>Here <code>AbstractNode</code> is the common supertype of <code>Node</code> and <code>Source</code>.</p><p>As a final example, here&#39;s how to extend row shuffling to nodes:</p><pre><code class="language-julia">using Random
Random.shuffle(X::AbstractNode) = node(Y -&gt; MLJ.selectrows(Y, Random.shuffle(1:nrows(Y))), X)
X = (x1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
     x2 = [:one, :two, :three, :four, :five, :six, :seven, :eight, :nine, :ten])
Xs = source(X)
W = shuffle(Xs)</code></pre><pre><code class="language-julia">Node @ 9…86 = #4(6…62)</code></pre><pre><code class="language-julia">W()</code></pre><pre><code class="language-julia">(x1 = [1, 4, 3, 6, 8, 5, 7, 2, 9, 10],
 x2 = Symbol[:one, :four, :three, :six, :eight, :five, :seven, :two, :nine, :ten],)</code></pre><h2><a class="nav-anchor" id="The-learning-network-API-1" href="#The-learning-network-API-1">The learning network API</a></h2><p>Three julia types are part of learning networks: <code>Source</code>, <code>Node</code> and <code>NodalMachine</code>. A <code>NodalMachine</code> is returned by the <code>machine</code> constructor when given nodal arguments instead of concrete data.</p><p>The definitions of <code>Node</code> and <code>NodalMachine</code> are coupled because every <code>NodalMachine</code> has <code>Node</code> objects in its <code>args</code> field (the <em>training arguments</em> specified in the constructor) and every <code>Node</code> must specify a <code>NodalMachine</code>, unless it is static (see below).</p><p>Formally, a learning network defines <em>two</em> labeled directed acyclic graphs (DAG&#39;s) whose nodes are <code>Node</code> or <code>Source</code> objects, and whose labels are <code>NodalMachine</code> objects. We obtain the first DAG from directed edges of the form <span>$N1 -&gt; N2$</span> whenever <span>$N1$</span> is an <em>argument</em> of <span>$N2$</span> (see below). Only this DAG is relevant when calling a node, as discussed in examples above and below. To form the second DAG (relevant when calling or calling <code>fit!</code> on a node) one adds edges for which <span>$N1$</span> is <em>training argument</em> of the the machine which labels <span>$N1$</span>. We call the second, larger DAG, the <em>complete learning network</em> below (but note only edges of the smaller network are explicitly drawn in diagrams, for simplicity).</p><h3><a class="nav-anchor" id="Source-nodes-1" href="#Source-nodes-1">Source nodes</a></h3><p>Only source nodes reference concrete data. A <code>Source</code> object has a single field, <code>data</code>.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.source-Tuple{Any}" href="#MLJ.source-Tuple{Any}"><code>MLJ.source</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">Xs = source(X)
ys = source(y, kind=:target)
ws = source(w, kind=:weight)</code></pre><p>Defines, respectively, learning network <code>Source</code> objects for wrapping some input data <code>X</code> (<code>kind=:input</code>), some target data <code>y</code>, or some sample weights <code>w</code>.  The values of each variable <code>X, y, w</code> can be anything, even <code>nothing</code>, if the network is for exporting as a stand-alone model only. For training and testing the unexported network, appropriate vectors, tables, or other data containers are expected.</p><pre><code class="language-none">Xs = source()
ys = source(kind=:target)
ws = source(kind=:weight)</code></pre><p>Define source nodes wrapping <code>nothing</code> instead of concrete data. Such definitions suffice if a learning network is to be exported without testing.</p><p>The calling behaviour of a <code>Source</code> object is this:</p><pre><code class="language-none">Xs() = X
Xs(rows=r) = selectrows(X, r)  # eg, X[r,:] for a DataFrame
Xs(Xnew) = Xnew</code></pre><p>See also: [<code>@from_network</code>](@ref], <a href="#MLJ.sources"><code>sources</code></a>, <a href="#MLJ.origins"><code>origins</code></a>, <a href="#MLJ.node"><code>node</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/23bb4b5baf4089878267c11f6be2384c54acf037/src/networks.jl#L10-L39">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.rebind!" href="#MLJ.rebind!"><code>MLJ.rebind!</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">rebind!(s)</code></pre><p>Attach new data <code>X</code> to an existing source node <code>s</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/23bb4b5baf4089878267c11f6be2384c54acf037/src/networks.jl#L58-L62">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.sources" href="#MLJ.sources"><code>MLJ.sources</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">sources(W::AbstractNode; kind=:any)</code></pre><p>A vector of all sources referenced by calls <code>N()</code> and <code>fit!(N)</code>. These are the sources of the directed acyclic graph associated with the learning network terminating at <code>N</code>. The return value can be restricted further by specifying <code>kind=:input</code>, <code>kind=:target</code>, <code>kind=:weight</code>, etc.</p><p>Not to be confused with <code>origins(N)</code> which refers to the same graph with edges corresponding to training arguments deleted.</p><p>See also: <a href="#MLJ.origins"><code>origins</code></a>, <a href="#MLJ.source-Tuple{Any}"><code>source</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/23bb4b5baf4089878267c11f6be2384c54acf037/src/networks.jl#L531-L544">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.origins" href="#MLJ.origins"><code>MLJ.origins</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">origins(s)
</code></pre><p>Return a list of all origins of a node <code>N</code> accessed by a call <code>N()</code>. These are the source nodes of the acyclic directed graph (DAG) associated with the learning network terminating at <code>N</code>, if edges corresponding to training arguments are excluded. A <code>Node</code> object cannot be called on new data unless it has a unique origin.</p><p>Not to be confused with <code>sources(N)</code> which refers to the same graph but without the training edge deletions.</p><p>See also: <a href="#MLJ.node"><code>node</code></a>, <a href="#MLJ.source-Tuple{Any}"><code>source</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/23bb4b5baf4089878267c11f6be2384c54acf037/src/networks.jl#L69">source</a><div><div><pre><code class="language-julia">origins(X)
</code></pre><p>Access the origins (source nodes) of a given node.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/23bb4b5baf4089878267c11f6be2384c54acf037/src/networks.jl#L228">source</a></section><h3><a class="nav-anchor" id="Nodal-machines-1" href="#Nodal-machines-1">Nodal machines</a></h3><p>The key components of a <code>NodalMachine</code> object are:</p><ul><li><p>A <em>model</em>,  specifying a learning algorithm and hyperparameters.</p></li><li><p>Training <em>arguments</em>, which specify the nodes acting as proxies for training data on calls to <code>fit!</code>.</p></li><li><p>A <em>fitresult</em>, for storing the outcomes of calls to <code>fit!</code>.</p></li></ul><p>A nodal machine is trained in the same way as a regular machine with one difference: Instead of training the model on the wrapped data <em>indexed</em> on <code>rows</code>, it is trained on the wrapped nodes <em>called</em> on <code>rows</code>, with calling being a recursive operation on nodes within a learning network (see below).</p><h3><a class="nav-anchor" id="Nodes-1" href="#Nodes-1">Nodes</a></h3><p>The key components of a <code>Node</code> are:</p><ul><li><p>An <em>operation</em>, which will either be <em>static</em> (a fixed function) or <em>dynamic</em> (such as <code>predict</code> or <code>transform</code>, dispatched on a nodal machine <code>NodalMachine</code>).</p></li><li><p>A nodal <em>machine</em> on which to dispatch the operation (void if the operation is static).</p></li><li><p>Upstream connections to other nodes (including source nodes) specified by <em>arguments</em> (one for each argument of the operation).</p></li><li><p>A dependency <em>tape</em>, listing of all upstream nodes in the complete learning network, with an order consistent with the learning network as a DAG.</p></li></ul><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.node" href="#MLJ.node"><code>MLJ.node</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">N = node(f::Function, args...)</code></pre><p>Defines a <code>Node</code> object <code>N</code> wrapping a static operation <code>f</code> and arguments <code>args</code>. Each of the <code>n</code> elements of <code>args</code> must be a <code>Node</code> or <code>Source</code> object. The node <code>N</code> has the following calling behaviour:</p><pre><code class="language-none">N() = f(args[1](), args[2](), ..., args[n]())
N(rows=r) = f(args[1](rows=r), args[2](rows=r), ..., args[n](rows=r))
N(X) = f(args[1](X), args[2](X), ..., args[n](X))

J = node(f, mach::NodalMachine, args...)</code></pre><p>Defines a dynamic <code>Node</code> object <code>J</code> wrapping a dynamic operation <code>f</code> (<code>predict</code>, <code>predict_mean</code>, <code>transform</code>, etc), a nodal machine <code>mach</code> and arguments <code>args</code>. Its calling behaviour, which depends on the outcome of training <code>mach</code> (and, implicitly, on training outcomes affecting its arguments) is this:</p><pre><code class="language-none">J() = f(mach, args[1](), args[2](), ..., args[n]())
J(rows=r) = f(mach, args[1](rows=r), args[2](rows=r), ..., args[n](rows=r))
J(X) = f(mach, args[1](X), args[2](X), ..., args[n](X))</code></pre><p>Generally <code>n=1</code> or <code>n=2</code> in this latter case.</p><pre><code class="language-none">predict(mach, X::AbsractNode, y::AbstractNode)
predict_mean(mach, X::AbstractNode, y::AbstractNode)
predict_median(mach, X::AbstractNode, y::AbstractNode)
predict_mode(mach, X::AbstractNode, y::AbstractNode)
transform(mach, X::AbstractNode)
inverse_transform(mach, X::AbstractNode)</code></pre><p>Shortcuts for <code>J = node(predict, mach, X, y)</code>, etc.</p><p>Calling a node is a recursive operation which terminates in the call to a source node (or nodes). Calling nodes on <em>new</em> data <code>X</code> fails unless the number of such nodes is one.</p><p>See also: <a href="#MLJ.source-Tuple{Any}"><code>source</code></a>, <a href="#MLJ.origins"><code>origins</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/23bb4b5baf4089878267c11f6be2384c54acf037/src/networks.jl#L387-L427">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="StatsBase.fit!-Tuple{Node}" href="#StatsBase.fit!-Tuple{Node}"><code>StatsBase.fit!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">fit!(N::Node; rows=nothing, verbosity::Int=1, force::Bool=false)</code></pre><p>Train all machines in the learning network terminating at node <code>N</code>, in an appropriate order. These machines are those returned by <code>machines(N)</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/23bb4b5baf4089878267c11f6be2384c54acf037/src/networks.jl#L299-L305">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="StatsBase.fit!-Tuple{MLJ.AbstractMachine}" href="#StatsBase.fit!-Tuple{MLJ.AbstractMachine}"><code>StatsBase.fit!</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">fit!(mach::Machine; rows=nothing, verbosity=1, force=false)</code></pre><p>When called for the first time, call <code>MLJBase.fit</code> on <code>mach.model</code> and store the returned fit-result and report. Subsequent calls do nothing unless: (i) <code>force=true</code>, or (ii) the specified <code>rows</code> are different from those used the last time a fit-result was computed, or (iii) <code>mach.model</code> has changed since the last time a fit-result was computed (the machine is <em>stale</em>). In cases (i) or (ii) <code>MLJBase.fit</code> is called on <code>mach.model</code>. Otherwise, <code>MLJBase.update</code> is called.</p><pre><code class="language-none">fit!(mach::NodalMachine; rows=nothing, verbosity=1, force=false)</code></pre><p>When called for the first time, attempt to call <code>MLJBase.fit</code> on <code>fit.model</code>. This will fail if an argument of the machine depends ultimately on some other untrained machine for successful calling, but this is resolved by instead calling <code>fit!</code> on fitting any node <code>N</code> for which <code>mach in machines(N)</code> is true, which trains all necessary machines in an appropriate order. Subsequent <code>fit!</code> calls do nothing unless: (i) <code>force=true</code>, or (ii) some machine on which <code>mach</code> depends has computed a new fit-result since <code>mach</code> last computed its fit-result, or (iii) the specified <code>rows</code> have changed since the last time a fit-result was last computed, or (iv) <code>mach</code> is stale (see below). In cases (i), (ii) or (iii), <code>MLJBase.fit</code> is called. Otherwise <code>MLJBase.update</code> is called.</p><p>A machine <code>mach</code> is <em>stale</em> if <code>mach.model</code> has changed since the last time a fit-result was computed, or if if one of its training arguments is <code>stale</code>. A node <code>N</code> is stale if <code>N.machine</code> is stale or one of its arguments is stale. Source nodes are never stale.</p><p>Note that a nodal machine obtains its training data by <em>calling</em> its node arguments on the specified <code>rows</code> (rather than <em>indexing</em> its arguments on those rows) and that this calling is a recursive operation on nodes upstream of those arguments.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/23bb4b5baf4089878267c11f6be2384c54acf037/src/machines.jl#L73-L108">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.@from_network" href="#MLJ.@from_network"><code>MLJ.@from_network</code></a> — <span class="docstring-category">Macro</span>.</div><div><div><pre><code class="language-julia">@from_network(NewCompositeModel(fld1=model1, fld2=model2, ...) &lt;= N
@from_network(NewCompositeModel(fld1=model1, fld2=model2, ...) &lt;= N is_probabilistic=false</code></pre><p>Create a new stand-alone model type called <code>NewCompositeModel</code>, using a learning network as a blueprint. Here <code>N</code> refers to the terminal node of the learning network (from which final predictions or transformations are fetched). </p><p><em>Important.</em> If the learning network is supervised (has a source with <code>kind=:target</code>) and makes probabilistic predictions, then one must declare <code>is_probabilistic=true</code>. In the deterministic case the keyword argument can be omitted.</p><p>The model type <code>NewCompositeModel</code> is equipped with fields named <code>:fld1</code>, <code>:fld2</code>, ..., which correspond to component models <code>model1</code>, <code>model2</code>, ...,  appearing in the network (which must therefore be elements of <code>models(N)</code>).  Deep copies of the specified component models are used as default values in an automatically generated keyword constructor for <code>NewCompositeModel</code>.</p><p><strong>Return value</strong></p><p>A new <code>NewCompositeModel</code> instance, with default field values.</p><p>For details and examples refer to the &quot;Learning Networks&quot; section of the documentation.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/23bb4b5baf4089878267c11f6be2384c54acf037/src/composites.jl#L347-L375">source</a></section><footer><hr/><a class="previous" href="../built_in_transformers/"><span class="direction">Previous</span><span class="title">Built-in Transformers</span></a><a class="next" href="../homogeneous_ensembles/"><span class="direction">Next</span><span class="title">Homogeneous Ensembles</span></a></footer></article></body></html>
