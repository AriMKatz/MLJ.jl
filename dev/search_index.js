var documenterSearchIndex = {"docs":
[{"location":"#","page":"Getting Started","title":"Getting Started","text":"MLJ homepage","category":"page"},{"location":"#Getting-Started-1","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"#Basic-supervised-training-and-testing-1","page":"Getting Started","title":"Basic supervised training and testing","text":"","category":"section"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> using MLJ\njulia> using RDatasets\njulia> iris = dataset(\"datasets\", \"iris\"); # a DataFrame","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"In MLJ one can either wrap data for supervised learning in a formal task (see Working with Tasks), or work directly with the data, split into its input and target parts:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> const X = iris[:, 1:4];\njulia> const y = iris[:, 5];","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"A model is a container for hyperparameters. Assuming the DecisionTree package is in your installation load path, we can instantiate a DecisionTreeClassifier model like this:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> @load DecisionTreeClassifier\nimport MLJModels ✔\nimport DecisionTree ✔\nimport MLJModels.DecisionTree_.DecisionTreeClassifier ✔\n\njulia> tree_model = DecisionTreeClassifier(target_type=String, max_depth=2)\nDecisionTreeClassifier(target_type = String,\n                       pruning_purity = 1.0,\n                       max_depth = 2,\n                       min_samples_leaf = 1,\n                       min_samples_split = 2,\n                       min_purity_increase = 0.0,\n                       n_subfeatures = 0.0,\n                       display_depth = 5,\n                       post_prune = false,\n                       merge_purity_threshold = 0.9,) @ 1…72","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Wrapping the model in data creates a machine which will store training outcomes (called fit-results):","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> tree = machine(tree_model, X, y)\nMachine @ 5…78","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Training and testing on a hold-out set:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> train, test = partition(eachindex(y), 0.7, shuffle=true); # 70:30 split\njulia> fit!(tree, rows=train)\njulia> yhat = predict(tree, X[test,:]);\njulia> misclassification_rate(yhat, y[test]);\n\n┌ Info: Training Machine{DecisionTreeClassifier{S…} @ 1…36.\n└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:68\n0.08888888888888889","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Or, in one line:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> evaluate!(tree, resampling=Holdout(fraction_train=0.7, shuffle=true), measure=misclassification_rate)\n0.08888888888888889","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Changing a hyperparameter and re-evaluating:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> tree_model.max_depth = 3\njulia> evaluate!(tree, resampling=Holdout(fraction_train=0.5, shuffle=true), measure=misclassification_rate)\n0.06666666666666667","category":"page"},{"location":"#Next-steps-1","page":"Getting Started","title":"Next steps","text":"","category":"section"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"To learn a little more about what MLJ can do, take the MLJ tour. Read the remainder of this document before considering more serious use of MLJ.","category":"page"},{"location":"#Prerequisites-1","page":"Getting Started","title":"Prerequisites","text":"","category":"section"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"MLJ assumes some familiarity with the CategoricalArrays.jl package, used for representing categorical data. For probabilistic predictors, a basic acquaintance with Distributions.jl is also assumed.","category":"page"},{"location":"#Data-containers-and-scientific-types-1","page":"Getting Started","title":"Data containers and scientific types","text":"","category":"section"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"The MLJ user should acquaint themselves with some basic assumptions about the form of data expected by MLJ, as outlined below. ","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"In principle, anywhere a table is expected in MLJ - for example, X above - any tabular format supporting the Tables.jl interface is allowed. (At present our API is more restrictive; see this issue with Tables.jl. If your Tables.jl compatible format is not working in MLJ, please post an issue.) In particular, DataFrame, JuliaDB.IndexedTable and TypedTables.Table objects are supported, as are named tuples of equi-length vectors (\"column tables\" in Tables.jl parlance).","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"A single feature (such as the target y above) is expected to be a Vector or CategoricalVector, according to the scientific type of the data (see below). A multivariate target can be any table.","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"On the other hand, the element types you use to represent your data has implicit consequences about how MLJ will interpret that data.","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"WIP: Eventually users will use task constructors to coerce element types into the requisite form. At present, the user must do so manually, before passing data to the constructors.","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"To articulate MLJ's conventions about data representation, MLJ distinguishes between machine data types on the one hand (Float64, Bool, String, etc) and scientific data types on the other, represented by new Julia types: Continuous, Multiclass{N}, FiniteOrderedFactor{N}, and Count (unbounded ordered factor), with obvious interpretations. These types are organized in a type hierarchy rooted in a new abstract type Found:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"(Image: )","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Note that Multiclass{2} has the alias Binary.","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"A scientific type is any subtype of Union{Missing,Found}. Scientific types have no instances.","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Scientific types appear when querying model metadata, as in this example:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> info(\"DecisionTreeClassifier\")[:target_scitype]\n\nUnion{Multiclass,FiniteOrderedFactor}","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Basic data convention. The scientific type of data that a Julia object x can represent is defined by scitype(x). If scitype(x) == Other, then x cannot represent scalar data in MLJ.","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> (scitype(42), scitype(π), scitype(\"Julia\"))\n\n(Count, Continuous, MLJBase.Other)","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"In particular, integers cannot be used to represent Multiclass or FiniteOrderedFactor data; these can be represented by an unordered or ordered CategoricalValue or CategoricalString:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"T scitype(x) for x::T\nMissing Missing\nReal Continuous\nInteger Count\nCategoricalValue Multiclass{N} where N = nlevels(x), provided x.pool.ordered == false\nCategoricalString Multiclass{N} where N = nlevels(x), provided x.pool.ordered == false\nCategoricalValue FiniteOrderedFactor{N} where N = nlevels(x), provided x.pool.ordered == true\nCategoricalString FiniteOrderedFactor{N} where N = nlevels(x) provided x.pool.ordered == true\nInteger Count","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Here nlevels(x) = length(levels(x.pool)).","category":"page"},{"location":"working_with_tasks/#Working-with-Tasks-1","page":"Working with Tasks","title":"Working with Tasks","text":"","category":"section"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"In MLJ a task is a synthesis of three elements: data, an interpretation of that data, and a learning objective. Once one has a task one is ready to choose learning models.","category":"page"},{"location":"working_with_tasks/#Scientific-types-and-the-interpretation-of-data-1","page":"Working with Tasks","title":"Scientific types and the interpretation of data","text":"","category":"section"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Generally the columns of a table, such as a DataFrame, represents real quantities. However, the nature of a quantity is not always clear from the representation. For example, we might count phone calls using the UInt32 type but also use UInt32 to represent a categorical feature, such as the species of conifers. MLJ mitigates such ambiguity by: (i) distinguishing between the machine and scientific type of scalar data; (ii) disallowing the representation of multiple scientific types by the same machine type during learning; and (iii) establising a convention for what scientific types a given machine type may represent (see the table at the end of Getting Started).","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Explicitly specifying scientific types during the construction of a MLJ task is the user's opportunity to articulate how the supplied data should be interpreted.","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"WIP: At present scitypes cannot be specified and the user must manually coerce data before task construction. ","category":"page"},{"location":"working_with_tasks/#Learning-objectives-1","page":"Working with Tasks","title":"Learning objectives","text":"","category":"section"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"In MLJ specifying a learing objective means specifying: (i) whether learning is supervised or not; (ii) whether, in the supervised case, predictions are to be probabilistic or deterministic; and (iii) what part of the data is relevant and what role is each part to play.","category":"page"},{"location":"working_with_tasks/#Sample-usage-1","page":"Working with Tasks","title":"Sample usage","text":"","category":"section"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Load a built-in task:","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"using MLJ\ntask = load_iris()","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Extract input and target:","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"X, y = task()\nX[1:3, :]","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Constructing a task from data:","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"# reconstruct:\ndf = copy(X)\ndf.species = y\ntask = SupervisedTask(data=df, target=:species, is_probabilistic=true)\nshow(task, 1)","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"List models matching a task:","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"models(task)","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Row selection for a task:","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"nrows(task)","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"task[1:2].y","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Shuffle the rows of a task:","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"using Random\nrng = MersenneTwister(1234)\nshuffle!(rng, task) # rng is optional\ntask[1:2].y","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Binding a model to a task and evalutating performance:","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"@load DecisionTreeClassifier\nmach = machine(DecisionTreeClassifier(target_type=String), task)\nevaluate!(mach, operation=predict_mode, resampling=Holdout(), measure=misclassification_rate, verbosity=0)","category":"page"},{"location":"working_with_tasks/#API-Reference-1","page":"Working with Tasks","title":"API Reference","text":"","category":"section"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"UnsupervisedTask","category":"page"},{"location":"working_with_tasks/#MLJBase.UnsupervisedTask","page":"Working with Tasks","title":"MLJBase.UnsupervisedTask","text":"task = UnsupervisedTask(data=nothing, ignore=Symbol[], input_is_multivariate=true, verbosity=1)\n\nConstruct an unsupervised learning task with given input data, which should be a table or, in the case of univariate inputs, a single vector. \n\nRows of data must correspond to patterns and columns to features. Columns in data whose names appear in ignore are ignored.\n\nX = task()\n\nReturn the input data in form to be used in models.\n\n\n\n\n\n","category":"type"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"SupervisedTask","category":"page"},{"location":"working_with_tasks/#MLJBase.SupervisedTask","page":"Working with Tasks","title":"MLJBase.SupervisedTask","text":"task = SupervisedTask(X, y; is_probabilistic=nothing, input_is_multivariate=true, target_is_multivariate=false, verbosity=1)\n\nConstruct a supervised learning task with input features X and target y. Both X and y must be tables or vectors, according to whether they are multivariate or univariate. Table rows must correspond to patterns and columns to features. The boolean keyword argument is_probabilistic must be specified.\n\ntask = SupervisedTask(data=nothing, is_probabilistic=nothing, target=nothing, ignore=Symbol[], input_is_multivariate=true, verbosity)\n\nConstruct a supervised learning task with input features X and target y, where y is the column vector from data named target (if this is a single symbol) or, a table whose columns are those named in target (if this is vector); X consists of all remaining columns of data not named in ignore.\n\nX, y = task()\n\nReturns the input X and target y of the task, also available as task.X and task.y.\n\n\n\n\n\n","category":"type"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"models()","category":"page"},{"location":"working_with_tasks/#MLJ.models-Tuple{}","page":"Working with Tasks","title":"MLJ.models","text":"models(; show_dotted=false)\n\nList all models as a dictionary indexed on package name. Models available for immediate use appear under the key \"MLJ\". \n\nBy declaring show_dotted=true models not in the top-level of the current namespace - which require dots to call, such as MLJ.DeterministicConstantModel - are also included.\n\nmodels(task; show_dotted=false)\n\nList all models matching the specified task. \n\nSee also: localmodels\n\n\n\n\n\n","category":"method"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"localmodels()","category":"page"},{"location":"working_with_tasks/#MLJ.localmodels-Tuple{}","page":"Working with Tasks","title":"MLJ.localmodels","text":"localmodels()\n\nList all models available for immediate use. Equivalent to models()[\"MLJ\"]\n\nlocalmodels(task)\n\nList all such models additionally matching the specified task. Equivalent to models(task)[\"MLJ\"].\n\n\n\n\n\n","category":"method"},{"location":"learning_networks/#Learning-Networks-1","page":"Learning Networks","title":"Learning Networks","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"MLJ has a flexible interface for building networks from multiple machine learning elements, whose complexity extend beyond the \"pipelines\" of other machine learning toolboxes. ","category":"page"},{"location":"learning_networks/#Overview-1","page":"Learning Networks","title":"Overview","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"In the future the casual MLJ user will be able to build common pipeline architetures, such as linear compositites and stacks, with simple macro invocations. Handcrafting a learning network, as outlined below, is an advanced MLJ feature, assuming familiarity with the basics outlined in Getting Started. The syntax for building a learning network is essentially an extension of the basic syntax but with data objects replaced with nodes (\"dynamic data\").","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"In MLJ, a learning network is a graph whose nodes apply an operation, such as predict or transform, using a fixed machine (requiring training) - or which, alternatively, applies a regular (untrained) mathematical operation to its input(s). In practice, a learning network works with fixed sources for its training/evaluation data, but can be built and tested in stages. By contrast, an exported learning network is a learning network exported as a stand-alone, re-usable Model object, to which all the MLJ Model meta-algorthims can be applied (ensembling, systematic tuning, etc).","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"As we shall see, exporting a learning network as a reusable model, is quite simple. While one can entirely skip the build-and-train steps, experimenting with raw learning networks may be the best way to understand how the stand-alone models work.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"In MLJ learning networks treat the flow of information during training and predicting separately. For this reason, simpler examples may appear more a little more complicated than in other approaches. However, in more sophisticated examples, such as stacking, this separation is essential.","category":"page"},{"location":"learning_networks/#Building-a-simple-learning-network-1","page":"Learning Networks","title":"Building a simple learning network","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"(Image: )","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"The diagram above depicts a learning network which standardises the input data X, learns an optimal Box-Cox transformation for the target y, predicts new target values using ridge regression, and then inverse-transforms those predictions, for later comparison with the original test data. The machines are labelled yellow.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"To implement the network, we begin by loading data needed for training and evaluation into source nodes. For testing purposes, we use synthetic data:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"using MLJ # hide\nusing DataFrames, Statistics # hide\nXraw = rand(300,3)\ny = exp(Xraw[:,1] - Xraw[:,2] - 2Xraw[:,3] + 0.1*rand(300))\nX = DataFrame(Xraw)\nys = source(y)\nXs = source(X)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"We label nodes we will construct according to their outputs in the diagram. Notice that the nodes z and yhat use the same machine, namely box, for different operations.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"To construct the W node we first need to define the machine stand that it will use to transform inputs.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"stand_model = Standardizer()\nstand = machine(stand_model, Xs)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Because Xs is a node, instead of concrete data, we can call transform on the machine without first training it, and the result is the new node W, instead of concrete transformed data:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"W = transform(stand, Xs)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"To get actual transformed data we call the node appropriately, which will require we first train the node. Training a node, rather than a machine, triggers training of all necessary machines in the network.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"test, train = partition(eachindex(y), 0.8)\nfit!(W, rows=train)\nW()           # transform all data\nW(rows=test ) # transform only test data\nW(X[3:4,:])   # transform any data, new or old","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"If you like, you can think of W (and the other nodes we will define) as \"dynamic data\": W is data, in the sense that it an be called (\"indexed\") on rows, but dynamic, in the sense the result depends on the outcome of training events.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"The other nodes of our network are defined similarly:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"box_model = UnivariateBoxCoxTransformer()  # for making data look normally-distributed\nbox = machine(box_model, ys)\nz = transform(box, ys)\n\nridge_model = RidgeRegressor(lambda=0.1)\nridge =machine(ridge_model, W, z)\nzhat = predict(ridge, W)\n\nyhat = inverse_transform(box, zhat)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"We are ready to train and evaluate the completed network. Notice that the standardizer, stand, is not retrained, as MLJ remembers that it was trained earlier:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"fit!(yhat, rows=train)\nrms(y[test], yhat(rows=test)) # evaluate","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"We can change a hyperparameters and retrain:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"ridge_model.lambda = 0.01\nfit!(yhat, rows=train) \nrms(y[test], yhat(rows=test))","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Notable feature. The machine, ridge::NodalMachine{RidgeRegressor}, is retrained, because its underlying model has been mutated. However, since the outcome of this training has no effect on the training inputs of the machines stand and box, these transformers are left untouched. (During construction, each node and machine in a learning network determines and records all machines on which it depends.) This behaviour, which extends to exported learning networks, means we can tune our wrapped regressor without re-computing transformations each time the hyperparameter is changed. ","category":"page"},{"location":"learning_networks/#Exporting-a-learning-network-as-a-stand-alone-model-1","page":"Learning Networks","title":"Exporting a learning network as a stand-alone model","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"To export a learning network:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Define a new mutable struct model type.\nWrap the learning network code in a model fit method.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"All learning networks that make determinisic (or, probabilistic) predictions export as models of subtype Deterministic{Node} (respectively, Probabilistic{Node}):","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"mutable struct WrappedRidge <: Deterministic{Node}\n    ridge_model\nend\n\nWrappedRidge(; ridge_model=RidgeRegressor) = WrappedRidge(ridge_model); # keyword constructor","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Now satisfied that our wrapped Ridge Regression learning network works, we simply cut and paste its defining code into a fit method:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"function MLJ.fit(model::WrappedRidge, X, y)\n    Xs = source(X)\n    ys = source(y)\n\n    stand_model = Standardizer()\n    stand = machine(stand_model, Xs)\n    W = transform(stand, Xs)\n\n    box_model = UnivariateBoxCoxTransformer()  # for making data look normally-distributed\n    box = machine(box_model, ys)\n    z = transform(box, ys)\n\n    ridge_model = model.ridge_model ###\n    ridge =machine(ridge_model, W, z)\n    zhat = predict(ridge, W)\n\n    yhat = inverse_transform(box, zhat)\n    fit!(yhat, verbosity=0)\n    \n    return yhat\nend","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"The line marked ###, where the new exported model's hyperparameter ridge_model is spliced into the network, is the only modification.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"What's going on here? MLJ's machine interface is built atop a more primitive model interface, implemented for each algorithm. Each supervised model type (eg, RidgeRegressor) requires model fit and predict methods, which are called by the corresponding machine fit! and predict methods. We don't need to define a  model predict method here because MLJ provides a fallback which simply calls the node returned by fit on the data supplied: MLJ.predict(model::Supervised{Node}, Xnew) = yhat(Xnew).","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"The export process is complete and we can wrap our exported model around any data or task we like, and evaluate like any other model:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"task = load_boston()\nwrapped_model = WrappedRidge(ridge_model=ridge_model)\nmach = machine(wrapped_model, task)\nevaluate!(mach, resampling=CV(), measure=rms, verbosity=0)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Another example of an exported learning network is given in the next subsection.","category":"page"},{"location":"learning_networks/#Static-operations-on-nodes-1","page":"Learning Networks","title":"Static operations on nodes","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Continuing to view nodes as \"dynamic data\", we can, in addition to applying \"dynamic\" operations like predict and transform to nodes, overload ordinary \"static\" operations as well. Common operations, like addition, scalar multiplication, exp and log work out-of-the box. To demonstrate this, consider the code below defining a composite model that:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"(1) one-hot encodes the input table X (2) log transforms the continuous target y (3) fits specified K-nearest neighbour and ridge regressor models to the data (4) computes a weighted average of individual model predictions (5) inverse transforms (exponentiates) the blended predictions","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Note, in particular, the lines defining zhat and yhat, which combine several static node operations.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"mutable struct KNNRidgeBlend <:Deterministic{Node}\n\n    knn_model\n    ridge_model\n    weights::Tuple{Float64, Float64}\n\nend\n\nfunction MLJ.fit(model::KNNRidgeBlend, X, y)\n    \n    Xs = source(X) \n    ys = source(y)\n\n    hot = machine(OneHotEncoder(), Xs)\n\n    # W, z, zhat and yhat are nodes in the network:\n    \n    W = transform(hot, Xs) # one-hot encode the input\n    z = log(ys) # transform the target\n    \n    ridge_model = model.ridge_model\n    knn_model = model.knn_model\n\n    ridge = machine(ridge_model, W, z) \n    knn = machine(knn_model, W, z)\n\n    # average the predictions of the KNN and ridge models\n    zhat = model.weights[1]*predict(ridge, W) + weights[2]*predict(knn, W) \n\n    # inverse the target transformation\n    yhat = exp(zhat) \n\n    fit!(yhat, verbosity=0)\n    \n    return yhat\nend\n","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"task = load_boston()\nknn_model = KNNRegressor(K=2)\nridge_model = RidgeRegressor(lambda=0.1)\nweights = (0.9, 0.1)\nblended_model = KNNRidgeBlend(knn_model, ridge_model, weights)\nmach = machine(blended_model, task)\nevaluate!(mach, resampling=Holdout(fraction_train=0.7), measure=rmsl) ","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"To overerload a function for application to nodes, we the node method.  Here are some examples taken from MLJ source (at work in the example above):","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Base.log(v::Vector{<:Number}) = log.(v)\nBase.log(X::AbstractNode) = node(log, X)\n\nimport Base.+\n+(y1::AbstractNode, y2::AbstractNode) = node(+, y1, y2)\n+(y1, y2::AbstractNode) = node(+, y1, y2)\n+(y1::AbstractNode, y2) = node(+, y1, y2)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Here AbstractNode is the common supertype of Node and Source.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"As a final example, here's how to extend row shuffling to nodes:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"using Random\nRandom.shuffle(X::AbstractNode) = node(Y -> MLJ.selectrows(Y, Random.shuffle(1:nrows(Y))), X)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"using Random # hide \nRandom.shuffle(X::AbstractNode) = node(Y -> MLJ.selectrows(Y, Random.shuffle(1:nrows(Y))), X) # hide\nX = (x1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n     x2 = [:one, :two, :three, :four, :five, :six, :seven, :eight, :nine, :ten])\nXs = source(X)\nW = shuffle(Xs)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"W()","category":"page"},{"location":"learning_networks/#The-learning-network-API-1","page":"Learning Networks","title":"The learning network API","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Three types are part of learning networks: Source, Node and NodalMachine. A NodalMachine is returned by the machine constructor when given nodal arguments instead of concrete data.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"The definitions of Node and NodalMachine are coupled because every NodalMachine has Node objects in its args field (the training arguments specified in the constructor) and every Node must specify a NodalMachine, unless it is static (see below).","category":"page"},{"location":"learning_networks/#Source-nodes-1","page":"Learning Networks","title":"Source nodes","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Only source nodes reference concrete data. A Source object has a single field, data. ","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"source(X)\nsources","category":"page"},{"location":"learning_networks/#MLJ.source-Tuple{Any}","page":"Learning Networks","title":"MLJ.source","text":"Xs = source(X)\n\nDefines a Source object out of data X. The data can be a vector, categorical vector, or table. The calling behaviour of a source node is this:\n\nXs() = X\nXs(rows=r) = selectrows(X, r)  # eg, X[r,:] for a DataFrame\nXs(Xnew) = Xnew\n\nSee also: sources, node\n\n\n\n\n\n","category":"method"},{"location":"learning_networks/#MLJ.sources","page":"Learning Networks","title":"MLJ.sources","text":"sources(N)\n\nReturn a list of all ultimate sources of  a node N. \n\nSee also: node, source\n\n\n\n\n\n","category":"function"},{"location":"learning_networks/#Nodal-machines-1","page":"Learning Networks","title":"Nodal machines","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"The key components of a NodalMachine object are:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"A model,  specifying a learning algorithm and hyperparameters.\nTraining arguments, which specify the nodes acting as proxies for training data on calls to fit!.\nA fit-result, for storing the outcomes of calls to fit!.\nA dependency tape (a vector or DAG) containing elements of type NodalMachine, obtained by merging the tapes of all training arguments.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"A nodal machine is trained in the same way as a regular machine with one difference: Instead of training the model on the wrapped data indexed on rows, it is trained on the wrapped nodes called on rows, with calling being a recursive operation on nodes within a learning network (see below).","category":"page"},{"location":"learning_networks/#Nodes-1","page":"Learning Networks","title":"Nodes","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"The key components of a Node are:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"An operation, which will either be static (a fixed function) or dynamic (such as predict or transform, dispatched on a nodal machine NodalMachine).\nA nodal machine on which to dispatch the operation (void if the operation is static).\nUpstream connections to other nodes (including source nodes) specified by arguments (one for each argument of the operation).\nA dependency tape, obtained by merging the the tapes of all arguments (nodal machines) and adding the present node's nodal machine.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"node","category":"page"},{"location":"learning_networks/#MLJ.node","page":"Learning Networks","title":"MLJ.node","text":"N = node(f::Function, args...)\n\nDefines a Node object N wrapping a static operation f and arguments args. Each of the n element of args must be a Node or Source object. The node N has the following calling behaviour:\n\nN() = f(args[1](), args[2](), ..., args[n]())\nN(rows=r) = f(args[1](rows=r), args[2](rows=r), ..., args[n](rows=r))\nN(X) = f(args[1](X), args[2](X), ..., args[n](X))\n\nJ = node(f, mach::NodalMachine, args...)\n\nDefines a dynamic Node object J wrapping a dynamic operation f (predict, predict_mean, transform, etc), a nodal machine mach and arguments args. Its calling behaviour, which depends on the outcome of training mach (and, implicitly, on training outcomes affecting its arguments) is this:\n\nJ() = f(mach, args[1](), args[2](), ..., args[n]())\nJ(rows=r) = f(mach, args[1](rows=r), args[2](rows=r), ..., args[n](rows=r))\nJ(X) = f(mach, args[1](X), args[2](X), ..., args[n](X))\n\nGenerally n=1 or n=2 in this latter case. \n\nCalling a node is a recursive operation which terminates in the call to a source node (or nodes). Calling nodes on new data X fails unless the number of source nodes is unique.  \n\nSee also: source, sources\n\n\n\n\n\n","category":"type"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"fit!(N::Node; rows=nothing, verbosity=1, force=false)","category":"page"},{"location":"learning_networks/#StatsBase.fit!-Tuple{Node}","page":"Learning Networks","title":"StatsBase.fit!","text":"fit!(N::Node; rows=nothing, verbosity=1, force=false)\n\nWhen called for the first time, train all machines in the dependency tape of N, a necessary and sufficient condition for N() to be defined. Use only those rows with indices in rows, or use all rows if unspecified. \n\nIn subsequent calls to fit! the same machines are updated, but only if force=true, or if the rows specified for training are different from the last train, or if they are stale.\n\nA machine mach is stale if mach.model has changed since it was last trained, or if if one of its training arguments is stale. A node N is stale if N.machine is stale or one of its arguments is stale. A source node is never stale.\n\n\n\n\n\n","category":"method"},{"location":"simple_user_defined_models/#Simple-User-Defined-Models-1","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"","category":"section"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"To quickly implement a new supervised model in MLJ, it suffices to:","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"Define a mutable struct to store hyperparameters. This is either a subtype of Probabilistic{Any} or Deterministic{Any}, depending on whether probabilistic or ordinary point predictions are intended. This struct is the model.\nDefine a fit method, dispatched on the model, returning learned parameters, also known as the fit-result.\nDefine a predict method, dispatched on the model, and passed the fit-result, to return predictions on new patterns.","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"In the examples below, the training input X of fit, and the new input Xnew passed to predict, are tables. Each training target y is a Vector or CategoricalVector, according to its scientific type, or a table in the multivariate case. ","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"The predicitions returned by predict have the same form as y for deterministic models, but are Vectors of distibutions for probabilistic models.","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"For your models to implement an optional update method, to buy into the MLJ logging protocol, or report training statistics or other model-specific functionality, a fit method with a slightly different signature and output is required. To enable checks of the scientific type of data passed to your model by MLJ's meta-algorithms, one needs to implement additional traits. A clean! method can be defined to check that hyperparameter values are within normal ranges. For details, see Adding Models for General Use.","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"For an unsupervised model, implement transform and, optionally, inverse_transform using the same signature at `predict below.","category":"page"},{"location":"simple_user_defined_models/#A-simple-deterministic-regressor-1","page":"Simple User Defined Models","title":"A simple deterministic regressor","text":"","category":"section"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"Here's a quick-and-dirty implementation of a ridge regressor with no intercept:","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"import MLJBase\nusing LinearAlgebra\n\nmutable struct MyRegressor <: MLJBase.Deterministic{Any}\n    lambda::Float64\nend\n\n# fit returns coefficients minimizing a penalized rms loss function:\nfunction MLJBase.fit(model::MyRegressor, X, y)\n    x = MLJBase.matrix(X)                     # convert table to matrix\n    fitresult = (x'x - model.lambda*I)\\(x'y)  # the coefficients\n    return fitresult\nend\n\n# predict uses coefficients to make new prediction:\nMLJBase.predict(model::MyRegressor, fitresult, Xnew) = MLJBase.matrix(Xnew)fitresult","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"After loading this code, all MLJ's basic meta-algorithms can be applied to MyRegressor:","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"julia> using MLJ\njulia> task = load_boston()\njulia> model = MyRegressor(1.0)\njulia> regressor = machine(model, task)\njulia> evaluate!(regressor, resampling=CV(), measure=rms) |> mean\n7.434221318358656\n","category":"page"},{"location":"simple_user_defined_models/#A-simple-probabilistic-classifier-1","page":"Simple User Defined Models","title":"A simple probabilistic classifier","text":"","category":"section"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"The following probabilistic model simply fits a probability distribution to the MultiClass training target (i.e., ignores X) and returns this pdf for any new pattern:","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"import MLJBase\nimport Tables\nimport Distributions\n\nstruct MyClassifier <: MLJBase.Probabilistic{Any}\nend\n\n# `fit` ignores the inputs X and returns the training target y\n# probability distribution:\nfunction MLJBase.fit(model::MyClassifier, X, y)\n    fitresult = Distributions.fit(MLJBase.UnivariateNominal, y)\n    return fitresult\nend\n\n# `predict` retunrs the passed fitresult (pdf) for all new patterns:\nfunction MLJBase.predict(model::MyClassifier, fitresult, Xnew)\n    row_iterator = Tables.rows(Xnew)\n    return [fitresult for r in row_iterator]\nend","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"For more details on the UnivariateNominal distribution, query MLJBase.UnivariateNominal.","category":"page"},{"location":"adding_models_for_general_use/#Adding-New-Models-1","page":"Adding Models for General Use","title":"Adding New Models","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"This guide outlines in detail the specification of the MLJ model interface and provides guidelines for implementing the interface for models intended for general use. For sample implementations, see MLJModels/src.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The machine learning tools provided by MLJ can be applied to the models in any package that imports the package MLJBase and implements the API defined there, as outlined below. For a quick-and-dirty implementation of user-defined models see Simple User Defined Models.  To make new models available to all MLJ users, see Where to place code implementing new models.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"It is assumed the reader has read Getting Started. To implement the API described here, some familiarity with the following packages is also helpful:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Distributions.jl (for probabilistic predictions)\nCategoricalArrays.jl (essential if you are implementing a model handling data of Multiclass or FiniteOrderedFactor scitype)\nTables.jl (if you're algorithm needs input data in a novel format).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"In MLJ, the basic interface exposed to the user, built atop the model interface described here, is the machine interface. After a first reading of this document, the reader may wish to refer to MLJ Internals for context.","category":"page"},{"location":"adding_models_for_general_use/#Overview-1","page":"Adding Models for General Use","title":"Overview","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A model is an object storing hyperparameters associated with some machine learning algorithm.  In MLJ, hyperparameters include configuration parameters, like the number of threads, and special instructions, such as \"compute feature rankings\", which may or may not affect the final learning outcome.  However, the logging level (verbosity below) is excluded.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The name of the Julia type associated with a model indicates the associated algorithm (e.g., DecisionTreeClassifier). The outcome of training a learning algorithm is called a fit-result. For ordinary multilinear regression, for example, this would be the coefficients and intercept. For a general supervised model, it is the (generally minimal) information needed to make new predictions.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The ultimate supertype of all models is MLJBase.Model, which has two abstract subtypes:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"abstract type Supervised{R} <: Model end\nabstract type Unsupervised <: Model end","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Here the parameter R refers to a fit-result type. By declaring a model to be a subtype of MLJBase.Supervised{R} you guarantee the fit-result to be of type R and, if R is concrete, one may improve the performance of homogeneous ensembles of the model (as defined by the built-in MLJ EnsembleModel wrapper). There is no abstract type for fit-results because these types are generally declared outside of MLJBase.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"WIP: The necessity to declare the fitresult type R may disappear in the future (issue #93).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Supervised models are further divided according to whether they are able to furnish probabilistic predictions of the target(s) (which they will do so by default) or directly predict \"point\" estimates, for each new input pattern:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"abstract type Probabilistic{R} <: Supervised{R} end\nabstract type Deterministic{R} <: Supervised{R} end","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Further division of model types is realized through Trait declarations.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Associated with every concrete subtype of Model there must be a fit method, which implements the associated algorithm to produce the fit-result. Additionally, every Supervised model has a predict method, while Unsupervised models must have a transform method. More generally, methods such as these, that are dispatched on a model instance and a fit-result (plus other data), are called operations. Probabilistic supervised models optionally implement a predict_mode operation (in the case of classifiers) or a predict_mean and/or predict_median operations (in the case of regressors) overriding obvious fallbacks provided by MLJBase. Unsupervised models may implement an inverse_transform operation.","category":"page"},{"location":"adding_models_for_general_use/#New-model-type-declarations-and-optional-clean!-method-1","page":"Adding Models for General Use","title":"New model type declarations and optional clean! method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Here is an example of a concrete supervised model type declaration, made after defining an appropriate fit-result type (an optional step):","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"import MLJ\n\nstruct LinearFitResult{F<:AbstractFloat} <: MLJBase.MLJType\n    coefficients::Vector{F}\n    bias::F\nend\n\nmutable struct RidgeRegressor{F} <: MLJBase.Deterministic{LinearFitResult{F}}\n    target_type::Type{F}\n    lambda::Float64\nend","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Note. Model fields may be of any type except NamedTuple.  (This is because named tuples are used to represented the nested hyperparameters  of composite models (models that have other models as fields.)","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Models (which are mutable) should not be given internal constructors. It is recommended that they be given an external lazy keyword constructor of the same name. This constructor defines default values for every field, and optionally corrects invalid field values by calling a clean! method (whose fallback returns an empty message string):","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"function MLJ.clean!(model::RidgeRegressor)\n    warning = \"\"\n    if model.lambda < 0\n        warning *= \"Need lambda ≥ 0. Resetting lambda=0. \"\n        model.lambda = 0\n    end\n    return warning\nend\n\n# keyword constructor\nfunction RidgeRegressor(; target_type=Float64, lambda=0.0)\n\n    model = RidgeRegressor(target_type, lambda)\n\n    message = MLJBase.clean!(model)\n    isempty(message) || @warn message\n\n    return model\n    \nend","category":"page"},{"location":"adding_models_for_general_use/#Supervised-models-1","page":"Adding Models for General Use","title":"Supervised models","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Below we describe the compulsory and optional methods to be specified for each concrete type SomeSupervisedModelType{R} <: MLJBase.Supervised{R}. ","category":"page"},{"location":"adding_models_for_general_use/#The-form-of-data-for-fitting-and-predicting-1","page":"Adding Models for General Use","title":"The form of data for fitting and predicting","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"In every circumstance, the argument X passed to the fit method described below, and the argument Xnew of the predict method, will be some table supporting the Tables.jl API. The interface implementer can control the scientific type of data appearing in X with an appropriate input_scitype declaration (see Trait declarations, as union_scitypes(X) <: input_scitype(SomeSupervisedModel) will always hold. See Convenience methods below for the definition of union_scitypes. If the core algorithm requires data in a different or more specific form, then fit will need to coerce the table into the form desired. To this end, MLJ provides the convenience method MLJBase.matrix; MLJBase.matrix(Xtable) has type Matrix{T} where T is the tightest common type of elements of Xtable, and Xtable is any table.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Tables.jl has recently added a matrix method as well.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Other convenience methods provided by MLJBase for handling tabular data are: selectrows, selectcols, select, schema (for extracting the size, names and eltypes of a table) and table (for materializing an abstract matrix, or named tuple of vectors, as a table matching a given prototype). See Convenience methods below for details.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Note that generally the same type coercions applied to X by fit will need to be applied by predict to Xnew. ","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Important convention It is to be understood that the columns of the table X correspond to features and the rows to patterns.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For univariate targets, y is always a Vector or CategoricalVector, according to the value of the trait:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"target_scitype(SomeSupervisedModelType) type of y a supertype of eltype(y)\nContinuous Vector Real\n<: Multiclass CategoricalVector Union{CategoricalString, CategoricalValue}\n<: FiniteOrderedFactor CategoricalVector Union{CategoricalString, CategoricalValue}\nCount Vector Integer","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The form of the target data y passed to fit is constrained by the target_scitype trait declaration. In the univariate case, all elements of y will satisfy union_scitype(y) <: target_scitype(SomeSupervisedModelType).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"So, for example, if your model is a binary classifier, you declare","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"target_scitype(SomeSupervisedModelType)=Multiclass{2}","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"If it can predict any number of classes, you might instead declare","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"target_scitype(SomeSupervisedModelType)=Union{Multiclass, FiniteOrderedFactor}","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"See also the table in Getting Started.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For multivariate targets, the elements of y, a table, will be constrained by column_scitypes_as_tuple(y) <: target_scitype(SomeSupervisedModelType) For example, if you declare target_scitype(SomeSupervisedModelType) = Tuple{Continuous,Count}, then y will have two columns, the first with Real elements, the second with Integer elements.","category":"page"},{"location":"adding_models_for_general_use/#The-fit-method-1","page":"Adding Models for General Use","title":"The fit method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A compulsory fit method returns three objects:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.fit(model::SomeSupervisedModelType, verbosity::Int, X, y) -> fitresult, cache, report","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Note: The Int typing of verbosity cannot be omitted.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"fitresult::R is the fit-result in the sense above (which becomes an  argument for predict discussed below).\nreport is a (possibly empty) NamedTuple, for example,  report=(deviance=..., dof_residual=..., stderror=..., vcov=...).  Any training-related statistics, such as internal estimates of the  generalization error, and feature rankings, should be returned in  the report tuple. How, or if, these are generated should be  controlled by hyperparameters (the fields of model). Fitted  parameters, such as the coefficients of a linear model, do not go  in the report as they will be extractable from fitresult (and  accessible to MLJ through the fitted_params method, see below).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"3.\tThe value of cache can be nothing, unless one is also defining       an update method (see below). The Julia type of cache is not       presently restricted.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"It is not necessary for fit to provide dimension checks or to call clean! on the model; MLJ will carry out such checks.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The method fit should never alter hyperparameter values. If the package is able to suggest better hyperparameters, as a byproduct of training, return these in the report field.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"One should test that actual fit-results have the type declared in the model mutable struct declaration. To help with this, MLJBase.fitresult_type(m) returns the declared type, for any supervised model (or model type) m.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The verbosity level (0 for silent) is for passing to learning algorithm itself. A fit method wrapping such an algorithm should generally avoid doing any of its own logging.","category":"page"},{"location":"adding_models_for_general_use/#The-fitted_params-method-1","page":"Adding Models for General Use","title":"The fitted_params method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A fitted_params method may be optionally overloaded. It's purpose is to provide MLJ accesss to a user-friendly representation of the learned parameters of the model (as opposed to the hyperparameters). They must be extractable from fitresult.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.fitted_params(model::SomeSupervisedModelType, fitresult) -> friendly_fitresult::NamedTuple","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For a linear model, for example, one might declare something like friendly_fitresult=(coefs=[...], bias=...).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The fallback is to return (fitresult=fitresult,).","category":"page"},{"location":"adding_models_for_general_use/#The-predict-method-1","page":"Adding Models for General Use","title":"The predict method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A compulsory predict method has the form","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.predict(model::SomeSupervisedModelType, fitresult, Xnew) -> yhat","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Here Xnew is an any table whose entries satisfy the same scitype constraints as discussed for X above.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Prediction types for deterministic responses. In the case of Deterministic models, yhat must have the same form as the target y passed to the fit method (see above discussion on the form of data for fitting), with one exception: If predicting a Count, the prediction may be Continuous. For all models predicting a Multiclass or FiniteOrderedFactor, the categorical vectors returned by predict must have the levels in the categorical pool of the target data presented in training, even if not all levels appear in the training data or prediction itself. That is, we must have levels(yhat) == levels(y).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Unfortunately, code not written with the preservation of categorical levels in mind poses special problems. To help with this, MLJ provides a utility CategoricalDecoder which can decode a CategoricalArray into a plain array, and re-encode a prediction with the original levels intact. The CategoricalDecoder object created during fit will need to be bundled with fitresult to make it available to predict during re-encoding. ","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"So, for example, if the core algorithm being wrapped by fit expects a nominal target yint of type Vector{Int64} then a fit method may look something like this:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"function MLJBase.fit(model::SomeSupervisedModelType, verbosity, X, y)\n    decoder = MLJBase.CategoricalDecoder(y, Int64)\n    yint = transform(decoder, y)\n    core_fitresult = SomePackage.fit(X, yint, verbosity=verbosity)\n    fitresult = (decoder, core_fitresult)\n    cache = nothing\n    report = nothing\n    return fitresult, cache, report\nend","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"while a corresponding deterministic predict operation might look like this:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"function MLJBase.predict(model::SomeSupervisedModelType, fitresult, Xnew)\n    decoder, core_fitresult = fitresult\n    yhat = SomePackage.predict(core_fitresult, Xnew)\n    return inverse_transform(decoder, yhat)\nend","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Query ?MLJBase.DecodeCategorical for more information.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"If you are coding a learning algorithm from scratch, rather than wrapping an existing one, conversions may be unnecessary. It may suffice to record the pool of y and bundle that with the fitresult for predict to append to the levels of its categorical output.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Prediction types for probabilistic responses. In the case of Probabilistic models with univariate targets, yhat must be a Vector whose elements are distributions (one distribution per row of Xnew).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A distribution is any instance of a subtype of Distributions.Distribution from the package Distributions.jl, or any instance of the additional types UnivariateNominal and MultivariateNominal defined in MLJBase.jl (or any other type D you define for which MLJBase.isdistribution(::D) = true, meaning Base.rand and Distributions.pdf are implemented, as well Distributions.mean/Distribution.median or Distributions.mode).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Use UnivariateNominal for Probabilistic models predicting Multiclass or FiniteOrderedFactor targets. For example, suppose levels(y)=[\"yes\", \"no\", \"maybe\"] and set L=levels(y). Then, if the predicted probabilities for some input pattern are [0.1, 0.7, 0.2], respectively, then the prediction returned for that pattern will be UnivariateNominal(L, [0.1, 0.7, 0.2]). Query ?UnivariateNominal for more information.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The predict method will need access to all levels in the pool of the target variable presented y presented for training, which consequently need to be encoded in the fitresult returned by fit. If a CategoricalDecoder object, decoder, has been bundled in fitresult, as in the deterministic example above, then the levels are given by levels(decoder). Levels not observed in the training data  (i.e., only in its pool) should be assigned probability zero.","category":"page"},{"location":"adding_models_for_general_use/#Trait-declarations-1","page":"Adding Models for General Use","title":"Trait declarations","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"There are a number of recommended trait declarations for each model mutable structure SomeSupervisedModelType <: Supervised you define. Basic fitting, resampling and tuning in MLJ does not require these traits but some advanced MLJ meta-algorithms may require them now, or in the future. In particular, MLJ's models(::Task) method (matching models to user-specified tasks) can only identify models having a complete set of trait declarations. A full set of declarations is shown below for the DecisionTreeClassifier type (defined in the submodule DecisionTree_ of MLJModels):","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.load_path(::Type{<:DecisionTreeClassifier}) = \"MLJModels.DecisionTree_.DecisionTreeClassifier\" \nMLJBase.package_name(::Type{<:DecisionTreeClassifier}) = \"DecisionTree\"\nMLJBase.package_uuid(::Type{<:DecisionTreeClassifier}) = \"7806a523-6efd-50cb-b5f6-3fa6f1930dbb\"\nMLJBase.package_url(::Type{<:DecisionTreeClassifier}) = \"https://github.com/bensadeghi/DecisionTree.jl\"\nMLJBase.is_pure_julia(::Type{<:DecisionTreeClassifier}) = true\nMLJBase.input_is_multivariate(::Type{<:DecisionTreeClassifier}) = true\nMLJBase.input_scitypes(::Type{<:DecisionTreeClassifier}) = MLJBase.Continuous\nMLJBase.target_scitype(::Type{<:DecisionTreeClassifier}) = MLJBase.Multiclass","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Note that models predicting multivariate targets will need to need to have target_scitype return an appropriate Tuple type. ","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For an explanation of Found and Other in the table below, see Scientific Types.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"method return type declarable return values default value\ntarget_scitype DataType subtype of Found or tuple of such types Union{Found,NTuple{<:Found}}\ninput_scitypes DataType subtype of Union{Missing,Found} Union{Missing,Found}\ninput_is_multivariate Bool true or false true\nis_pure_julia Bool true or false false\nload_path String unrestricted \"unknown\"\npackage_name String unrestricted \"unknown\"\npackage_uuid String unrestricted \"unknown\"\npackage_url String unrestricted \"unknown\"","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"You can test declarations of traits by calling info(SomeModelType).","category":"page"},{"location":"adding_models_for_general_use/#The-update!-method-1","page":"Adding Models for General Use","title":"The update! method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"An update method may be optionally overloaded to enable a call by MLJ to retrain a model (on the same training data) to avoid repeating computations unnecessarily.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.update(model::SomeSupervisedModelType, verbosity, old_fitresult, old_cache, X, y) -> fitresult, cache, report","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"If an MLJ Machine is being fit! and it is not the first time, then update is called instead of fit unless fit! has been called with new rows. However, MLJBase defines a fallback for update which just calls fit. For context, see MLJ Internals. ","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Learning networks wrapped as models constitute one use-case: One would like each component model to be retrained only when hyperparameter changes \"upstream\" make this necessary. In this case MLJ provides a fallback (specifically, the fallback is for any subtype of Supervised{Node}). A second important use-case is iterative models, where calls to increase the number of iterations only restarts the iterative procedure if other hyperparameters have also changed. For an example, see builtins/Ensembles.jl.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"In the event that the argument fitresult (returned by a preceding call to fit) is not sufficient for performing an update, the author can arrange for fit to output in its cache return value any additional information required, as this is also passed as an argument to the update method.","category":"page"},{"location":"adding_models_for_general_use/#Multivariate-models-1","page":"Adding Models for General Use","title":"Multivariate models","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"TODO","category":"page"},{"location":"adding_models_for_general_use/#Unsupervised-models-1","page":"Adding Models for General Use","title":"Unsupervised models","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"TODO","category":"page"},{"location":"adding_models_for_general_use/#Convenience-methods-1","page":"Adding Models for General Use","title":"Convenience methods","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.CategoricalDecoder","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.CategoricalDecoder","page":"Adding Models for General Use","title":"MLJBase.CategoricalDecoder","text":"CategoricalDecoder(C::CategoricalArray)\nCategoricalDecoder(C::CategoricalArray, eltype, start_at_zero=false)\n\nConstruct a decoder for transforming a CategoricalArray{T} object into an ordinary array, and for re-encoding similar arrays back into a CategoricalArray{T} object having the same pool (and, in particular, the same levels) as C. If eltype is not specified then the element type of the transformed array is T. Otherwise, the element type is eltype and the elements are conversions to eltype of the internal (unsigned integer) refs of the CategoricalArray, shifted backwards by one if start_at_zero=false. One must have eltype <: Real.\n\nIf eltype = Bool, then start_at_zero is ignored.\n\ntransform(decoder::CategoricalDecoder, C::CategoricalArray)\n\nTransform C into an ordinary Array.\n\ninverse_transform(decoder::CategoricalDecoder, A::Array)\n\nTransform an array A suitably compatible with decoder into a CategoricalArray having the same pool as C.\n\nlevels(decoder::CategoricalDecoder)\nlevels_seen(decoder::CategoricaDecoder)\n\nReturn, respectively, all levels in pool of the categorical vector C used to construct decoder (ie, levels(C)), and just those levels explicitly appearing as entries of C (ie, unique(C)).\n\nExample\n\njulia> using CategoricalArrays\njulia> C = categorical([\"a\" \"b\"; \"a\" \"c\"])\n2×2 CategoricalArray{String,2,UInt32}:\n \"a\"  \"b\"\n \"a\"  \"c\"\n\njulia> decoder = MLJBase.CategoricalDecoder(C, eltype=Float64);\njulia> A = transform(decoder, C)\n2×2 Array{Float64,2}:\n 1.0  2.0\n 1.0  3.0\n\njulia> inverse_transform(decoder, A[1:1,:])\n1×2 CategoricalArray{String,2,UInt32}:\n \"a\"  \"b\"\n\njulia> levels(ans)\n3-element Array{String,1}:\n \"a\"\n \"b\"\n \"c\"\n\n\n\n\n\n","category":"type"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.matrix","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.matrix","page":"Adding Models for General Use","title":"MLJBase.matrix","text":"MLJBase.matrix(X)\n\nConvert a table source X into an Matrix; or, if X is a AbstractMatrix, return X. Optimized for column-based sources.\n\nIf instead X is a sparse table, then a SparseMatrixCSC object is returned. The integer relabelling of column names follows the lexicographic ordering (as indicated by schema(X).names).\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.table","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.table","page":"Adding Models for General Use","title":"MLJBase.table","text":"MLJBase.table(cols; prototype=cols)\n\nConvert a named tuple of vectors cols, into a table. The table type returned is the \"preferred sink type\" for prototype (see the Tables.jl documentation). \n\nMLJBase.table(X::AbstractMatrix; names=nothing, prototype=nothing)\n\nConvert an abstract matrix X into a table with names (a tuple of symbols) as column names, or with labels (:x1, :x2, ..., :xn) where n=size(X, 2), if names is not specified.  If prototype=nothing, then a named tuple of vectors is returned.\n\nEquivalent to table(cols, prototype=prototype) where cols is the named tuple of columns of X, with keys(cols) = names.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.select","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.select","page":"Adding Models for General Use","title":"MLJBase.select","text":"select(X, r, c)\n\nSelect element of a table or sparse table at row r and column c. In the case of sparse data where the key (r, c), zero or missing is returned, depending on the value type.\n\nSee also: selectrows, selectcols\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.selectrows","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.selectrows","page":"Adding Models for General Use","title":"MLJBase.selectrows","text":"selectrows(X, r)\n\nSelect single or multiple rows from any table, sparse table, or abstract vector X.  If X is tabular, the object returned is a table of the preferred sink type of typeof(X), even a single row is selected.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.selectcols","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.selectcols","page":"Adding Models for General Use","title":"MLJBase.selectcols","text":"selectcols(X, c)\n\nSelect single or multiple columns from any table or sparse table X. If c is an abstract vector of integers or symbols, then the object returned is a table of the preferred sink type of typeof(X). If c is a single integer or column, then a Vector or CategoricalVector is returned.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.schema","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.schema","page":"Adding Models for General Use","title":"MLJBase.schema","text":"schema(X)\n\nReturns a struct with properties names, types with the obvious meanings. Here X is any table or sparse table.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.nrows","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.nrows","page":"Adding Models for General Use","title":"MLJBase.nrows","text":"nrows(X)\n\nReturn the number of rows in a table, sparse table, or abstract vector.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.union_scitypes","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.column_scitypes_as_tuple","category":"page"},{"location":"adding_models_for_general_use/#Where-to-place-code-implementing-new-models-1","page":"Adding Models for General Use","title":"Where to place code implementing new models","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Note that different packages can implement models having the same name without causing conflicts, although an MLJ user cannot simultaneously load two such models.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"There are two options for making a new model implementation available to all MLJ users:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Native implementations (preferred option). The implementation code lives in the same package that contains the learning algorithms implementing the interface. In this case, it is sufficient to open an issue at MLJRegistry requesting the package to be registered with MLJ. Registering a package allows the MLJ user to access its models' metadata and to selectively load them.\nExternal implementations (short-term alternative). The model implementation code is necessarily separate from the package SomePkg defining the learning algorithm being wrapped. In this case, the recommended procedure is to include the implementation code at MLJModels/src via a pull-request, and test code at MLJModels/test. Assuming SomePkg is the only package imported by the implementation code, one needs to: (i) register SomePkg at MLJRegistry as explained above; and (ii) add a corresponding @require line in the PR to MLJModels/src/MLJModels.jl to enable lazy-loading of that package by MLJ (following the pattern of existing additions). If other packages must be imported, add them to the MLJModels project file after checking they are not already there. If it is really necessary, packages can be also added to Project.toml for testing purposes.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Additionally, one needs to ensure that the implementation code defines the package_name and load_path model traits appropriately, so that MLJ's @load macro can find the necessary code (see MLJModels/src for examples). The @load command can only be tested after registration. If changes are made, lodge an issue at MLJRegistry to make the changes available to MLJ.  ","category":"page"},{"location":"internals/#Internals-1","page":"Internals","title":"Internals","text":"","category":"section"},{"location":"internals/#The-machine-interface,-simplified-1","page":"Internals","title":"The machine interface, simplified","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"The following is simplified description of the Machine interface. See also the Glossary","category":"page"},{"location":"internals/#The-Machine-type-1","page":"Internals","title":"The Machine type","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"mutable struct Machine{M<Model}\n\n    model::M\n    fitresult\n    cache\n    args::Tuple    # e.g., (X, y) for supervised models\n    report\n    rows # remember last rows used \n    \n    function Machine{M}(model::M, args...) where M<:Model\n        machine = new{M}(model)\n        machine.args = args\n        machine.report = Dict{Symbol,Any}()\n        return machine\n    end\n\nend","category":"page"},{"location":"internals/#Constructor-1","page":"Internals","title":"Constructor","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"machine(model::M, Xtable, y) = Machine{M}(model, Xtable, y)","category":"page"},{"location":"internals/#fit!-and-predict/transform-1","page":"Internals","title":"fit! and predict/transform","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"function fit!(machine::Machine; rows=nothing, verbosity=1) \n\n    warning = clean!(mach.model)\n    isempty(warning) || verbosity < 0 || @warn warning \n\n    if rows == nothing\n        rows = (:) \n    end\n\n    rows_have_changed  = (!isdefined(mach, :rows) || rows != mach.rows)\n\n    args = [MLJ.selectrows(arg, rows) for arg in mach.args]\n\t\n    if !isdefined(mach, :fitresult) || rows_have_changed || force \n        mach.fitresult, mach.cache, report =\n            fit(mach.model, verbosity, args...)\n    else # call `update`:\n        mach.fitresult, mach.cache, report =\n            update(mach.model, verbosity, mach.fitresult, mach.cache, args...)\n    end\n\n    if rows_have_changed\n        mach.rows = deepcopy(rows)\n    end\n\n    if report != nothing\n        merge!(mach.report, report)\n    end\n\n    return mach\n\nend\n\nfunction predict(machine::Machine{<:Supervised}, Xnew)\n    if isdefined(machine, :fitresult)\n        return predict(machine.model, machine.fitresult, Xnew))\n    else\n        throw(error(\"$machine is not trained and so cannot predict.\"))\n    end\nend\n\nfunction transform(machine::Machine{<:Unsupervised}, Xnew)\n    if isdefined(machine, :fitresult)\n        return transform(machine.model, machine.fitresult, Xnew))\n    else\n        throw(error(\"$machine is not trained and so cannot transform.\"))\n    end\nend","category":"page"},{"location":"glossary/#Glossary-1","page":"Glossary","title":"Glossary","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Note: This glossary includes some detail intended mainly for MLJ developers.","category":"page"},{"location":"glossary/#Basics-1","page":"Glossary","title":"Basics","text":"","category":"section"},{"location":"glossary/#task-(object-of-type-Task)-1","page":"Glossary","title":"task (object of type Task)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Data plus a learning objective (e.g., \"probabilistic prediction of Sales\"). In MLJ a task does not include a description of how the completed task is to be evaluated.","category":"page"},{"location":"glossary/#hyperparameters-1","page":"Glossary","title":"hyperparameters","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Parameters on which some learning algorithm depends, specified before the algorithm is applied, and where learning is interpreted in the broadest sense. For example, PCA feature reduction is a \"preprocessing\" transformation \"learning\" a projection from training data, governed by a dimension hyperparameter. Hyperparameters in our sense may specify configuration (eg, number of parallel processes) even when this does not effect the end-product of learning. (But we exlcude verbosity level.)","category":"page"},{"location":"glossary/#model-(object-of-abstract-type-Model)-1","page":"Glossary","title":"model (object of abstract type Model)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Object collecting together hyperameters of a single algorithm. Most models are classified either as supervised or unsupervised models (generally, \"transformers\").","category":"page"},{"location":"glossary/#fit-result-(type-generally-defined-outside-of-MLJ)-1","page":"Glossary","title":"fit-result (type generally defined outside of MLJ)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Also known as \"learned\" or \"fitted\" parameters, these are \"weights\", \"coefficients\", or similar paramaters learned by an algorithm, after adopting the prescribed hyperparameters. For example, decision trees of a random forest, the coefficients and intercept of a linear model, or the rotation and projection matrices of PCA reduction scheme.","category":"page"},{"location":"glossary/#operation-1","page":"Glossary","title":"operation","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Data-manipulating operations (methods) parameterized by some fit-result. For supervised learners, the predict or predict_mode methods, for transformers, the transform or inverse_transform method. In some contexts, such an operation might be replaced by an ordinary operation (method) that does not depend on an fit-result, which are then then called static operations for clarity. An operation that is not static is dynamic.","category":"page"},{"location":"glossary/#machine-(object-of-type-Machine)-1","page":"Glossary","title":"machine (object of type Machine)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"An object consisting of:","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(1) A model ","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(2) A fit-result (undefined until training)","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(3) Training arguments (one for each data argument of the model's associated fit method). A training argument is data used for training. Generally, there are two training arguments for supervised models, and just one for unsuperivsed models.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"In addition machines store \"report\" metadata, for recording algorithm-specific statistics of training (eg, internal estimate of generalization error, feature importances); and they cache information allowing the fit-result to be updated without repeating unnecessary information.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Trainable models are trained by calls to a fit method which may be passed an optional argument specifying the rows of data to be used in training.","category":"page"},{"location":"glossary/#Learning-Networks-and-Composite-Models-1","page":"Glossary","title":"Learning Networks and Composite Models","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Note: Multiple nodal machines may share the same model, and multiple learning nodes may share the same nodal machine.","category":"page"},{"location":"glossary/#source-node-(object-of-type-Source)-1","page":"Glossary","title":"source node (object of type Source)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"A container for training data and point of entry for new data in a learning network (see below).","category":"page"},{"location":"glossary/#nodal-machine-(object-of-type-NodalMachine)-1","page":"Glossary","title":"nodal machine (object of type NodalMachine)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Like a machine with the following exceptions:","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(1) Training arguments are source nodes or regular nodes (see below) in the learning network, instead of data.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(2) The object internally records dependencies on other other nodal machines, as implied by the training arguments, and so on. ","category":"page"},{"location":"glossary/#node-(object-of-type-Node)-1","page":"Glossary","title":"node (object of type Node)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Essentially a nodal machine wrapped in an associated operation (e.g., predict or inverse_transform). It detail, it consists of:","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(1) An operation, static or dynamic.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(2) A nodal machine, void if the operation is static.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(3) Upstream connections to other learning or source nodes, specified by a list    of arguments (one for each argument of the operation).","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(4) Metadata recording the dependencies of the object's machine, and the dependecies on other nodal machines implied by its arguments.","category":"page"},{"location":"glossary/#learning-network-1","page":"Glossary","title":"learning network","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"An acyclic directed graph implicit in the connections of a collection of source(s) and nodes. Each connected component is ordinarily restricted to have a unique source.","category":"page"},{"location":"glossary/#wrapper-1","page":"Glossary","title":"wrapper","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Any model with one or more other models as hyperparameters.","category":"page"},{"location":"glossary/#composite-model-1","page":"Glossary","title":"composite model","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Any wrapper, or any learning network \"exported\" as a model (see Learning Networks).","category":"page"},{"location":"api/#API-1","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Functions-1","page":"API","title":"Functions","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"Modules = [MLJ,MLJBase,MLJModels]","category":"page"},{"location":"api/#MLJ.EnsembleModel-Tuple{}","page":"API","title":"MLJ.EnsembleModel","text":"EnsembleModel(atom=nothing, weights=Float64[], bagging_fraction=0.8, rng_seed=0, n=100, parallel=true)\n\nCreate a model for training an ensemble of n learners, with optional bagging, each with associated model atom. Ensembling is useful if fit!(machine(atom, data...)) does not create identical models on repeated calls (ie, is a stochastic model, such as a decision tree with randomized node selection criteria), or if bagging_fraction is set to a value less than 1.0, or both. The constructor fails if no atom is specified.\n\nPredictions are weighted according to the vector weights (to allow for external optimization) except in the case that atom is a Deterministic classifier. Uniform weights are used if weight has zero length.\n\nThe ensemble model is Deterministic or Probabilistic, according to the corresponding supertype of atom. In the case of deterministic classifiers (target_scitype(atom) <: Union{Multiclass,FiniteOrderedFactor}), the predictions are majority votes, and for regressors (target_scitype(atom)<: Continuous) they are ordinary averages. Probabilistic predictions are obtained by averaging the atomic probability distribution/mass functions; in particular, for regressors, the ensemble prediction on each input pattern has the type MixtureModel{VF,VS,D} from the Distributions.jl package, where D is the type of predicted distribution for atom.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.TunedModel-Tuple{}","page":"API","title":"MLJ.TunedModel","text":"tuned_model = TunedModel(; model=nothing,\n                         tuning=Grid(),\n                         resampling=Holdout(),\n                         measure=nothing,\n                         operation=predict,\n                         nested_ranges=NamedTuple(),\n                         minimize=true,\n                         full_report=true)\n\nConstruct a model wrapper for hyperparameter optimization of a supervised learner.\n\nCalling fit!(mach) on a machine mach=machine(tuned_model, X, y) will: (i) Instigate a search, over clones of model with the hyperparameter mutations specified by nested_ranges, for that model optimizing the specified measure, according to evaluations carried out using the specified tuning strategy and resampling strategy; and (ii) Fit a machine, mach_optimal = mach.fitresult, wrapping the optimal model object in all the provided data X, y. Calling predict(mach, Xnew) then returns predictions on Xnew of the machine mach_optimal.\n\nIf measure is a score, rather than a loss, specify minimize=false.\n\nThe optimal clone of model is accessible as fitted_params(mach).best_model. In the case of two-parameter tuning, a Plots.jl plot of performance estimates is returned by plot(mach) or heatmap(mach).\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.evaluate!-Tuple{Machine}","page":"API","title":"MLJ.evaluate!","text":"evaluate!(mach, resampling=CV(), measure=nothing, operation=predict, verbosity=1)\n\nEstimate the performance of a machine mach using the specified resampling strategy (defaulting to 6-fold cross-validation) and measure, which can be a single measure or vector. \n\nAlthough evaluate! is mutating, mach.model and mach.args are preserved.\n\nResampling and testing is based exclusively on data in rows, when specified.\n\nIf no measure is specified, then default_measure(mach.model) is used, unless this default is nothing and an error is thrown.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.iterator-Union{Tuple{M}, Tuple{M,NamedTuple}} where M<:Model","page":"API","title":"MLJ.iterator","text":"iterator(model::Model, param_iterators::NamedTuple)\n\nIterator over all models of type typeof(model) defined by param_iterators.\n\nEach name in the nested :name => value pairs of param_iterators should be the name of a (possibly nested) field of model; and each element of flat_values(param_iterators) (the corresponding final values) is an iterator over values of one of those fields.\n\nSee also iterator and params.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.learning_curve!-Tuple{Machine{#s74} where #s74<:Supervised}","page":"API","title":"MLJ.learning_curve!","text":"curve = learning_curve!(mach; resolution=30, resampling=Holdout(), measure=rms, operation=predict, nested_range=nothing, n=1)\n\nGiven a supervised machine mach, returns a named tuple of objects needed to generate a plot of performance measurements, as a function of the single hyperparameter specified in nested_range. The tuple curve has the following keys: :parameter_name, :parameter_scale, :parameter_values, :measurements.\n\nFor n not equal to 1, multiple curves are computed, and the value of curve.measurements is an array, one column for each run. This is useful in the case of models with indeterminate fit-results, such as a random forest.\n\nX, y = datanow()\natom = RidgeRegressor()\nensemble = EnsembleModel(atom=atom)\nmach = machine(ensemble, X, y)\nr_lambda = range(atom, :lambda, lower=0.1, upper=100, scale=:log10)\ncurve = MLJ.learning_curve!(mach; nested_range=(atom=(lambda=r_lambda,),))\nusing Plots\nplot(curve.parameter_values, curve.measurements, xlab=curve.parameter_name, xscale=curve.parameter_scale)\n\nSmart fitting applies. For example, if the model is an ensemble model, and the hyperparemeter parameter is n, then atomic models are progressively added to the ensemble, not recomputed from scratch for each new value of n.\n\natom.lambda=1.0\nr_n = range(ensemble, :n, lower=2, upper=150)\ncurves = MLJ.learning_curve!(mach; nested_range=(n=r_n,), verbosity=3, n=5)\nplot(curves.parameter_values, curves.measurements, xlab=curves.parameter_name)\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.rmsp-Tuple{AbstractArray{#s12,1} where #s12<:Real,Any}","page":"API","title":"MLJ.rmsp","text":"Root mean squared percentage loss \n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.sources-Tuple{MLJ.Source}","page":"API","title":"MLJ.sources","text":"sources(N)\n\nReturn a list of all ultimate sources of  a node N. \n\nSee also: node, source\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.info-Tuple{String}","page":"API","title":"MLJBase.info","text":"info(model, pkg=nothing)\n\nReturn the dictionary of metadata associated with model::String. If more than one package implements model then pkg::String will need to be specified.\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.fit!-Tuple{MLJ.AbstractMachine}","page":"API","title":"StatsBase.fit!","text":"fit!(mach::Machine; rows=nothing, verbosity=1)\n\nTrain the machine mach using the algorithm and hyperparameters specified by mach.model, using those rows of the wrapped data having indices in rows.\n\nfit!(mach::NodalMachine; rows=nothing, verbosity=1)\n\nA nodal machine is trained in the same way as a regular machine with one difference: Instead of training the model on the wrapped data indexed on rows, it is trained on the wrapped nodes called on rows, with calling being a recursive operation on nodes within a learning network.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.@curve-Tuple{Any,Any,Any}","page":"API","title":"MLJ.@curve","text":"@curve\n\nThe code,\n\n@curve var range code\n\nevaluates code, replacing appearances of var therein with each value in range. The range and corresponding evaluations are returned as a tuple of arrays. For example,\n\n@curve  x 1:3 (x^2 + 1)\n\nevaluates to\n\n([1,2,3], [2, 5, 10])\n\nThis is convenient for plotting functions using, eg, the Plots package:\n\nplot(@curve x 1:3 (x^2 + 1))\n\nA macro @pcurve parallelizes the same behaviour.  A two-variable implementation is also available, operating as in the following example:\n\njulia> @curve x [1,2,3] y [7,8] (x + y)\n([1,2,3],[7 8],[8.0 9.0; 9.0 10.0; 10.0 11.0])\n\njulia> ans[3]\n3×2 Array{Float64,2}:\n  8.0   9.0\n  9.0  10.0\n 10.0  11.0\n\nN.B. The second range is returned as a row vector for consistency with the output matrix. This is also helpful when plotting, as in:\n\njulia> u1, u2, A = @curve x range(0, stop=1, length=100) α [1,2,3] x^α\njulia> u2 = map(u2) do α \"α = \"*string(α) end\njulia> plot(u1, A, label=u2)\n\nwhich generates three superimposed plots - of the functions x, x^2 and x^3 - each labels with the exponents α = 1, 2, 3 in the legend.\n\n\n\n\n\n","category":"macro"},{"location":"api/#MLJ.SimpleDeterministicCompositeModel","page":"API","title":"MLJ.SimpleDeterministicCompositeModel","text":"SimpleDeterministicCompositeModel(;regressor=ConstantRegressor(), \n                          transformer=FeatureSelector())\n\nConstruct a composite model consisting of a transformer (Unsupervised model) followed by a Deterministic model. Mainly intended for internal testing .\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.copy","page":"API","title":"Base.copy","text":"copy(params::NamedTuple, values=nothing)\n\nReturn a copy of params with new values. That is, flat_values(copy(params, values)) == values is true, while the nested keys remain unchanged.\n\nIf values is not specified a deep copy is returned. \n\n\n\n\n\n","category":"function"},{"location":"api/#Base.merge!-Tuple{Array{T,1} where T,Array{T,1} where T}","page":"API","title":"Base.merge!","text":"merge!(tape1, tape2)\n\nIncrementally appends to tape1 all elements in tape2, excluding any element previously added (or any element of tape1 in its initial state).\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.range-Union{Tuple{D}, Tuple{MLJType,Symbol}} where D","page":"API","title":"Base.range","text":"r = range(model, :hyper; values=nothing)\n\nDefines a NominalRange object for a field hyper of model. Note that r is not directly iterable but iterator(r) iterates over values.\n\nr = range(model, :hyper; upper=nothing, lower=nothing, scale=:linear)\n\nDefines a NumericRange object for a field hyper of model.  Note that r is not directly iteratable but iterator(r, n) iterates over n values between lower and upper values, according to the specified scale. The supported scales are :linear, :log, :log10, :log2. Values for Integer types are rounded (with duplicate values removed, resulting in possibly less than n values).\n\nAlternatively, if a function f is provided as scale, then iterator(r, n) iterates over the values [f(x1), f(x2), ... , f(xn)], where x1, x2, ..., xn are linearly spaced between lower and upper.\n\nSee also: iterator\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.flat_keys-Tuple{Pair{Symbol,B} where B}","page":"API","title":"MLJ.flat_keys","text":" flat_keys(params::NamedTuple)\n\nUse dot-concatentation to express each possibly nested key of params in string form.\n\nExample\n\njulia> flat_keys((A=(x=2, y=3), B=9)))\n[\"A.x\", \"A.y\", \"B\"]\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.get_type-Tuple{Any,Symbol}","page":"API","title":"MLJ.get_type","text":"get_type(T, field::Symbol)\n\nReturns the type of the field field of DataType T. Not a type-stable function.  \n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.scale-Tuple{MLJ.NominalRange}","page":"API","title":"MLJ.scale","text":"MLJ.scale(r::ParamRange)\n\nReturn the scale associated with the ParamRange object r. The possible return values are: :none (for a NominalRange), :linear, :log, :log10, :log2, or :custom (if r.scale is function).\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.unwind-Tuple","page":"API","title":"MLJ.unwind","text":"unwind(iterators...)\n\nRepresent all possible combinations of values generated by iterators as rows of a matrix A. In more detail, A has one column for each iterator in iterators and one row for each distinct possible combination of values taken on by the iterators. Elements in the first column cycle fastest, those in the last clolumn slowest. \n\nExample\n\njulia> iterators = ([1, 2], [\"a\",\"b\"], [\"x\", \"y\", \"z\"]);\njulia> MLJ.unwind(iterators...)\n12×3 Array{Any,2}:\n 1  \"a\"  \"x\"\n 2  \"a\"  \"x\"\n 1  \"b\"  \"x\"\n 2  \"b\"  \"x\"\n 1  \"a\"  \"y\"\n 2  \"a\"  \"y\"\n 1  \"b\"  \"y\"\n 2  \"b\"  \"y\"\n 1  \"a\"  \"z\"\n 2  \"a\"  \"z\"\n 1  \"b\"  \"z\"\n 2  \"b\"  \"z\"\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.StratifiedKFold","page":"API","title":"MLJBase.StratifiedKFold","text":"StratifiedKFold(strata,k)\n\nStruct for StratifiedKFold provide strata's and number of partitions(k) and simply collect the object for the indices.  Taken from MLBase (https://github.com/JuliaStats/MLBase.jl).\n\n\n\n\n\n","category":"type"},{"location":"api/#MLJBase.SupervisedTask-Tuple{Any,Any}","page":"API","title":"MLJBase.SupervisedTask","text":"task = SupervisedTask(X, y; is_probabilistic=nothing, input_is_multivariate=true, target_is_multivariate=false, verbosity=1)\n\nConstruct a supervised learning task with input features X and target y. Both X and y must be tables or vectors, according to whether they are multivariate or univariate. Table rows must correspond to patterns and columns to features. The boolean keyword argument is_probabilistic must be specified.\n\ntask = SupervisedTask(data=nothing, is_probabilistic=nothing, target=nothing, ignore=Symbol[], input_is_multivariate=true, verbosity)\n\nConstruct a supervised learning task with input features X and target y, where y is the column vector from data named target (if this is a single symbol) or, a table whose columns are those named in target (if this is vector); X consists of all remaining columns of data not named in ignore.\n\nX, y = task()\n\nReturns the input X and target y of the task, also available as task.X and task.y.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.UnivariateNominal","page":"API","title":"MLJBase.UnivariateNominal","text":"UnivariateNominal(prob_given_level)\n\nA discrete univariate distribution whose finite support is the set of keys of the provided dictionary, prob_given_level. The dictionary values specify the corresponding probabilities, which must be nonnegative and sum to one.\n\nUnivariateNominal(levels, p)\n\nA discrete univariate distribution whose finite support is the elements of the vector levels, and whose corresponding probabilities are elements of the vector p.\n\nlevels(d::UnivariateNominal)\n\nReturn the levels of d.\n\nd = UnivariateNominal([\"yes\", \"no\", \"maybe\"], [0.1, 0.2, 0.7])\npdf(d, \"no\") # 0.2\nmode(d) # \"maybe\"\nrand(d, 5) # [\"maybe\", \"no\", \"maybe\", \"maybe\", \"no\"]\nd = fit(UnivariateNominal, [\"maybe\", \"no\", \"maybe\", \"yes\"])\npdf(d, \"maybe\") ≈ 0.5 # true\nlevels(d) # [\"yes\", \"no\", \"maybe\"]\n\nIf v is a CategoricalVector then fit(UnivariateNominal, v) includes all levels in pool of v in its support, assigning unseen levels probability zero.\n\n\n\n\n\n","category":"type"},{"location":"api/#MLJBase.UnsupervisedTask-Tuple{}","page":"API","title":"MLJBase.UnsupervisedTask","text":"task = UnsupervisedTask(data=nothing, ignore=Symbol[], input_is_multivariate=true, verbosity=1)\n\nConstruct an unsupervised learning task with given input data, which should be a table or, in the case of univariate inputs, a single vector. \n\nRows of data must correspond to patterns and columns to features. Columns in data whose names appear in ignore are ignored.\n\nX = task()\n\nReturn the input data in form to be used in models.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.column_scitypes_as_tuple-Tuple{Any}","page":"API","title":"MLJBase.column_scitypes_as_tuple","text":"column_scitypes_as_tuple_type(X)\n\nReturns Tuple{T1, T2, ..., Tn} where Tj is the union of scitypes of elements in the jth column of X. Here X is any table, sparse table, or abstract matrix.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.container_type-Tuple{Any}","page":"API","title":"MLJBase.container_type","text":"container_type(X)\n\nReturn :table, :sparse, or :other, according to whether X is a supported table format, a supported sparse table format, or something else.\n\nThe first two formats, together abstract vectors, support the MLJBase accessor methods selectrows, selectcols, select, nrows, schema, and union_scitypes.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.datanow-Tuple{}","page":"API","title":"MLJBase.datanow","text":"Get some supervised data now!!\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.fitresult_type-Tuple{Type{#s37} where #s37<:Supervised}","page":"API","title":"MLJBase.fitresult_type","text":"MLJBase.fitresult_type(m)\n\nReturns the fitresult type of any supervised model (or model type) m, as declared in the model mutable struct declaration.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.load_ames-Tuple{}","page":"API","title":"MLJBase.load_ames","text":"Load the full version of the well-known Ames Housing task.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.load_boston-Tuple{}","page":"API","title":"MLJBase.load_boston","text":"Load a well-known public regression dataset with nominal features.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.load_crabs-Tuple{}","page":"API","title":"MLJBase.load_crabs","text":"Load a well-known crab classification dataset with nominal features.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.load_iris-Tuple{}","page":"API","title":"MLJBase.load_iris","text":"Load a well-known public classification task with nominal features.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.load_reduced_ames-Tuple{}","page":"API","title":"MLJBase.load_reduced_ames","text":"Load a reduced version of the well-known Ames Housing task, having six numerical and six categorical features.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.matrix-Tuple{Any}","page":"API","title":"MLJBase.matrix","text":"MLJBase.matrix(X)\n\nConvert a table source X into an Matrix; or, if X is a AbstractMatrix, return X. Optimized for column-based sources.\n\nIf instead X is a sparse table, then a SparseMatrixCSC object is returned. The integer relabelling of column names follows the lexicographic ordering (as indicated by schema(X).names).\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.nrows-Tuple{Any}","page":"API","title":"MLJBase.nrows","text":"nrows(X)\n\nReturn the number of rows in a table, sparse table, or abstract vector.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.params-Tuple{Any}","page":"API","title":"MLJBase.params","text":"params(m)\n\nRecursively convert any object of subtype MLJType into a named tuple, keyed on the fields of m. The named tuple is possibly nested because params is recursively applied to the field values, which themselves might be MLJType objects. \n\nUsed, in particluar, in the case that m is a model, to inspect its nested hyperparameters:\n\njulia> params(EnsembleModel(atom=ConstantClassifier()))\n(atom = (target_type = Bool,),\n weights = Float64[],\n bagging_fraction = 0.8,\n rng_seed = 0,\n n = 100,\n parallel = true,)\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.partition-Tuple{AbstractArray{Int64,1},Vararg{Any,N} where N}","page":"API","title":"MLJBase.partition","text":"partition(rows::AbstractVector{Int}, fractions...; shuffle=false)\n\nSplits the vector rows into a tuple of vectors whose lengths are given by the corresponding fractions of length(rows). The last fraction is not provided, as it is inferred from the preceding ones. So, for example,\n\njulia> partition(1:1000, 0.2, 0.7)\n(1:200, 201:900, 901:1000)\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.schema-Tuple{Any}","page":"API","title":"MLJBase.schema","text":"schema(X)\n\nReturns a struct with properties names, types with the obvious meanings. Here X is any table or sparse table.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.scitype-Tuple{Any}","page":"API","title":"MLJBase.scitype","text":"scitype(x)\n\nReturn the scientific type for scalar values that object x can represent. Returns the type Other if x cannot represent scalar data.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.select-Tuple{Any,Any,Any}","page":"API","title":"MLJBase.select","text":"select(X, r, c)\n\nSelect element of a table or sparse table at row r and column c. In the case of sparse data where the key (r, c), zero or missing is returned, depending on the value type.\n\nSee also: selectrows, selectcols\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.selectcols-Tuple{Any,Any}","page":"API","title":"MLJBase.selectcols","text":"selectcols(X, c)\n\nSelect single or multiple columns from any table or sparse table X. If c is an abstract vector of integers or symbols, then the object returned is a table of the preferred sink type of typeof(X). If c is a single integer or column, then a Vector or CategoricalVector is returned.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.selectrows-Tuple{Any,Any}","page":"API","title":"MLJBase.selectrows","text":"selectrows(X, r)\n\nSelect single or multiple rows from any table, sparse table, or abstract vector X.  If X is tabular, the object returned is a table of the preferred sink type of typeof(X), even a single row is selected.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.table-Tuple{NamedTuple}","page":"API","title":"MLJBase.table","text":"MLJBase.table(cols; prototype=cols)\n\nConvert a named tuple of vectors cols, into a table. The table type returned is the \"preferred sink type\" for prototype (see the Tables.jl documentation). \n\nMLJBase.table(X::AbstractMatrix; names=nothing, prototype=nothing)\n\nConvert an abstract matrix X into a table with names (a tuple of symbols) as column names, or with labels (:x1, :x2, ..., :xn) where n=size(X, 2), if names is not specified.  If prototype=nothing, then a named tuple of vectors is returned.\n\nEquivalent to table(cols, prototype=prototype) where cols is the named tuple of columns of X, with keys(cols) = names.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.union_scitypes-Tuple{Any}","page":"API","title":"MLJBase.union_scitypes","text":"union_scitypes(X)\n\nReturn the union over all elements x of X of scitype(x). Here X can be any table, sparse table, or abstract arrray.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.@constant-Tuple{Any}","page":"API","title":"MLJBase.@constant","text":"@constant x = value\n\nEquivalent to const x = value but registers the binding thus:\n\nMLJBase.HANDLE_GIVEN_ID[objectid(value)] = :x\n\nRegistered objects get displayed using the variable name to which it was bound in calls to show(x), etc.\n\nWARNING: As with any const declaration, binding x to new value of the same type is not prevented and the registration will not be updated.\n\n\n\n\n\n","category":"macro"},{"location":"api/#MLJBase.@more-Tuple{}","page":"API","title":"MLJBase.@more","text":"@more\n\nEntered at the REPL, equivalent to show(ans, 100). Use to get a recursive description of all fields of the last REPL value.\n\n\n\n\n\n","category":"macro"},{"location":"api/#MLJBase._cummulative-Union{Tuple{UnivariateNominal{L,T}}, Tuple{T}, Tuple{L}} where T<:Real where L","page":"API","title":"MLJBase._cummulative","text":"_cummulative(d::UnivariateNominal)\n\nReturn the cummulative probability vector [0, ..., 1] for the distribution d, using whatever ordering is used in the dictionary d.prob_given_level. Used only for to implement random sampling from d.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase._rand-Tuple{Any}","page":"API","title":"MLJBase._rand","text":"rand(pcummulative)\n\nRandomly sample the distribution with discrete support 1:n which has cummulative probability vector p_cummulative=[0, ..., 1] (of length n+1). Does not check the first and last elements of p_cummulative but does not use them either. \n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase._recursive_show-Tuple{IO,MLJType,Any,Any}","page":"API","title":"MLJBase._recursive_show","text":"_recursive_show(stream, object, current_depth, depth)\n\nGenerate a table of the field values of the MLJType object, dislaying each value by calling the method _show on it. The behaviour of _show(stream, f) is as follows:\n\nIf f is itself a MLJType object, then its short form is shown\n\nand _recursive_show generates as separate table for each of its field values (and so on, up to a depth of argument depth).\n\nOtherwise f is displayed as \"(omitted T)\" where T = typeof(f),\n\nunless istoobig(f) is false (the istoobig fall-back for arbitrary types being true). In the latter case, the long (ie, MIME\"plain/text\") form of f is shown. To override this behaviour, overload the _show method for the type in question. \n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.abbreviated-Tuple{Any}","page":"API","title":"MLJBase.abbreviated","text":"to display abbreviated versions of integers\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.handle-Tuple{Any}","page":"API","title":"MLJBase.handle","text":"return abbreviated object id (as string)  or it's registered handle (as string) if this exists\n\n\n\n\n\n","category":"method"},{"location":"api/#Index-1","page":"API","title":"Index","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"","category":"page"},{"location":"frequently_asked_questions/#Frequently-Asked-Questions-1","page":"FAQ","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"frequently_asked_questions/#Julia-already-has-a-great-machine-learning-toolbox,-ScitkitLearn.jl.-Why-MLJ?-1","page":"FAQ","title":"Julia already has a great machine learning toolbox, ScitkitLearn.jl. Why MLJ?","text":"","category":"section"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"An alternative machine learning toolbox for Julia users is ScikitLearn.jl. Initially intended as a Julia wrapper for the popular python library scikit-learn, ML algorithms written in Julia can also implement the ScikitLearn.jl API. Meta-algorithms (systematic tuning, pipelining, etc) remain python wrapped code, however.","category":"page"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"While ScitkiLearn.jl provides the Julia user with access to a mature and large library of machine learning models, the scikit-learn API on which it is modeled, dating back to 2007, is not likely to evolve significantly in the future. MLJ enjoys (or will enjoy) several features that should make it an attractive alternative in the longer term:","category":"page"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"One language. ScikitLearn.jl wraps python code, which in turn wraps C code for performance-critical routines. A Julia machine learning algorithm that implements the MLJ model interface is 100% Julia. Writing code in Julia is almost as fast as python and well-written Julia code runs almost as fast as C. Additionally, a single language design provides superior interoperability. For example, one can implement: (i) gradient-descent tuning of hyperparameters, using automatic differentiation libraries such as Flux.jl; and (ii) GPU performance boosts without major code refactoring, using CuArrays.jl.\nRegistry for model metadata. In ScikitLearn.jl the list of available models, as well as model metadata (whether a model handles categorical inputs, whether is can make probabilistic predictions, etc) must be gleaned from documentation. In MLJ, this information is more structured and is accessible to MLJ via an external model registry (without the models needing to be loaded). This forms the basis of a \"task\" interface and facilitates model composition (see below).\nTask interface. Once the MLJ user specifies a \"task\" (e.g., \"make probabilistic predictions of home value, based on features x, y, z\") then MLJ can automatically search for models matching that task, assisting in systematic benchmarking and model selection.\nFlexible API for model composition. Pipelines in scikit-learn are more of an afterthought than an integral part of the original design. By contrast, MLJ's user-interaction API was predicated on the requirements of a flexible \"learning network\" API, one that allows models to be connected in essentially arbitrary ways (including target transforming and inverse-transforming). Networks can be built and tested in stages before being exported as first-class stand-alone models. Networks feature \"smart\" training (only necessary components are retrained after parameter changes) and will eventually be trainable using a DAG scheduler. With the help of Julia's meta-programming features, constructing common architectures, such as linear pipelines and stacks, will be one-line operations.\nClean probabilistic API. The scikit-learn API does not specify a universal standard for the form of probabilistic predictions. By fixing a probabilistic API along the lines of the skpro project, MLJ aims to improve support for Bayesian statistics and probabilistic graphical models.\nUniversal adoption of categorical data types. Python's scientific array library NumPy has no dedicated data type for representing categorical data (i.e., no type that tracks the pool of all possible classes). Generally scikit-learn models deal with this by requiring data to be relabeled as integers. However, the naive user trains a model on relabeled categorical data only to discover that evaluation on a test set crashes his code because a categorical feature takes on a value not observed in training. MLJ mitigates such issues by insisting on the use of categorical data types, and by insisting that MLJ model implementations preserve the class pools. If, for example, a training target contains classes in the pool that do not actually appear in the training set, a probabilistic prediction will nevertheless predict a distribution whose support includes the missing class, but which is appropriately weighted with probability zero.","category":"page"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"Finally, we note that there is a project underway to implement (some of) the ScikitLearn.jl models as MLJ models, as an temporary expedient.","category":"page"},{"location":"NEWS/#MLJ-News-1","page":"MLJ News","title":"MLJ News","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Development news for MLJ and its satellite packages,  MLJBase, MLJRegistry and MLJModels","category":"page"},{"location":"NEWS/#unversioned-commits-8-April-2019-(some-time-after-20:00-GMT)-1","page":"MLJ News","title":"unversioned commits 8 April 2019 (some time after 20:00 GMT)","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Addition of XGBoost models XGBoostRegressor, XGBoostClassifier and XGBoostCount. Resolves #65.\nDocumentation reorganized as GitHub pages. Includes some additions but still a work in progress.","category":"page"},{"location":"NEWS/#unversioned-commits-1-March-2019-(some-time-after-03:50-GMT)-1","page":"MLJ News","title":"unversioned commits 1 March 2019 (some time after 03:50 GMT)","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Addition of \"scientific type\" hierarchy, including Continuous, Discrete, Multiclass, and Other subtypes of Found (to complement Missing). See Getting Started for more one this.  Resolves: #86\nRevamp of model traits to take advantage of scientific types, with output_kind replaced with target_scitype, input_kind replaced with input_scitype. Also, output_quantity dropped, input_quantity replaced with Bool-valued input_is_multivariate, and is_pure_julia made Bool-valued. Trait definitions in all model implementations and effected meta-algorithms have been updated. Related: #81\nSubstantial update of the core guide Adding New Models to reflect above changes and in response to new model implementer queries. Some design \"decisions\" regarding multivariate targets now explict there.\nthe order the y and yhat arguments of measures (aka loss functions) have been reversed. Progress on: #91\nUpdate of Standardizer and OneHotEncoder to mesh with new scitypes.\nNew improved task constructors infer task metadata from data scitypes. This brings us close to a simple implementation of basic task-model matching. Query the doc-strings for SupervisedTask and UnsupervisedTask for details.  Machines can now dispatch on tasks instead of X and y. A task, task, is now callable: task() returns (X, y) for supervised models, and X for unsupervised models.  Progress on:  #86\nthe data in the load_ames() test task has been replaced by the full data set, and load_reduced_ames() now loads a reduced set.","category":"page"}]
}
