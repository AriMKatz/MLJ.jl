var documenterSearchIndex = {"docs":
[{"location":"getting_started/#Getting-Started-1","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/#Basic-supervised-training-and-testing-1","page":"Getting Started","title":"Basic supervised training and testing","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> using MLJ\njulia> using RDatasets\njulia> iris = dataset(\"datasets\", \"iris\"); # a DataFrame","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"In MLJ one can either wrap data for supervised learning in a formal task (see Working with Tasks), or work directly with the data, split into its input and target parts:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> const X = iris[:, 1:4];\njulia> const y = iris[:, 5];","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"A model is a container for hyperparameters:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> @load DecisionTreeClassifier\nimport MLJModels ✔\nimport DecisionTree ✔\nimport MLJModels.DecisionTree_.DecisionTreeClassifier ✔\n\njulia> tree_model = DecisionTreeClassifier(target_type=String, max_depth=2)\nDecisionTreeClassifier(target_type = String,\n                       pruning_purity = 1.0,\n                       max_depth = 2,\n                       min_samples_leaf = 1,\n                       min_samples_split = 2,\n                       min_purity_increase = 0.0,\n                       n_subfeatures = 0.0,\n                       display_depth = 5,\n                       post_prune = false,\n                       merge_purity_threshold = 0.9,) @ 1…72","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Wrapping the model in data creates a machine which will store training outcomes (called fit-results):","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> tree = machine(tree_model, X, y)\nMachine @ 5…78","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Training and testing on a hold-out set:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> train, test = partition(eachindex(y), 0.7, shuffle=true); # 70:30 split\njulia> fit!(tree, rows=train)\njulia> yhat = predict(tree, X[test,:]);\njulia> misclassification_rate(yhat, y[test]);\n\n┌ Info: Training Machine{DecisionTreeClassifier{S…} @ 1…36.\n└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:68\n0.08888888888888889","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Or, in one line:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> evaluate!(tree, resampling=Holdout(fraction_train=0.7, shuffle=true), measure=misclassification_rate)\n0.08888888888888889","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Changing a hyperparameter and re-evaluating:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> tree_model.max_depth = 3\njulia> evaluate!(tree, resampling=Holdout(fraction_train=0.5, shuffle=true), measure=misclassification_rate)\n0.06666666666666667","category":"page"},{"location":"getting_started/#Next-steps-1","page":"Getting Started","title":"Next steps","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"To learn a little more about what MLJ can do, take the MLJ tour. Read the remainder of this document before considering more serious use of MLJ.","category":"page"},{"location":"getting_started/#Prerequisites-1","page":"Getting Started","title":"Prerequisites","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"MLJ assumes some familiarity with the CategoricalArrays.jl package, used for representing categorical data. For probabilistic predictors, a basic acquaintance with Distributions.jl is also assumed.","category":"page"},{"location":"getting_started/#Data-1","page":"Getting Started","title":"Data","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"While MLJ is data container agnostic it is very fussy about element types. The MLJ user should acquaint themselves with some basic assumptions about the form of data expected by MLJ, as outlined below. ","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Eventually task constructors will make the coercion of data into the requisite form more automated. The following remarks will be less critical to the casual user. At present, however, task constructors assume data is in the requisite form.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"In principle, anywhere a table is expected in MLJ (eg, X above) any tabular format supporting the Tables.jl interface is allowed. (At present our API is more restrictive; see this issue with Tables.jl. If your Tables.jl compatible format is not working in MLJ, please post an issue.) In particular, DataFrame, JuliaDB.IndexedTable and TypedTables.Table objects are supported, as are named tuples of equi-length vectors (\"column tables\" in Tables.jl parlance).","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"A single feature (such as the target y above) is expected to be a Vector or CategoricalVector, according to the scientific type of the data (see below). A multivariate target can be any table.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"On the other hand, the element types you use to represent your data has implicit consequences about how MLJ will interpret that data.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"To articulate MLJ's conventions about data representation, MLJ distinguishes between machine data types on the one hand (Float64, Bool, String, etc) and scientific data types on the other, represented by new Julia types: Continuous, Multiclass{N}, FiniteOrderedFactor{N}, and Count (unbounded ordered factor), with obvious interpretations. These types, which are part of a type hierarchy (see Scientific Data Types), are used by MLJ for dispatch, but have no corresponding instances.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Scientific types appear when querying model metadata, as in this example:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> info(\"DecisionTreeClassifier\")[:target_scitype]\n\nUnion{Multiclass,FiniteOrderedFactor}","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Basic data convention. The scientific type of data that a Julia object x can represent is defined by scitype(x). If scitype(x) == Other, then x cannot represent scalar data in MLJ.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"julia> (scitype(42), scitype(π), scitype(\"Julia\"))\n\n(Count, Continuous, MLJBase.Other)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"In particular, integers cannot be used to represent Multiclass or FiniteOrderedFactor data; these can be represented by an unordered or ordered CategoricalValue or CategoricalString:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"T scitype(x) for x::T\nMissing Missing\nReal Continuous\nInteger Count\nCategoricalValue Multiclass{N} where N = nlevels(x), provided x.pool.ordered == false\nCategoricalString Multiclass{N} where N = nlevels(x), provided x.pool.ordered == false\nCategoricalValue FiniteOrderedFactor{N} where N = nlevels(x), provided x.pool.ordered == true\nCategoricalString FiniteOrderedFactor{N} where N = nlevels(x) provided x.pool.ordered == true\nInteger Count","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Here nlevels(x) = length(levels(x.pool)).","category":"page"},{"location":"scientific_data_types/#Scientific-Data-Types-1","page":"Scientific Data Types","title":"Scientific Data Types","text":"","category":"section"},{"location":"scientific_data_types/#","page":"Scientific Data Types","title":"Scientific Data Types","text":"\"Scientific\" data types are formalized in MLJ with the addition of new Julia type hierarchy, rooted in an abstract type called Found. A a scientific type is defined to be any subtype of Union{Missing, Found}:","category":"page"},{"location":"scientific_data_types/#","page":"Scientific Data Types","title":"Scientific Data Types","text":"Continuous <: Found \nDiscrete <: Found\n    Multiclass{N} <: Discrete\n    OrderedFactor <: Discrete\n\t    FiniteOrderedFactor{N} <: OrderedFactor \n\t    Count <: OrderedFactor\nOther <: Found","category":"page"},{"location":"scientific_data_types/#","page":"Scientific Data Types","title":"Scientific Data Types","text":"Note that Multiclass{2} has the alias Binary.","category":"page"},{"location":"scientific_data_types/#","page":"Scientific Data Types","title":"Scientific Data Types","text":"These types are used in MLJ purely for dispatch; they are never instantiated.","category":"page"},{"location":"scientific_data_types/#","page":"Scientific Data Types","title":"Scientific Data Types","text":"A given scientific type may have multiple machine type representations (eg, Float64 and Float32 both represent Continuous). However, MLJ adopts the simplifying (but not universally adopted) convention that the same machine data type cannot be used to represent multiple scientific types. We may accordingly express MLJ's convention on scientific type representations using a function, scitype, which associates to every Julia object a corresponding subtype of Found:","category":"page"},{"location":"scientific_data_types/#","page":"Scientific Data Types","title":"Scientific Data Types","text":"Basic data convention Scalar data with intended scientific type K can be represented in MLJ by a Julia object x if scitype(x) = K.","category":"page"},{"location":"scientific_data_types/#","page":"Scientific Data Types","title":"Scientific Data Types","text":"So, for example, you may check that scitype(4.56) = Continuous and scitype(4) = Count. In particular, you cannot use integers (which include booleans) to represent (nominal) multiclass data. You should use a CategoricalString or CategoricalValue type (automatic if your data appears in a CategoricalVector).","category":"page"},{"location":"scientific_data_types/#","page":"Scientific Data Types","title":"Scientific Data Types","text":"In fact, if scitype(x) = K then scitype(y) = K for all y with typeof(y)=typeof(x) unless, x and y are instances of CategoricalValue or CategoricalString. In this latter case, the assertion continues to hold if and only if x and y have the same ordered flag and the same number of levels.","category":"page"},{"location":"scientific_data_types/#","page":"Scientific Data Types","title":"Scientific Data Types","text":"Here, approximately, is the definition of scitype on scalar types:","category":"page"},{"location":"scientific_data_types/#","page":"Scientific Data Types","title":"Scientific Data Types","text":"nlevels(x) = length(levels(x.pool))\n\nscitype(::Any) = Other # fallback\n\nscitype(::Missing) = Missing\nscitype(::Real) = Continuous\nscitype(::Integer) = Count\n\nscitype(c::CategoricalValue) =\n    c.pool.ordered ? FiniteOrderedFactor{nlevels(c)} : Multiclass{nlevels(c)}\n\nscitype(c::CategoricalString) =\n    c.pool.ordered ? FiniteOrderedFactor{nlevels(c)} : Multiclass{nlevels(c)}","category":"page"},{"location":"scientific_data_types/#","page":"Scientific Data Types","title":"Scientific Data Types","text":"Additionally, we may compute the union of element scitypes for vectors, tables and sparse tables, using the methods union_scitypes and column_scitypes_as_tuple.","category":"page"},{"location":"adding_new_models/#Adding-New-Models-1","page":"Adding New Models","title":"Adding New Models","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"This guide outlines the specification of the MLJ model interface and provides guidelines for implementing the interface for models defined in external packages. For sample implementations, see MLJModels/src.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"The machine learning tools provided by MLJ can be applied to the models in any package that imports the package MLJBase and implements the API defined there, as outlined in detail below. For a quick-and-dirty implementation of user-defined models see The Simplified Model API.  To make new models available to all MLJ users, see Where to place code implementing new models.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"It is assumed the reader has read Getting Started. To implement the API described here, some familiarity with the following packages is also helpful:","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Distributions.jl","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"(for probabilistic predictions)","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"CategoricalArrays.jl","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"(essential if you are implementing a model handling data of Multiclass or FiniteOrderedFactor scitype)","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Tables.jl (if you're","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"algorithm needs input data in a novel format).","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"<!– As a temporary measure, –> <!– the MLJ package also implements the MLJ interface for some –> <!– non-compliant packages, using lazily loaded modules (\"glue code\") –> <!– residing in –> <!– src/interfaces –> <!– of the MLJ.jl repository. A checklist for adding models in this latter –> <!– way is given at the end; a template is given here: –> <!– \"src/interfaces/DecisionTree.jl\". –>","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"In MLJ, the basic interface exposed to the user, built atop the model interface described here, is the machine interface. After a first reading of this document, the reader may wish to refer to MLJ Internals for context.","category":"page"},{"location":"adding_new_models/#Overview-1","page":"Adding New Models","title":"Overview","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"A model is an object storing hyperparameters associated with some machine learning algorithm, where \"learning algorithm\" is broadly interpreted.  In MLJ, hyperparameters include configuration parameters, like the number of threads, and special instructions, such as \"compute feature rankings\", which may or may not affect the final learning outcome.  However, the logging level (verbosity below) is excluded.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"The name of the Julia type associated with a model indicates the associated algorithm (e.g., DecisionTreeClassifier). The outcome of training a learning algorithm is here called a fit-result. For ordinary multilinear regression, for example, this would be the coefficients and intercept. For a general supervised model, it is the (generally minimal) information needed to make new predictions.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"The ultimate supertype of all models is MLJBase.Model, which has two abstract subtypes:","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"abstract type Supervised{R} <: Model end\nabstract type Unsupervised <: Model end","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Here the parameter R refers to a fit-result type. By declaring a model to be a subtype of MLJBase.Supervised{R} you guarantee the fit-result to be of type R and, if R is concrete, one may improve the performance of homogeneous ensembles of the model (as defined by the built-in MLJ EnsembleModel wrapper). There is no abstract type for fit-results because these types are generally declared outside of MLJBase.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"The necessity to declare the fitresult type R may disappear in the future (issue #93).","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Supervised models are further divided according to whether they are able to furnish probabilistic predictions of the target(s) (which they will do so by default) or directly predict \"point\" estimates, for each new input pattern:","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"abstract type Probabilistic{R} <: Supervised{R} end\nabstract type Deterministic{R} <: Supervised{R} end","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Further division of model types is realized through trait declarations.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Associated with every concrete subtype of Model there must be a fit method, which implements the associated algorithm to produce the fit-result. Additionally, every Supervised model has a predict method, while Unsupervised models must have a transform method. More generally, methods such as these, that are dispatched on a model instance and a fit-result (plus other data), are called operations. Probabilistic supervised models optionally implement a predict_mode operation (in the case of classifiers) or a predict_mean and/or predict_median operations (in the case of regressors) overriding obvious fallbacks provided by MLJBase. Unsupervised models may implement an inverse_transform operation.","category":"page"},{"location":"adding_new_models/#New-model-type-declarations-and-optional-clean!-method-1","page":"Adding New Models","title":"New model type declarations and optional clean! method","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Here is an example of a concrete supervised model type declaration, made after defining an appropriate fit-result type (an optional step):","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"import MLJ\n\nstruct LinearFitResult{F<:AbstractFloat} <: MLJBase.MLJType\n    coefficients::Vector{F}\n    bias::F\nend\n\nmutable struct RidgeRegressor{F} <: MLJBase.Deterministic{LinearFitResult{F}}\n    target_type::Type{F}\n    lambda::Float64\nend","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Note. Model fields may be of any type except NamedTuple.  (This is because named tuples are used to represented the nested hyperparameters  of composite models (models that have other models as fields.)","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Models (which are mutable) should not be given internal constructors. It is recommended that they be given an external lazy keyword constructor of the same name. This constructor defines default values for every field, and optionally corrects invalid field values by calling a clean! method (whose fallback returns an empty message string):","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"function MLJ.clean!(model::RidgeRegressor)\n    warning = \"\"\n    if model.lambda < 0\n        warning *= \"Need lambda ≥ 0. Resetting lambda=0. \"\n        model.lambda = 0\n    end\n    return warning\nend\n\n# keyword constructor\nfunction RidgeRegressor(; target_type=Float64, lambda=0.0)\n\n    model = RidgeRegressor(target_type, lambda)\n\n    message = MLJBase.clean!(model)\n    isempty(message) || @warn message\n\n    return model\n    \nend","category":"page"},{"location":"adding_new_models/#The-model-API-for-supervised-models-1","page":"Adding New Models","title":"The model API for supervised models","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Below we describe the compulsory and optional methods to be specified for each concrete type SomeSupervisedModelType{R} <: MLJBase.Supervised{R}. ","category":"page"},{"location":"adding_new_models/#The-form-of-data-for-fitting-and-predicting-1","page":"Adding New Models","title":"The form of data for fitting and predicting","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"In every circumstance, the argument X passed to the fit method described below, and the argument Xnew of the predict method, will be some table supporting the Tables.jl API. The interface implementer can control the scientific type of data appearing in X with an appropriate input_scitype declaration (see Trait Declarations below). If the core algorithm requires data in a different or more specific form, then fit will need to coerce the table into the form desired. To this end, MLJ provides the convenience method MLJBase.matrix; MLJBase.matrix(Xtable) is a two-dimensional Array{T} where T is the tightest common type of elements of Xtable, and Xtable is any table.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Tables.jl has recently added a matrix method as well.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Other convenience methods provided by MLJBase for handling tabular data are: selectrows, selectcols, select, schema (for extracting the size, names and eltypes of a table) and table (for materializing an abstract matrix, or named tuple of vectors, as a table matching a given prototype). Query the doc-strings for details.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Note that generally the same type coercions applied to X by fit will need to be applied by predict to Xnew. ","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Important convention It is to be understood that the columns of the table X correspond to features and the rows to patterns.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"The form of the target data y passed to fit is constrained by the target_scitype trait declaration. All elements of y will satisfy scitype(y) <: target_scitype(SomeSupervisedModelType). Furthermore, for univariate targets, y is always a Vector or CategoricalVector, according to the value of the trait:","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"target_scitype(SomeSupervisedModelType) type of y tightest known supertype of eltype(y)\nContinuous Vector Real\n<: Multiclass CategoricalVector Union{CategoricalString, CategoricalValue}\n<: FiniteOrderedFactor CategoricalVector Union{CategoricalString, CategoricalValue}\nCount Vector Integer","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"So, for example, if your model is a binary classifier, you declare","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"target_scitype(SomeSupervisedModelType)=Multiclass{2}","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"If it can predict any number of classes, you might instead declare","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"target_scitype(SomeSupervisedModelType)=Union{Multiclass, FiniteOrderedFactor}","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"See also the table in Getting Started.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"For multivariate targets, y will be a table whose columns have the scitypes indicated in the Tuple type returned by target_scitype; for example, if you declare target_scitype(SomeSupervisedModelType) = Tuple{Continuous,Count}, then y will have two columns, the first with Real elements, the second with Integer elements.","category":"page"},{"location":"adding_new_models/#The-fit-method-1","page":"Adding New Models","title":"The fit method","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"A compulsory fit method returns three objects:","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"MLJBase.fit(model::SomeSupervisedModelType, verbosity::Int, X, y) -> fitresult, cache, report","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Note: The Int typing of verbosity cannot be omitted.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"fitresult::R is the fit-result in the sense above (which becomes an  argument for predict discussed below).\nreport is a (possibly empty) NamedTuple, for example,  report=(deviance=..., dof_residual=..., stderror=..., vcov=...).  Any training-related statistics, such as internal estimates of the  generalization error, and feature rankings, should be returned in  the report tuple. How, or if, these are generated should be  controlled by hyperparameters (the fields of model). Fitted  parameters, such as the coefficients of a linear model, do not go  in the report as they will be extractable from fitresult (and  accessible to MLJ through the fitted_params method, see below).","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"3.\tThe value of cache can be nothing, unless one is also defining    an update method (see below). The Julia type of cache is not    presently restricted.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"It is not necessary for fit to provide dimension checks or to call clean! on the model; MLJ will carry out such checks.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"The method fit should never alter hyperparameter values. If the package is able to suggest better hyperparameters, as a byproduct of training, return these in the report field.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"One should test that actual fit-results have the type declared in the model mutable struct declaration. To help with this, MLJBase.fitresult_type(m) returns the declared type, for any supervised model (or model type) m.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"The verbosity level (0 for silent) is for passing to learning algorithm itself. A fit method wrapping such an algorithm should generally avoid doing any of its own logging.","category":"page"},{"location":"adding_new_models/#The-fitted_params-method-1","page":"Adding New Models","title":"The fitted_params method","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"A fitted_params method may be optionally overloaded. It's purpose is to provide MLJ accesss to a user-friendly representation of the learned parameters of the model (as opposed to the hyperparameters). They must be extractable from fitresult.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"MLJBase.fitted_params(model::SomeSupervisedModelType, fitresult) -> friendly_fitresult::NamedTuple","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"For a linear model, for example, one might declare something like friendly_fitresult=(coefs=[...], bias=...).","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"The fallback is to return (fitresult=fitresult,).","category":"page"},{"location":"adding_new_models/#The-predict-method-1","page":"Adding New Models","title":"The predict method","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"A compulsory predict method has the form","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"MLJBase.predict(model::SomeSupervisedModelType, fitresult, Xnew) -> yhat","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Here Xnew is an any table whose entries satisfy the same scitype constraints as discussed for X above.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Prediction types for deterministic responses. In the case of Deterministic models, yhat must have the same form as the target y passed to the fit method (see above discussion on the form of data for fitting), with one exception: If predicting a Count, the prediction may be Continuous. For all models predicting a Multiclass or FiniteOrderedFactor, the categorical vectors returned by predict must have the levels in the categorical pool of the target data presented in training, even if not all levels appear in the training data or prediction itself. That is, we must have levels(yhat) == levels(y).","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Unfortunately, code not written with the preservation of categorical levels in mind poses special problems. To help with this, MLJ provides a utility CategoricalDecoder which can decode a CategoricalArray into a plain array, and re-encode a prediction with the original levels intact. The CategoricalDecoder object created during fit will need to be bundled with fitresult to make it available to predict during re-encoding. ","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"So, for example, if the core algorithm being wrapped by fit expects a nominal target yint of type Vector{Int64} then a fit method may look something like this:","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"function MLJBase.fit(model::SomeSupervisedModelType, verbosity, X, y)\n    decoder = MLJBase.CategoricalDecoder(y, Int64)\n    yint = transform(decoder, y)\n    core_fitresult = SomePackage.fit(X, yint, verbosity=verbosity)\n    fitresult = (decoder, core_fitresult)\n    cache = nothing\n    report = nothing\n    return fitresult, cache, report\nend","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"while a corresponding deterministic predict operation might look like this:","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"function MLJBase.predict(model::SomeSupervisedModelType, fitresult, Xnew)\n    decoder, core_fitresult = fitresult\n    yhat = SomePackage.predict(core_fitresult, Xnew)\n    return inverse_transform(decoder, yhat)\nend","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Query ?MLJBase.DecodeCategorical for more information.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"If you are coding a learning algorithm from scratch, rather than wrapping an existing one, conversions may be unnecessary. It may suffice to record the pool of y and bundle that with the fitresult for predict to append to the levels of its categorical output.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Prediction types for probabilistic responses. In the case of Probabilistic models with univariate targets, yhat must be a Vector whose elements are distributions (one distribution per row of Xnew).","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"A distribution is any instance of a subtype of Distributions.Distribution from the package Distributions.jl, or any instance of the additional types UnivariateNominal and MultivariateNominal defined in MLJBase.jl (or any other type D you define for which MLJBase.isdistribution(::D) = true, meaning Base.rand and Distributions.pdf are implemented, as well Distributions.mean/Distribution.median or Distributions.mode).","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Use UnivariateNominal for Probabilistic models predicting Multiclass or FiniteOrderedFactor targets. For example, suppose levels(y)=[\"yes\", \"no\", \"maybe\"] and set L=levels(y). Then, if the predicted probabilities for some input pattern are [0.1, 0.7, 0.2], respectively, then the prediction returned for that pattern will be UnivariateNominal(L, [0.1, 0.7, 0.2]). Query ?UnivariateNominal for more information.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"The predict method will need access to all levels in the pool of the target variable presented y presented for training, which consequently need to be encoded in the fitresult returned by fit. If a CategoricalDecoder object, decoder, has been bundled in fitresult, as in the deterministic example above, then the levels are given by levels(decoder). Levels not observed in the training data  (i.e., only in its pool) should be assigned probability zero.","category":"page"},{"location":"adding_new_models/#Trait-declarations-1","page":"Adding New Models","title":"Trait declarations","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"There are a number of recommended trait declarations for each model mutable structure SomeSupervisedModelType <: Supervised you define. Basic fitting, resampling and tuning in MLJ does not require these traits but some advanced MLJ meta-algorithms may require them now, or in the future. In particular, MLJ's models(::Task) method (matching models to user-specified tasks) can only identify models having a complete set of trait declarations. A full set of declarations is shown below for the DecisionTreeClassifier type (defined in the submodule DecisionTree_ of MLJModels):","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"MLJBase.load_path(::Type{<:DecisionTreeClassifier}) = \"MLJModels.DecisionTree_.DecisionTreeClassifier\" \nMLJBase.package_name(::Type{<:DecisionTreeClassifier}) = \"DecisionTree\"\nMLJBase.package_uuid(::Type{<:DecisionTreeClassifier}) = \"7806a523-6efd-50cb-b5f6-3fa6f1930dbb\"\nMLJBase.package_url(::Type{<:DecisionTreeClassifier}) = \"https://github.com/bensadeghi/DecisionTree.jl\"\nMLJBase.is_pure_julia(::Type{<:DecisionTreeClassifier}) = true\nMLJBase.input_is_multivariate(::Type{<:DecisionTreeClassifier}) = true\nMLJBase.input_scitypes(::Type{<:DecisionTreeClassifier}) = MLJBase.Continuous\nMLJBase.target_scitype(::Type{<:DecisionTreeClassifier}) = MLJBase.Multiclass","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Note that models predicting multivariate targets will need to need to have target_scitype return an appropriate Tuple type. ","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"For an explanation of Found and Other in the table below, see Scientific Types.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"method return type declarable return values default value\ntarget_scitype DataType subtype of Found or tuple of such types Union{Found,NTuple{<:Found}}\ninput_scitypes DataType subtype of Union{Missing,Found} Union{Missing,Found}\ninput_is_multivariate Bool true or false true\nis_pure_julia Bool true or false false\nload_path String unrestricted \"unknown\"\npackage_name String unrestricted \"unknown\"\npackage_uuid String unrestricted \"unknown\"\npackage_url String unrestricted \"unknown\"","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"You can test declarations of traits by calling info(SomeModelType).","category":"page"},{"location":"adding_new_models/#The-update!-method-1","page":"Adding New Models","title":"The update! method","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"An update method may be optionally overloaded to enable a call by MLJ to retrain a model (on the same training data) to avoid repeating computations unnecessarily.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"MLJBase.update(model::SomeSupervisedModelType, verbosity, old_fitresult, old_cache, X, y) -> fitresult, cache, report","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"If an MLJ Machine is being fit! and it is not the first time, then update is called instead of fit unless fit! has been called with new rows. However, MLJBase defines a fallback for update which just calls fit. For context, see MLJ Internals. ","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Learning networks wrapped as models constitute one use-case: One would like each component model to be retrained only when hyperparameter changes \"upstream\" make this necessary. In this case MLJ provides a fallback (specifically, the fallback is for any subtype of Supervised{Node}). A second important use-case is iterative models, where calls to increase the number of iterations only restarts the iterative procedure if other hyperparameters have also changed. For an example, see builtins/Ensembles.jl.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"In the event that the argument fitresult (returned by a preceding call to fit) is not sufficient for performing an update, the author can arrange for fit to output in its cache return value any additional information required, as this is also passed as an argument to the update method.","category":"page"},{"location":"adding_new_models/#Multivariate-models-1","page":"Adding New Models","title":"Multivariate models","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"TODO","category":"page"},{"location":"adding_new_models/#Unsupervised-models-1","page":"Adding New Models","title":"Unsupervised models","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"TODO","category":"page"},{"location":"adding_new_models/#Where-to-place-code-implementing-new-models-1","page":"Adding New Models","title":"Where to place code implementing new models","text":"","category":"section"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Note that different packages can implement models having the same name without causing conflicts, although an MLJ user cannot simultaneously load two such models.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"There are two options for making a new model implementation available to all MLJ users:","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Native implementations (preferred option). The implementation code lives in the same package that contains the learning algorithms implementing the interface. In this case, it is sufficient to open an issue at MLJRegistry requesting the package to be registered with MLJ. Registering a package allows the MLJ user to access its models' metadata and to selectively load them.\nExternal implementations (short-term alternative). The model implementation code is necessarily separate from the package SomePkg defining the learning algorithm being wrapped. In this case, the recommended procedure is to include the implementation code at MLJModels/src via a pull-request, and test code at MLJModels/test. Assuming SomePkg is the only package imported by the implementation code, one needs to: (i) register SomePkg at MLJRegistry as explained above; and (ii) add a corresponding @require line in the PR to MLJModels/src/MLJModels.jl to enable lazy-loading of that package by MLJ (following the pattern of existing additions). If other packages must be imported, add them to the MLJModels project file after checking they are not already there. If it is really necessary, packages can be also added to Project.toml for testing purposes.","category":"page"},{"location":"adding_new_models/#","page":"Adding New Models","title":"Adding New Models","text":"Additionally, one needs to ensure that the implementation code defines the package_name and load_path model traits appropriately, so that MLJ's @load macro can find the necessary code (see MLJModels/src for examples). The @load command can only be tested after registration. If changes are made, lodge an issue at MLJRegistry to make the changes available to MLJ.  ","category":"page"},{"location":"the_simplified_model_api/#The-Simplified-Model-API-1","page":"The Simplified Model API","title":"The Simplified Model API","text":"","category":"section"},{"location":"the_simplified_model_api/#","page":"The Simplified Model API","title":"The Simplified Model API","text":"To quickly implement a new supervised model in MLJ, it suffices to:","category":"page"},{"location":"the_simplified_model_api/#","page":"The Simplified Model API","title":"The Simplified Model API","text":"Define a mutable struct to store hyperparameters. This is either a subtype of Probabilistic{Any} or Deterministic{Any}, depending on whether probabilistic or ordinary point predictions are intended. This struct is the model.\nDefine a fit method, dispatched on the model, returning learned parameters, also known as the fit-result.\nDefine a predict method, dispatched on the model, and passed the fit-result, to return predictions on new patterns.","category":"page"},{"location":"the_simplified_model_api/#","page":"The Simplified Model API","title":"The Simplified Model API","text":"In the examples below, the training input X of fit, and the new input Xnew passed to predict, are tables implementing the Tables.jl interface. Each training target y is a Vector for regressors, and a  CategoricalVector for classifiers. The predicitions returned by predict have the same form as y for deterministic models, but are Vectors of distibutions for probabilistic models.","category":"page"},{"location":"the_simplified_model_api/#","page":"The Simplified Model API","title":"The Simplified Model API","text":"For your models to implement an optional update method, buy into the MLJ logging protocol, or report training statistics or other model-specific functionality, a fit method with a slightly different signature and output is required. To enable checks of the scientific type of data passed to your model by MLJ's meta-algorithms, one needs to implement additional traits. A clean! method can be defined to check that hyperparameter values are within normal ranges. For details, see Adding New Models.","category":"page"},{"location":"the_simplified_model_api/#A-simple-deterministic-regressor-1","page":"The Simplified Model API","title":"A simple deterministic regressor","text":"","category":"section"},{"location":"the_simplified_model_api/#","page":"The Simplified Model API","title":"The Simplified Model API","text":"Here's a quick-and-dirty implementation of a ridge regressor with no intercept:","category":"page"},{"location":"the_simplified_model_api/#","page":"The Simplified Model API","title":"The Simplified Model API","text":"import MLJBase\nusing LinearAlgebra\n\nmutable struct MyRegressor <: MLJBase.Deterministic{Any}\n    lambda::Float64\nend\n\n# fit returns coefficients minimizing a penalized rms loss function:\nfunction MLJBase.fit(model::MyRegressor, X, y)\n    x = MLJBase.matrix(X)                     # convert table to matrix\n    fitresult = (x'x - model.lambda*I)\\(x'y)  # the coefficients\n    return fitresult\nend\n\n# predict uses coefficients to make new prediction:\nMLJBase.predict(model::MyRegressor, fitresult, Xnew) = MLJBase.matrix(Xnew)fitresult","category":"page"},{"location":"the_simplified_model_api/#","page":"The Simplified Model API","title":"The Simplified Model API","text":"After loading this code, all MLJ's basic meta-algorithms can be applied to MyRegressor:","category":"page"},{"location":"the_simplified_model_api/#","page":"The Simplified Model API","title":"The Simplified Model API","text":"julia> using MLJ\njulia> task = load_boston()\njulia> model = MyRegressor(1.0)\njulia> regressor = machine(model, task)\njulia> evaluate!(regressor, resampling=CV(), measure=rms) |> mean\n7.434221318358656\n","category":"page"},{"location":"the_simplified_model_api/#A-simple-probabilistic-classifier-1","page":"The Simplified Model API","title":"A simple probabilistic classifier","text":"","category":"section"},{"location":"the_simplified_model_api/#","page":"The Simplified Model API","title":"The Simplified Model API","text":"The following probabilistic model simply fits a probability distribution to a MultiClasstraining target and returns this pdf for any new pattern:","category":"page"},{"location":"the_simplified_model_api/#","page":"The Simplified Model API","title":"The Simplified Model API","text":"import MLJBase\nimport Tables\nimport Distributions\n\nstruct MyClassifier <: MLJBase.Probabilistic{Any}\nend\n\n# `fit` ignores the inputs X and returns the training target y\n# probability distribution:\nfunction MLJBase.fit(model::MyClassifier, X, y)\n    fitresult = Distributions.fit(MLJBase.UnivariateNominal, y)\n    return fitresult\nend\n\n# `predict` retunrs the passed fitresult (pdf) for all new patterns:\nfunction MLJBase.predict(model::MyClassifier, fitresult, Xnew)\n    row_iterator = Tables.rows(Xnew)\n    return [fitresult for r in row_iterator]\nend","category":"page"},{"location":"the_simplified_model_api/#","page":"The Simplified Model API","title":"The Simplified Model API","text":"For more details on UnivariateNominal, query MLJBase.UnivariateNominal. ","category":"page"},{"location":"internals/#Internals-1","page":"Internals","title":"Internals","text":"","category":"section"},{"location":"internals/#The-machine-interface,-simplified-1","page":"Internals","title":"The machine interface, simplified","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"The following is simplified description of the Machine interface. See also the Glossary","category":"page"},{"location":"internals/#The-Machine-type-1","page":"Internals","title":"The Machine type","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"mutable struct Machine{M<Model}\n\n    model::M\n    fitresult\n    cache\n    args::Tuple    # e.g., (X, y) for supervised models\n    report\n    rows # remember last rows used \n    \n    function Machine{M}(model::M, args...) where M<:Model\n        machine = new{M}(model)\n        machine.args = args\n        machine.report = Dict{Symbol,Any}()\n        return machine\n    end\n\nend","category":"page"},{"location":"internals/#Constructor-1","page":"Internals","title":"Constructor","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"machine(model::M, Xtable, y) = Machine{M}(model, Xtable, y)","category":"page"},{"location":"internals/#fit!-and-predict/transform-1","page":"Internals","title":"fit! and predict/transform","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"function fit!(machine::Machine; rows=nothing, verbosity=1) \n\n    warning = clean!(mach.model)\n    isempty(warning) || verbosity < 0 || @warn warning \n\n    if rows == nothing\n        rows = (:) \n    end\n\n    rows_have_changed  = (!isdefined(mach, :rows) || rows != mach.rows)\n\n    args = [MLJ.selectrows(arg, rows) for arg in mach.args]\n\t\n    if !isdefined(mach, :fitresult) || rows_have_changed || force \n        mach.fitresult, mach.cache, report =\n            fit(mach.model, verbosity, args...)\n    else # call `update`:\n        mach.fitresult, mach.cache, report =\n            update(mach.model, verbosity, mach.fitresult, mach.cache, args...)\n    end\n\n    if rows_have_changed\n        mach.rows = deepcopy(rows)\n    end\n\n    if report != nothing\n        merge!(mach.report, report)\n    end\n\n    return mach\n\nend\n\nfunction predict(machine::Machine{<:Supervised}, Xnew)\n    if isdefined(machine, :fitresult)\n        return predict(machine.model, machine.fitresult, Xnew))\n    else\n        throw(error(\"$machine is not trained and so cannot predict.\"))\n    end\nend\n\nfunction transform(machine::Machine{<:Unsupervised}, Xnew)\n    if isdefined(machine, :fitresult)\n        return transform(machine.model, machine.fitresult, Xnew))\n    else\n        throw(error(\"$machine is not trained and so cannot transform.\"))\n    end\nend","category":"page"},{"location":"glossary/#Glossary-1","page":"Glossary","title":"Glossary","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Note: This glossary includes some detail intended mainly for MLJ developers.","category":"page"},{"location":"glossary/#Basics-1","page":"Glossary","title":"Basics","text":"","category":"section"},{"location":"glossary/#task-(object-of-type-Task)-1","page":"Glossary","title":"task (object of type Task)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Data plus a learning objective (e.g., \"probabilistic prediction of Sales\"). In MLJ a task does not include a description of how the completed task is to be evaluated.","category":"page"},{"location":"glossary/#hyperparameters-1","page":"Glossary","title":"hyperparameters","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Parameters on which some learning algorithm depends, specified before the algorithm is applied, and where learning is interpreted in the broadest sense. For example, PCA feature reduction is a \"preprocessing\" transformation \"learning\" a projection from training data, governed by a dimension hyperparameter. Hyperparameters in our sense may specify configuration (eg, number of parallel processes) even when this does not effect the end-product of learning. (But we exlcude verbosity level.)","category":"page"},{"location":"glossary/#model-(object-of-abstract-type-Model)-1","page":"Glossary","title":"model (object of abstract type Model)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Object collecting together hyperameters of a single algorithm. Most models are classified either as supervised or unsupervised models (generally, \"transformers\").","category":"page"},{"location":"glossary/#fit-result-(type-generally-defined-outside-of-MLJ)-1","page":"Glossary","title":"fit-result (type generally defined outside of MLJ)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Also known as \"learned\" or \"fitted\" parameters, these are \"weights\", \"coefficients\", or similar paramaters learned by an algorithm, after adopting the prescribed hyperparameters. For example, decision trees of a random forest, the coefficients and intercept of a linear model, or the rotation and projection matrices of PCA reduction scheme.","category":"page"},{"location":"glossary/#operation-1","page":"Glossary","title":"operation","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Data-manipulating operations (methods) parameterized by some fit-result. For supervised learners, the predict or predict_mode methods, for transformers, the transform or inverse_transform method. In some contexts, such an operation might be replaced by an ordinary operation (method) that does not depend on an fit-result, which are then then called static operations for clarity. An operation that is not static is dynamic.","category":"page"},{"location":"glossary/#machine-(object-of-type-Machine)-1","page":"Glossary","title":"machine (object of type Machine)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"An object consisting of:","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(1) A model ","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(2) A fit-result (undefined until training)","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(3) Training arguments (one for each data argument of the model's associated fit method). A training argument is data used for training. Generally, there are two training arguments for supervised models, and just one for unsuperivsed models.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"In addition machines store \"report\" metadata, for recording algorithm-specific statistics of training (eg, internal estimate of generalization error, feature importances); and they cache information allowing the fit-result to be updated without repeating unnecessary information.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Trainable models are trained by calls to a fit method which may be passed an optional argument specifying the rows of data to be used in training.","category":"page"},{"location":"glossary/#Learning-Networks-and-Composite-Models-1","page":"Glossary","title":"Learning Networks and Composite Models","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Note: Multiple nodal machines may share the same model, and multiple learning nodes may share the same nodal machine.","category":"page"},{"location":"glossary/#source-node-(object-of-type-Source)-1","page":"Glossary","title":"source node (object of type Source)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"A container for training data and point of entry for new data in a learning network (see below).","category":"page"},{"location":"glossary/#nodal-machine-(object-of-type-NodalMachine)-1","page":"Glossary","title":"nodal machine (object of type NodalMachine)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Like a machine with the following exceptions:","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(1) Training arguments are source nodes or regular nodes (see below) in the learning network, instead of data.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(2) The object internally records dependencies on other other nodal machines, as implied by the training arguments, and so on. ","category":"page"},{"location":"glossary/#node-(object-of-type-Node)-1","page":"Glossary","title":"node (object of type Node)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Essentially a nodal machine wrapped in an assoicated operation (e.g., predict or inverse_transform). It detail, it consists of:","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(1) An operation, static or dynamic.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(2) A nodal machine, void if the operation is static.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(3) Upstream connections to other learning or source nodes, specified by a list    of arguments (one for each argument of the operation).","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(4) Metadata recording the dependencies of the object's machine, and the dependecies on other nodal machines implied by its arguments.","category":"page"},{"location":"glossary/#learning-network-1","page":"Glossary","title":"learning network","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"An acyclic directed graph implicit in the connections of a collection of source(s) and nodes. Each connected component is ordinarily restricted to have a unique source.","category":"page"},{"location":"glossary/#wrapper-1","page":"Glossary","title":"wrapper","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Any model with one or more other models as hyperparameters.","category":"page"},{"location":"glossary/#composite-model-1","page":"Glossary","title":"composite model","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Any wrapper, or any learning network \"exported\" as a model (see Learning Networks).","category":"page"},{"location":"#MLJ.jl-1","page":"API","title":"MLJ.jl","text":"","category":"section"},{"location":"#","page":"API","title":"API","text":"Documentation for MLJ.jl","category":"page"},{"location":"#","page":"API","title":"API","text":"","category":"page"},{"location":"#Functions-1","page":"API","title":"Functions","text":"","category":"section"},{"location":"#","page":"API","title":"API","text":"@curve(var1, range, code)","category":"page"},{"location":"#","page":"API","title":"API","text":"Modules = [MLJ,MLJBase,MLJModels]","category":"page"},{"location":"#MLJ.EnsembleModel-Tuple{}","page":"API","title":"MLJ.EnsembleModel","text":"EnsembleModel(atom=nothing, weights=Float64[], bagging_fraction=0.8, rng_seed=0, n=100, parallel=true)\n\nCreate a model for training an ensemble of n learners, with optional bagging, each with associated model atom. Ensembling is useful if fit!(machine(atom, data...)) does not create identical models on repeated calls (ie, is a stochastic model, such as a decision tree with randomized node selection criteria), or if bagging_fraction is set to a value less than 1.0, or both. The constructor fails if no atom is specified.\n\nPredictions are weighted according to the vector weights (to allow for external optimization) except in the case that atom is a Deterministic classifier. Uniform weights are used if weight has zero length.\n\nThe ensemble model is Deterministic or Probabilistic, according to the corresponding supertype of atom. In the case of deterministic classifiers (target_scitype(atom) <: Union{Multiclass,FiniteOrderedFactor}), the predictions are majority votes, and for regressors (target_scitype(atom)<: Continuous) they are ordinary averages. Probabilistic predictions are obtained by averaging the atomic probability distribution/mass functions; in particular, for regressors, the ensemble prediction on each input pattern has the type MixtureModel{VF,VS,D} from the Distributions.jl package, where D is the type of predicted distribution for atom.\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.evaluate!-Tuple{Machine}","page":"API","title":"MLJ.evaluate!","text":"evaluate!(mach, resampling=CV(), measure=nothing, operation=predict, verbosity=1)\n\nEstimate the performance of a machine mach using the specified resampling (defaulting to 6-fold cross-validation) and measure, which can be a single measure or vector. In general operation is mutating (only mach.args, the data stored in the machine, is preserved).\n\nResampling and testing is based exclusively on data in rows, when specified.\n\nIf no measure is specified, then default_measure(mach.model) is used, unless this default is nothing and an error is thrown.\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.iterator-Union{Tuple{M}, Tuple{M,NamedTuple}} where M<:Model","page":"API","title":"MLJ.iterator","text":"iterator(model::Model, param_iterators::NamedTuple)\n\nIterator over all models of type typeof(model) defined by param_iterators.\n\nEach name in the nested :name => value pairs of param_iterators should be the name of a (possibly nested) field of model; and each element of flat_values(param_iterators) (the corresponding final values) is an iterator over values of one of those fields.\n\nSee also iterator and params.\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.learning_curve!-Tuple{Machine{#s74} where #s74<:Supervised}","page":"API","title":"MLJ.learning_curve!","text":"curve = learning_curve!(mach; resolution=30, resampling=Holdout(), measure=rms, operation=predict, nested_range=nothing, n=1)\n\nGiven a supervised machine mach, returns a named tuple of objects needed to generate a plot of performance measurements, as a function of the single hyperparameter specified in nested_range. The tuple curve has the following keys: :parameter_name, :parameter_scale, :parameter_values, :measurements.\n\nFor n not equal to 1, multiple curves are computed, and the value of curve.measurements is an array, one column for each run. This is useful in the case of models with indeterminate fit-results, such as a random forest.\n\nX, y = datanow()\natom = RidgeRegressor()\nensemble = EnsembleModel(atom=atom)\nmach = machine(ensemble, X, y)\nr_lambda = range(atom, :lambda, lower=0.1, upper=100, scale=:log10)\ncurve = MLJ.learning_curve!(mach; nested_range=(atom=(lambda=r_lambda,),))\nusing Plots\nplot(curve.parameter_values, curve.measurements, xlab=curve.parameter_name, xscale=curve.parameter_scale)\n\nSmart fitting applies. For example, if the model is an ensemble model, and the hyperparemeter parameter is n, then atomic models are progressively added to the ensemble, not recomputed from scratch for each new value of n.\n\natom.lambda=1.0\nr_n = range(ensemble, :n, lower=2, upper=150)\ncurves = MLJ.learning_curve!(mach; nested_range=(n=r_n,), verbosity=3, n=5)\nplot(curves.parameter_values, curves.measurements, xlab=curves.parameter_name)\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.localmodels-Tuple{}","page":"API","title":"MLJ.localmodels","text":"localmodels()\n\nList all models available for immediate use. Equivalent to models()[\"MLJ\"]\n\nlocalmodels(task)\n\nList all such models additionally matching the specified task. Equivalent to models(task)[\"MLJ\"].\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.models-Tuple{}","page":"API","title":"MLJ.models","text":"models(; show_dotted=false)\n\nList all models as a dictionary indexed on package name. Models available for immediate use (including external models loaded with @load and user-defined models) appear in localmodels()=models()[\"MLJ\"].\n\nBy declaring show_dotted=true models not in the top-level of the current namespace - which require dots to call, such as MLJ.DeterministicConstantModel - are also included.\n\nmodels(task; show_dotted=false)\n\nList all models matching the specified task. \n\nSee also: localmodels\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.rmsp-Tuple{AbstractArray{#s12,1} where #s12<:Real,Any}","page":"API","title":"MLJ.rmsp","text":"Root mean squared percentage loss \n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.info-Tuple{String}","page":"API","title":"MLJBase.info","text":"info(model, pkg=nothing)\n\nReturn the dictionary of metadata associated with model::String. If more than one package implements model then pkg::String will need to be specified.\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.SimpleDeterministicCompositeModel","page":"API","title":"MLJ.SimpleDeterministicCompositeModel","text":"SimpleDeterministicCompositeModel(;regressor=ConstantRegressor(), \n                          transformer=FeatureSelector())\n\nConstruct a composite model consisting of a transformer (Unsupervised model) followed by a Deterministic model. Mainly intended for internal testing .\n\n\n\n\n\n","category":"type"},{"location":"#Base.copy","page":"API","title":"Base.copy","text":"copy(params::NamedTuple, values=nothing)\n\nReturn a copy of params with new values. That is, flat_values(copy(params, values)) == values is true, while the nested keys remain unchanged.\n\nIf values is not specified a deep copy is returned. \n\n\n\n\n\n","category":"function"},{"location":"#Base.merge!-Tuple{Array{T,1} where T,Array{T,1} where T}","page":"API","title":"Base.merge!","text":"merge!(tape1, tape2)\n\nIncrementally appends to tape1 all elements in tape2, excluding any element previously added (or any element of tape1 in its initial state).\n\n\n\n\n\n","category":"method"},{"location":"#Base.range-Union{Tuple{D}, Tuple{MLJType,Symbol}} where D","page":"API","title":"Base.range","text":"r = range(model, :hyper; values=nothing)\n\nDefines a NominalRange object for a field hyper of model. Note that r is not directly iterable but iterator(r) iterates over values.\n\nr = range(model, :hyper; upper=nothing, lower=nothing, scale=:linear)\n\nDefines a NumericRange object for a field hyper of model.  Note that r is not directly iteratable but iterator(r, n) iterates over n values between lower and upper values, according to the specified scale. The supported scales are :linear, :log, :log10, :log2. Values for Integer types are rounded (with duplicate values removed, resulting in possibly less than n values).\n\nAlternatively, if a function f is provided as scale, then iterator(r, n) iterates over the values [f(x1), f(x2), ... , f(xn)], where x1, x2, ..., xn are linearly spaced between lower and upper.\n\nSee also: iterator\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.flat_keys-Tuple{Pair{Symbol,B} where B}","page":"API","title":"MLJ.flat_keys","text":" flat_keys(params::NamedTuple)\n\nUse dot-concatentation to express each possibly nested key of params in string form.\n\nExample\n\njulia> flat_keys((A=(x=2, y=3), B=9)))\n[\"A.x\", \"A.y\", \"B\"]\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.get_type-Tuple{Any,Symbol}","page":"API","title":"MLJ.get_type","text":"get_type(T, field::Symbol)\n\nReturns the type of the field field of DataType T. Not a type-stable function.  \n\n\n\n\n\n","category":"method"},{"location":"#MLJ.scale-Tuple{MLJ.NominalRange}","page":"API","title":"MLJ.scale","text":"MLJ.scale(r::ParamRange)\n\nReturn the scale associated with the ParamRange object r. The possible return values are: :none (for a NominalRange), :linear, :log, :log10, :log2, or :custom (if r.scale is function).\n\n\n\n\n\n","category":"method"},{"location":"#MLJ.unwind-Tuple","page":"API","title":"MLJ.unwind","text":"unwind(iterators...)\n\nRepresent all possible combinations of values generated by iterators as rows of a matrix A. In more detail, A has one column for each iterator in iterators and one row for each distinct possible combination of values taken on by the iterators. Elements in the first column cycle fastest, those in the last clolumn slowest. \n\nExample\n\njulia> iterators = ([1, 2], [\"a\",\"b\"], [\"x\", \"y\", \"z\"]);\njulia> MLJ.unwind(iterators...)\n12×3 Array{Any,2}:\n 1  \"a\"  \"x\"\n 2  \"a\"  \"x\"\n 1  \"b\"  \"x\"\n 2  \"b\"  \"x\"\n 1  \"a\"  \"y\"\n 2  \"a\"  \"y\"\n 1  \"b\"  \"y\"\n 2  \"b\"  \"y\"\n 1  \"a\"  \"z\"\n 2  \"a\"  \"z\"\n 1  \"b\"  \"z\"\n 2  \"b\"  \"z\"\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.SupervisedTask-Tuple{Any,Any}","page":"API","title":"MLJBase.SupervisedTask","text":"task = SupervisedTask(X, y; is_probabilistic=nothing, input_is_multivariate=true, target_is_multivariate=false)\n\nConstruct a supervised learning task with input features X and target y. Both X and y must be tables or vectors, according to whether they are multivariate or univariate. Table rows must correspond to patterns and columns to features. The boolean keyword argument is_probabilistic must be specified.\n\ntask = SupervisedTask(data=nothing, is_probabilistic=nothing, targets=nothing, ignore=Symbol[], input_is_multivariate=true)\n\nConstruct a supervised learning task with input features X and target y, where y is the column vector from data named target (if this is a single symbol) or, a table whose columns are those named in target (if this is vector); X consists of all remaining columns of data not named in ignore.\n\nX, y = task()\n\nReturns the input X and target y of the task, also available as task.X and task.y.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.UnivariateNominal","page":"API","title":"MLJBase.UnivariateNominal","text":"UnivariateNominal(prob_given_level)\n\nA discrete univariate distribution whose finite support is the set of keys of the provided dictionary, prob_given_level. The dictionary values specify the corresponding probabilities, which must be nonnegative and sum to one.\n\nUnivariateNominal(levels, p)\n\nA discrete univariate distribution whose finite support is the elements of the vector levels, and whose corresponding probabilities are elements of the vector p.\n\nlevels(d::UnivariateNominal)\n\nReturn the levels of d.\n\nd = UnivariateNominal([\"yes\", \"no\", \"maybe\"], [0.1, 0.2, 0.7])\npdf(d, \"no\") # 0.2\nmode(d) # \"maybe\"\nrand(d, 5) # [\"maybe\", \"no\", \"maybe\", \"maybe\", \"no\"]\nd = fit(UnivariateNominal, [\"maybe\", \"no\", \"maybe\", \"yes\"])\npdf(d, \"maybe\") ≈ 0.5 # true\nlevels(d) # [\"yes\", \"no\", \"maybe\"]\n\nIf v is a CategoricalVector then fit(UnivariateNominal, v) includes all levels in pool of v in its support, assigning unseen levels probability zero.\n\n\n\n\n\n","category":"type"},{"location":"#MLJBase.UnsupervisedTask-Tuple{}","page":"API","title":"MLJBase.UnsupervisedTask","text":"task = UnsupervisedTask(data=nothing, ignore=Symbol[], input_is_multivariate=true)\n\nConstruct an unsupervised learning task with given input data, which should be a table or, in the case of univariate inputs, a single vector. \n\nRows of data must correspond to patterns and columns to features. Columns in data whose names appear in ignore will be ignored by models trained on the task.\n\nX = task()\n\nReturn the input data (with ignored features removed). \n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.column_scitypes_as_tuple-Tuple{Any}","page":"API","title":"MLJBase.column_scitypes_as_tuple","text":"column_scitypes_as_tuple_type(X)\n\nReturns Tuple{T1, T2, ..., Tn} where Tj is the union of scitypes of elements in the jth column of X. Here X is any table, sparse table, or abstract matrix.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.container_type-Tuple{Any}","page":"API","title":"MLJBase.container_type","text":"container_type(X)\n\nReturn :table, :sparse, or :other, according to whether X is a supported table format, a supported sparse table format, or something else.\n\nThe first two formats, together abstract vectors, support the MLJBase accessor methods selectrows, selectcols, select, nrows, schema, and union_scitypes.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.datanow-Tuple{}","page":"API","title":"MLJBase.datanow","text":"Get some supervised data now!!\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.fitresult_type-Tuple{Type{#s35} where #s35<:Supervised}","page":"API","title":"MLJBase.fitresult_type","text":"MLJBase.fitresult_type(m)\n\nReturns the fitresult type of any supervised model (or model type) m, as declared in the model mutable struct declaration.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.load_ames-Tuple{}","page":"API","title":"MLJBase.load_ames","text":"Load the full version of the well-known Ames Housing task.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.load_boston-Tuple{}","page":"API","title":"MLJBase.load_boston","text":"Load a well-known public regression dataset with nominal features.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.load_crabs-Tuple{}","page":"API","title":"MLJBase.load_crabs","text":"Load a well-known crab classification dataset with nominal features.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.load_iris-Tuple{}","page":"API","title":"MLJBase.load_iris","text":"Load a well-known public classification task with nominal features.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.load_reduced_ames-Tuple{}","page":"API","title":"MLJBase.load_reduced_ames","text":"Load a reduced version of the well-known Ames Housing task, having six numerical and six categorical features.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.matrix-Tuple{Any}","page":"API","title":"MLJBase.matrix","text":"\"     MLJBase.matrix(X)\n\nConvert a table source X into an Matrix; or, if X is a AbstractMatrix, return X. Optimized for column-based sources.\n\nIf instead X is a sparse table, then a SparseMatrixCSC object is returned. The integer relabelling of column names follows the lexicographic ordering (as indicated by schema(X).names).\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.nrows-Tuple{Any}","page":"API","title":"MLJBase.nrows","text":"nrows(X)\n\nReturn the number of rows in a table, sparse table, or abstract vector.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.params-Tuple{Any}","page":"API","title":"MLJBase.params","text":"params(m)\n\nRecursively convert any object of subtype MLJType into a named tuple, keyed on the fields of m. The named tuple is possibly nested because params is recursively applied to the field values, which themselves might be MLJType objects. \n\nUsed, in particluar, in the case that m is a model, to inspect its nested hyperparameters:\n\njulia> params(EnsembleModel(atom=ConstantClassifier()))\n(atom = (target_type = Bool,),\n weights = Float64[],\n bagging_fraction = 0.8,\n rng_seed = 0,\n n = 100,\n parallel = true,)\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.partition-Tuple{AbstractArray{Int64,1},Vararg{Any,N} where N}","page":"API","title":"MLJBase.partition","text":"partition(rows::AbstractVector{Int}, fractions...; shuffle=false)\n\nSplits the vector rows into a tuple of vectors whose lengths are given by the corresponding fractions of length(rows). The last fraction is not provided, as it is inferred from the preceding ones. So, for example,\n\njulia> partition(1:1000, 0.2, 0.7)\n(1:200, 201:900, 901:1000)\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.schema-Tuple{Any}","page":"API","title":"MLJBase.schema","text":"schema(X)\n\nReturns a struct with properties names, types with the obvious meanings. Here X is any table or sparse table.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.select-Tuple{Any,Any,Any}","page":"API","title":"MLJBase.select","text":"select(X, r, c)\n\nSelect element of a table or sparse table at row r and column c. In the case of sparse data where the key (r, c), zero or missing is returned, depending on the value type.\n\nSee also: selectrows, selectcols\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.selectcols-Tuple{Any,Any}","page":"API","title":"MLJBase.selectcols","text":"selectcols(X, c)\n\nSelect single or multiple columns from any table or sparse table X. If c is an abstract vector of integers or symbols, then the object returned is a table of the preferred sink type of typeof(X). If c is a single integer or column, then a Vector or CategoricalVector is returned.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.selectrows-Tuple{Any,Any}","page":"API","title":"MLJBase.selectrows","text":"selectrows(X, r)\n\nSelect single or multiple rows from any table, sparse table, or abstract vector X.  If X is tabular, the object returned is a table of the preferred sink type of typeof(X), even a single row is selected.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.table-Tuple{NamedTuple}","page":"API","title":"MLJBase.table","text":"MLJBase.table(cols; prototype=cols)\n\nConvert a named tuple of vectors cols, into a table. The table type returned is the \"preferred sink type\" for prototype (see the Tables.jl documentation). \n\nMLJBase.table(X::AbstractMatrix; names=nothing, prototype=nothing)\n\nConvert an abstract matrix X into a table with names (a tuple of symbols) as column names, or with labels (:x1, :x2, ..., :xn) where n=size(X, 2), if names is not specified.  If prototype=nothing, then a named tuple of vectors is returned.\n\nEquivalent to table(cols, prototype=prototype) where cols is the named tuple of columns of X, with keys(cols) = names.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.union_scitypes-Tuple{Any}","page":"API","title":"MLJBase.union_scitypes","text":"union_scitypes(X)\n\nReturn the union over all elements x of X of scitype(x). Here X can be any table, sparse table, or abstract arrray.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.@constant-Tuple{Any}","page":"API","title":"MLJBase.@constant","text":"@constant x = value\n\nEquivalent to const x = value but registers the binding thus:\n\nMLJBase.HANDLE_GIVEN_ID[objectid(value)] = :x\n\nRegistered objects get displayed using the variable name to which it was bound in calls to show(x), etc.\n\nWARNING: As with any const declaration, binding x to new value of the same type is not prevented and the registration will not be updated.\n\n\n\n\n\n","category":"macro"},{"location":"#MLJBase.@more-Tuple{}","page":"API","title":"MLJBase.@more","text":"@more\n\nEntered at the REPL, equivalent to show(ans, 100). Use to get a recursive description of all fields of the last REPL value.\n\n\n\n\n\n","category":"macro"},{"location":"#MLJBase.CategoricalDecoder","page":"API","title":"MLJBase.CategoricalDecoder","text":"CategoricalDecoder(C::CategoricalArray)\nCategoricalDecoder(C::CategoricalArray, eltype)\n\nConstruct a decoder for transforming a CategoricalArray{T} object into an ordinary array, and for re-encoding similar arrays back into a CategoricalArray{T} object having the same pool (and, in particular, the same levels) as C. If eltype is not specified then the element type of the transformed array is T. Otherwise, the element type is eltype and the elements are promotions of the internal (integer) refs of the CategoricalArray. One must have R <: eltype <: Real where R is the reference type of the CategoricalArray (usually UInt32).\n\ntransform(decoder::CategoricalDecoder, C::CategoricalArray)\n\nTransform C into an ordinary Array.\n\ninverse_transform(decoder::CategoricalDecoder, A::Array)\n\nTransform an array A suitably compatible with decoder into a CategoricalArray having the same pool as C.\n\nlevels(decoder::CategoricalDecoder)\nlevels_seen(decoder::CategoricaDecoder)\n\nReturn, respectively, all levels in pool of the categorical vector C used to construct decoder (ie, levels(C)), and just those levels explicitly appearing as entries of C (ie, unique(C)).\n\nExample\n\njulia> using CategoricalArrays\njulia> C = categorical([\"a\" \"b\"; \"a\" \"c\"])\n2×2 CategoricalArray{String,2,UInt32}:\n \"a\"  \"b\"\n \"a\"  \"c\"\n\njulia> decoder = MLJBase.CategoricalDecoder(C, eltype=Float64);\njulia> A = transform(decoder, C)\n2×2 Array{Float64,2}:\n 1.0  2.0\n 1.0  3.0\n\njulia> inverse_transform(decoder, A[1:1,:])\n1×2 CategoricalArray{String,2,UInt32}:\n \"a\"  \"b\"\n\njulia> levels(ans)\n3-element Array{String,1}:\n \"a\"\n \"b\"\n \"c\"\n\n\n\n\n\n","category":"type"},{"location":"#MLJBase._cummulative-Union{Tuple{UnivariateNominal{L,T}}, Tuple{T}, Tuple{L}} where T<:Real where L","page":"API","title":"MLJBase._cummulative","text":"_cummulative(d::UnivariateNominal)\n\nReturn the cummulative probability vector [0, ..., 1] for the distribution d, using whatever ordering is used in the dictionary d.prob_given_level. Used only for to implement random sampling from d.\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase._rand-Tuple{Any}","page":"API","title":"MLJBase._rand","text":"rand(pcummulative)\n\nRandomly sample the distribution with discrete support 1:n which has cummulative probability vector p_cummulative=[0, ..., 1] (of length n+1). Does not check the first and last elements of p_cummulative but does not use them either. \n\n\n\n\n\n","category":"method"},{"location":"#MLJBase._recursive_show-Tuple{IO,MLJType,Any,Any}","page":"API","title":"MLJBase._recursive_show","text":"_recursive_show(stream, object, current_depth, depth)\n\nGenerate a table of the field values of the MLJType object, dislaying each value by calling the method _show on it. The behaviour of _show(stream, f) is as follows:\n\nIf f is itself a MLJType object, then its short form is shown\n\nand _recursive_show generates as separate table for each of its field values (and so on, up to a depth of argument depth).\n\nOtherwise f is displayed as \"(omitted T)\" where T = typeof(f),\n\nunless istoobig(f) is false (the istoobig fall-back for arbitrary types being true). In the latter case, the long (ie, MIME\"plain/text\") form of f is shown. To override this behaviour, overload the _show method for the type in question. \n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.abbreviated-Tuple{Any}","page":"API","title":"MLJBase.abbreviated","text":"to display abbreviated versions of integers\n\n\n\n\n\n","category":"method"},{"location":"#MLJBase.handle-Tuple{Any}","page":"API","title":"MLJBase.handle","text":"return abbreviated object id (as string)  or it's registered handle (as string) if this exists\n\n\n\n\n\n","category":"method"},{"location":"#Index-1","page":"API","title":"Index","text":"","category":"section"},{"location":"#","page":"API","title":"API","text":"","category":"page"},{"location":"frequently_asked_questions/#Frequently-Asked-Questions-1","page":"FAQ","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"frequently_asked_questions/#Julia-already-has-a-great-machine-learning-toolbox,-ScitkitLearn.jl.-Why-MLJ?-1","page":"FAQ","title":"Julia already has a great machine learning toolbox, ScitkitLearn.jl. Why MLJ?","text":"","category":"section"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"An alternative machine learning toolbox for Julia users is ScikitLearn.jl. Initially intended as a Julia wrapper for the popular python library scikit-learn, ML algorithms written in Julia can also implement the ScikitLearn.jl API. Meta-algorithms (systematic tuning, pipelining, etc) remain python wrapped code, however.","category":"page"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"While ScitkiLearn.jl provides the Julia user with access to a mature and large library of machine learning models, the scikit-learn API on which it is modeled, dating back to 2007, is not likely to evolve significantly in the future. MLJ enjoys (or will enjoy) several features that should make it an attractive alternative in the longer term:","category":"page"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"One language. ScikitLearn.jl wraps python code, which in turn wraps C code for performance-critical routines. A Julia machine learning algorithm that implements the MLJ model interface is 100% Julia. Writing code in Julia is almost as fast as python and well-written Julia code runs almost as fast as C. Additionally, a single language design provides superior interoperability. For example, one can implement: (i) gradient-descent tuning of hyperparameters, using automatic differentiation libraries such as Flux.jl; and (ii) GPU performance boosts without major code refactoring, using CuArrays.jl.\nRegistry for model metadata. In ScikitLearn.jl the list of available models, as well as model metadata (whether a model handles categorical inputs, whether is can make probabilistic predictions, etc) must be gleaned from documentation. In MLJ, this information is more structured and is accessible to MLJ via an external model registry (without the models needing to be loaded). This forms the basis of a \"task\" interface and facilitates model composition (see below).\nTask interface. Once the MLJ user specifies a \"task\" (e.g., \"make probabilistic predictions of home value, based on features x, y, z\") then MLJ can automatically search for models matching that task, assisting in systematic benchmarking and model selection.\nFlexible API for model composition. Pipelines in scikit-learn are more of an afterthought than an integral part of the original design. By contrast, MLJ's user-interaction API was predicated on the requirements of a flexible \"learning network\" API, one that allows models to be connected in essentially arbitrary ways (including target transforming and inverse-transforming). Networks can be built and tested in stages before being exported as first-class stand-alone models. Networks feature \"smart\" training (only necessary components are retrained after parameter changes) and will eventually be trainable using a DAG scheduler. With the help of Julia's meta-programming features, constructing common architectures, such as linear pipelines and stacks, will be one-line operations.\nClean probabilistic API. The scikit-learn API does not specify a universal standard for the form of probabilistic predictions. By fixing a probabilistic API along the lines of the skpro project, MLJ aims to improve support for Bayesian statistics and probabilistic graphical models.\nUniversal adoption of categorical data types. Python's scientific array library NumPy has no dedicated data type for representing categorical data (i.e., no type that tracks the pool of all possible classes). Generally scikit-learn models deal with this by requiring data to be relabeled as integers. However, the naive user trains a model on relabeled categorical data only to discover that evaluation on a test set crashes his code because a categorical feature takes on a value not observed in training. MLJ mitigates such issues by insisting on the use of categorical data types, and by insisting that MLJ model implementations preserve the class pools. If, for example, a training target contains classes in the pool that do not actually appear in the training set, a probabilistic prediction will nevertheless predict a distribution whose support includes the missing class, but which is appropriately weighted with probability zero.","category":"page"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"Finally, we note that there is a project underway to implement (some of) the ScikitLearn.jl models as MLJ models, as an temporary expedient.","category":"page"},{"location":"NEWS/#MLJ-News-1","page":"MLJ News","title":"MLJ News","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Development news for MLJ and its satellite packages,  MLJBase, MLJRegistry and MLJModels","category":"page"},{"location":"NEWS/#unversioned-commits-1-March-2019-(some-time-after-03:50-GMT)-1","page":"MLJ News","title":"unversioned commits 1 March 2019 (some time after 03:50 GMT)","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Addition of \"scientific type\" hierarchy, including Continuous, Discrete, Multiclass, and Other subtypes of Found (to complement Missing). See new documents Getting Started and Scientific Data Types for more one this.  Resolves: #86\nRevamp of model traits to take advantage of scientific types, with output_kind replaced with target_scitype, input_kind replaced with input_scitype. Also, output_quantity dropped, input_quantity replaced with Bool-valued input_is_multivariate, and is_pure_julia made Bool-valued. Trait definitions in all model implementations and effected meta-algorithms have been updated. Related: #81\nSubstantial update of the core guide Adding New Models to reflect above changes and in response to new model implementer queries. Some design \"decisions\" regarding multivariate targets now explict there.\nthe order the y and yhat arguments of measures (aka loss functions) have been reversed. Progress on: #91\nUpdate of Standardizer and OneHotEncoder to mesh with new scitypes.\nNew improved task constructors infer task metadata from data scitypes. This brings us close to a simple implementation of basic task-model matching. Query the doc-strings for SupervisedTask and UnsupervisedTask for details.  Machines can now dispatch on tasks instead of X and y. A task, task, is now callable: task() returns (X, y) for supervised models, and X for unsupervised models.  Progress on:  #86\nthe data in the load_ames() test task has been replaced by the full data set, and load_reduced_ames() now loads a reduced set.","category":"page"}]
}
