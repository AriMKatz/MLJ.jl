var documenterSearchIndex = {"docs":
[{"location":"#Getting-Started-1","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"#[Installation-instructions](https://github.com/alan-turing-institute/MLJ.jl/blob/master/README.md)-1","page":"Getting Started","title":"Installation instructions","text":"","category":"section"},{"location":"#Basic-supervised-training-and-testing-1","page":"Getting Started","title":"Basic supervised training and testing","text":"","category":"section"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> using MLJ\njulia> using RDatasets\njulia> iris = dataset(\"datasets\", \"iris\"); # a DataFrame","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"In MLJ one can either wrap data for supervised learning in a formal task (see Working with Tasks), or work directly with the data, split into its input and target parts:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> const X = iris[:, 1:4];\njulia> const y = iris[:, 5];","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"A model is a container for hyperparameters. Assuming the DecisionTree package is in your installation load path, we can instantiate a DecisionTreeClassifier model like this:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> @load DecisionTreeClassifier\nimport MLJModels ✔\nimport DecisionTree ✔\nimport MLJModels.DecisionTree_.DecisionTreeClassifier ✔\n\njulia> tree_model = DecisionTreeClassifier(max_depth=2)\nDecisionTreeClassifier(pruning_purity = 1.0,\n                       max_depth = 2,\n                       min_samples_leaf = 1,\n                       min_samples_split = 2,\n                       min_purity_increase = 0.0,\n                       n_subfeatures = 0.0,\n                       display_depth = 5,\n                       post_prune = false,\n                       merge_purity_threshold = 0.9,) @ 1…85","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Wrapping the model in data creates a machine which will store training outcomes:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> tree = machine(tree_model, X, y)\nMachine{DecisionTreeClassifier} @ 9…45","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Training and testing on a hold-out set:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> train, test = partition(eachindex(y), 0.7, shuffle=true); # 70:30 split\njulia> fit!(tree, rows=train)\njulia> yhat = predict(tree, X[test,:]);\njulia> misclassification_rate(yhat, y[test])\n\n[ Info: Training Machine{DecisionTreeClassifier} @ 9…45.\nMachine{DecisionTreeClassifier} @ 9…45\n\n0.044444444444444446","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Or, in one line:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> evaluate!(tree, resampling=Holdout(fraction_train=0.7, shuffle=true), measure=misclassification_rate)\n0.08888888888888889","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Changing a hyperparameter and re-evaluating:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> tree_model.max_depth = 3\njulia> evaluate!(tree, resampling=Holdout(fraction_train=0.5, shuffle=true), measure=misclassification_rate)\n0.06666666666666667","category":"page"},{"location":"#Next-steps-1","page":"Getting Started","title":"Next steps","text":"","category":"section"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"To learn a little more about what MLJ can do, take the MLJ tour, and then return to the manual as needed. Read at least the remainder of this page before considering serious use of MLJ.","category":"page"},{"location":"#Prerequisites-1","page":"Getting Started","title":"Prerequisites","text":"","category":"section"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"MLJ assumes some familiarity with the CategoricalValue and CategoricalString types from CategoricalArrays.jl, used here for representing categorical data. For probabilistic predictors, a basic acquaintance with Distributions.jl is also assumed.","category":"page"},{"location":"#Data-containers-and-scientific-types-1","page":"Getting Started","title":"Data containers and scientific types","text":"","category":"section"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"The MLJ user should acquaint themselves with some basic assumptions about the form of data expected by MLJ, as outlined below. ","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"machine(model::Supervised, X, y) \nmachine(model::Unsupervised, X)","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Multivariate input. The input X in the above machine constructors can be any table, where table means any data type supporting the Tables.jl interface.","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"At present our API is more restrictive; see this issue with Tables.jl. If your Tables.jl compatible format is not working in MLJ, please post an issue.","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"In particular, DataFrame, JuliaDB.IndexedTable and TypedTables.Table objects are supported, as are two Julia native formats: column tables (named tuples of equal length vectors) and row tables (vectors of named tuples sharing the same keys).","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Certain JuliaDB.NDSparse tables can be used for sparse data, but this is experimental and undocumented.","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Univariate input. For models which handle only univariate inputs (input_is_multivariate(model)=false) X cannot be a table but is expected to be some AbstractVector type.","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Targets. The target y in the first constructor above must be an AbstractVector. A multivariate target y will be a vector of tuples. The tuples need not have uniform length, so some forms of sequence prediction are supported. Only the element types of y matter (the types of y[j] for each j). Indeed if a machine accepts y as an argument it will be just as happy with identity.(y).","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Element types. The types of input and target elements has strict consequences for MLJ's behaviour. ","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"To articulate MLJ's conventions about data representation, MLJ distinguishes between machine data types on the one hand (Float64, Bool, String, etc) and scientific data types on the other, represented by new Julia types: Continuous, Count, Multiclass{N}, OrderedFactor{N} and Unknown, with obvious interpretations.  These types are organized in a type hierarchy rooted in a new abstract type Found.","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"A scientific type is any subtype of Union{Missing,Found}. Scientific types have no instances. (They are used behind the scenes is values for model trait functions.) Such types appear, for example, when querying model metadata:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> info(\"DecisionTreeClassifier\")[:target_scitype_union]","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Finite","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"subtypes(Finite)","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"2-element Array{Any,1}:\n Multiclass   \n OrderedFactor","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"This means that the scitype of all elements of DecisionTreeClassier target must be Multiclass or OrderedFactor.","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"The table below shows machine types that have scientific types different from Unknown:","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"T scitype(x) for x::T\nAbstractFloat Continuous\nInteger Count\nCategoricalValue Multiclass{N} where N = nlevels(x), provided x.pool.ordered == false\nCategoricalString Multiclass{N} where N = nlevels(x), provided x.pool.ordered == false\nCategoricalValue FiniteOrderedFactor{N} where N = nlevels(x), provided x.pool.ordered == true\nCategoricalString FiniteOrderedFactor{N} where N = nlevels(x) provided x.pool.ordered == true\nInteger Count\nMissing Missing","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Here nlevels(x) = length(levels(x.pool)).","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"You can use scitype(x) to determine the scientific type of a scalar object x. Non-scalar objects (with the exception of tuples of scalars) have Unknown scitype. ","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"julia> (scitype(42), scitype(float(π)), scitype(\"Julia\"))\n(Count, Continuous, Unknown)","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Special note on using integers. According to the above, integers cannot be used to represent Multiclass or OrderedFactor data. They these can be represented by an unordered or ordered CategoricalValue or CategoricalString (automatic if they are elements of a CategoricalArray).","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"Methods exist to coerce the scientific type of a vector or table (see below). Task constructors also allow one to force the data being wrapped to have the desired scientific type.","category":"page"},{"location":"#","page":"Getting Started","title":"Getting Started","text":"coerce","category":"page"},{"location":"#MLJ.coerce","page":"Getting Started","title":"MLJ.coerce","text":"coerce(T, v::AbstractVector)\n\nCoerce the machine types of elements of v to ensure the returned vector has T as its scitype_union, or Union{Missing,T}, if v has missing values.\n\njulia> v = coerce(Continuous, [1, missing, 5])\n3-element Array{Union{Missing, Float64},1}:\n 1.0     \n missing\n 5.0  \n\njulia> scitype_union(v)\nUnion{Missing,Continuous}\n\nSee also scitype, scitype_union, scitypes\n\n\n\n\n\ncoerce(d::Dict, X)\n\nReturn a copy of the table X with columns named in the keys of d coerced to have scitype_union equal to the corresponding value. \n\n\n\n\n\n","category":"function"},{"location":"working_with_tasks/#Working-with-Tasks-1","page":"Working with Tasks","title":"Working with Tasks","text":"","category":"section"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"In MLJ a task is a synthesis of three elements: data, an interpretation of that data, and a learning objective. Once one has a task one is ready to choose learning models.","category":"page"},{"location":"working_with_tasks/#Scientific-types-and-the-interpretation-of-data-1","page":"Working with Tasks","title":"Scientific types and the interpretation of data","text":"","category":"section"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Generally the columns of a table, such as a DataFrame, represents real quantities. However, the nature of a quantity is not always clear from the representation. For example, we might count phone calls using the UInt32 type but also use UInt32 to represent a categorical feature, such as the species of conifers. MLJ mitigates such ambiguity by: (i) distinguishing between the machine and scientific type of scalar data; (ii) disallowing the representation of multiple scientific types by the same machine type during learning; and (iii) establishing a convention for what scientific types a given machine type may represent (see the table at the end of Getting Started).","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Explicitly specifying scientific types during the construction of a MLJ task is the user's opportunity to articulate how the supplied data should be interpreted.","category":"page"},{"location":"working_with_tasks/#Learning-objectives-1","page":"Working with Tasks","title":"Learning objectives","text":"","category":"section"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"In MLJ specifying a learning objective means specifying: (i) whether learning is supervised or not; (ii) whether, in the supervised case, predictions are to be probabilistic or deterministic; and (iii) what part of the data is relevant and what role is each part to play.","category":"page"},{"location":"working_with_tasks/#Sample-usage-1","page":"Working with Tasks","title":"Sample usage","text":"","category":"section"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Load a built-in task:","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"using MLJ\nMLJ.color_off() # hide\ntask = load_iris()","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Extract input and target:","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"X, y = task()\nX[1:3, :]","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Now, starting with some tabular data...","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"using RDatasets\ndf = dataset(\"boot\", \"channing\");\nfirst(df, 4)","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"...we can check MLJ's interpretation of that data:","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"scitypes(df)","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"And construct a task by wrapping the data in a learning objective, and coercing the data into a form MLJ will correctly interpret. (The middle three fields of df refer to ages, in months, the last is a flag.):","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"task = supervised(data=df,\n                  target=:Exit,\n                  ignore=:Time,\n                  is_probabilistic=true,\n                  types=Dict(:Entry=>Continuous,\n                             :Exit=>Continuous,\n                             :Cens=>Multiclass))\nscitypes(task.X)","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"(Sex = Multiclass{2},\n Entry = Continuous,\n Cens = Multiclass{2},)","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Shuffle the rows of a task:","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"task = load_iris()\nusing Random\nrng = MersenneTwister(1234)\nshuffle!(rng, task) # rng is optional\ntask[1:4].y","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Counting and selecting rows of a task:","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"nrows(task)","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"task[1:2].y","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Listing the models available to complete a task:","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"models(task)","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"Binding a model to a task and evaluating performance:","category":"page"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"@load DecisionTreeClassifier\nmach = machine(DecisionTreeClassifier(), task)\nevaluate!(mach, operation=predict_mode, resampling=Holdout(), measure=misclassification_rate, verbosity=0)","category":"page"},{"location":"working_with_tasks/#API-Reference-1","page":"Working with Tasks","title":"API Reference","text":"","category":"section"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"supervised","category":"page"},{"location":"working_with_tasks/#MLJ.supervised","page":"Working with Tasks","title":"MLJ.supervised","text":"task = supervised(data=nothing, \n                  types=Dict(), \n                  target=nothing,  \n                  ignore=Symbol[], \n                  is_probabilistic=false, \n                  verbosity=1)\n\nConstruct a supervised learning task with input features X and target y, where: y is the column vector from data named target, if this is a single symbol, or a vector of tuples, if target is a vector; X consists of all remaining columns of data not named in ignore, and is a table unless it has only one column, in which case it is a vector.\n\nThe data types of elements in a column of data named as a key of the dictionary types are coerced to have a scientific type given by the corresponding value. Possible values are Continuous, Multiclass, OrderedFactor and Count. So, for example, types=Dict(:x1=>Count) means elements of the column of data named :x1 will be coerced into integers (whose scitypes are always Count).\n\ntask = supervised(X, y; \n                  input_is_multivariate=true, \n                  is_probabilistic=false, \n                  verbosity=1)\n\nA more customizable constructor, this returns a supervised learning task with input features X and target y, where: X must be a table or vector, according to whether it is multivariate or univariate, while y must be a vector whose elements are scalars, or tuples scalars (of constant length for ordinary multivariate predictions, and of variable length for sequence prediction). Table rows must correspond to patterns and columns to features. Type coercion is not available for this constructor (but see also coerce).\n\nX, y = task()\n\nReturns the input X and target y of the task, also available as task.X and task.y.\n\n\n\n\n\n","category":"function"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"unsupervised","category":"page"},{"location":"working_with_tasks/#MLJ.unsupervised","page":"Working with Tasks","title":"MLJ.unsupervised","text":"task = unsupervised(data=nothing, types=Dict(), ignore=Symbol[], verbosity=1)\n\nConstruct an unsupervised learning task with given input data, which should be a table or, in the case of univariate inputs, a single vector. \n\nThe data types of elements in a column of data named as a key of the dictionary types are coerced to have a scientific type given by the corresponding value. Possible values are Continuous, Multiclass, OrderedFactor and Count. So, for example, types=Dict(:x1=>Count) means elements of the column of data named :x1 will be coerced into integers (whose scitypes are always Count).\n\nRows of data must correspond to patterns and columns to features. Columns in data whose names appear in ignore are ignored.\n\nX = task()\n\nReturn the input data in form to be used in models.\n\nSee also scitype, scitype_union, scitypes\n\n\n\n\n\n","category":"function"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"models()","category":"page"},{"location":"working_with_tasks/#MLJ.models-Tuple{}","page":"Working with Tasks","title":"MLJ.models","text":"models(; show_dotted=false)\n\nList all models as a dictionary indexed on package name. Models available for immediate use appear under the key \"MLJ\". \n\nBy declaring show_dotted=true models not in the top-level of the current namespace - which require dots to call, such as MLJ.DeterministicConstantModel - are also included.\n\nmodels(task; show_dotted=false)\n\nList all models matching the specified task. \n\nSee also: localmodels\n\n\n\n\n\n","category":"method"},{"location":"working_with_tasks/#","page":"Working with Tasks","title":"Working with Tasks","text":"localmodels()","category":"page"},{"location":"working_with_tasks/#MLJ.localmodels-Tuple{}","page":"Working with Tasks","title":"MLJ.localmodels","text":"localmodels()\n\nList all models available for immediate use. Equivalent to models()[\"MLJ\"]\n\nlocalmodels(task)\n\nList all such models additionally matching the specified task. Equivalent to models(task)[\"MLJ\"].\n\n\n\n\n\n","category":"method"},{"location":"built_in_transformers/#Built-in-Transformers-1","page":"Built-in Transformers","title":"Built-in Transformers","text":"","category":"section"},{"location":"built_in_transformers/#","page":"Built-in Transformers","title":"Built-in Transformers","text":"MLJ.Transformers.UnivariateStandardizer\nMLJ.Transformers.Standardizer\nMLJ.Transformers.OneHotEncoder\nMLJ.Transformers.FeatureSelector\nMLJ.Transformers.UnivariateBoxCoxTransformer","category":"page"},{"location":"built_in_transformers/#MLJ.Transformers.UnivariateStandardizer","page":"Built-in Transformers","title":"MLJ.Transformers.UnivariateStandardizer","text":"UnivariateStandardizer()\n\nUnsupervised model for standardizing (whitening) univariate data. \n\n\n\n\n\n","category":"type"},{"location":"built_in_transformers/#MLJ.Transformers.Standardizer","page":"Built-in Transformers","title":"MLJ.Transformers.Standardizer","text":"Standardizer(; features=Symbol[])\n\nUnsupervised model for standardizing (whitening) the columns of tabular data. If features is empty then all columns v for which all elements have Continuous scitypes are standardized. For different behaviour, specify the names of features to be standardized.\n\nusing DataFrames\nX = DataFrame(x1=[0.2, 0.3, 1.0], x2=[4, 2, 3])\nstand_model = Standardizer()\ntransform(fit!(machine(stand_model, X)), X)\n\n3×2 DataFrame\n│ Row │ x1        │ x2    │\n│     │ Float64   │ Int64 │\n├─────┼───────────┼───────┤\n│ 1   │ -0.688247 │ 4     │\n│ 2   │ -0.458831 │ 2     │\n│ 3   │ 1.14708   │ 3     │\n\n\n\n\n\n","category":"type"},{"location":"built_in_transformers/#MLJ.Transformers.OneHotEncoder","page":"Built-in Transformers","title":"MLJ.Transformers.OneHotEncoder","text":"OneHotEncoder(; features=Symbol[], drop_last=false, ref_type=UInt32)\n\nUnsupervised model for one-hot encoding all features of Finite scitype, within some table. All such features are encoded, unless features is specified and non-empty.\n\nIf drop_last is true, the column for the last level of each categorical feature is dropped. New data to be transformed may lack features present in the fit data, but no new features can be present.\n\nAll categorical features to be transformed (which are necessarily of CategoricalValue or CategoricalString eltype) must have a reference type promotable to ref_type. Usually ref_type=UInt32 suffices, but ref_type=Int will always work.\n\nWarning: This transformer assumes that a categorical feature in new  data to be transformed will have the same pool encountered  during the fit.\n\n\n\n\n\n","category":"type"},{"location":"built_in_transformers/#MLJ.Transformers.FeatureSelector","page":"Built-in Transformers","title":"MLJ.Transformers.FeatureSelector","text":"FeatureSelector(features=Symbol[])\n\nAn unsupervised model for filtering features (columns) of a table. Only those features encountered during fitting will appear in transformed tables if features is empty (the default). Alternatively, if a non-empty features is specified, then only the specified features are used. Throws an error if a recorded or specified feature is not present in the transformation input.\n\n\n\n\n\n","category":"type"},{"location":"built_in_transformers/#MLJ.Transformers.UnivariateBoxCoxTransformer","page":"Built-in Transformers","title":"MLJ.Transformers.UnivariateBoxCoxTransformer","text":"UnivariateBoxCoxTransformer(; n=171, shift=false)\n\nUnsupervised model specifying a univariate Box-Cox transformation of a single variable taking non-negative values, with a possible preliminary shift. Such a transformation is of the form\n\nx -> ((x + c)^λ - 1)/λ for λ not 0\nx -> log(x + c) for λ = 0\n\nOn fitting to data n different values of the Box-Cox exponent λ (between -0.4 and 3) are searched to fix the value maximizing normality. If shift=true and zero values are encountered in the data then the transformation sought includes a preliminary positive shift c of 0.2 times the data mean. If there are no zero values, then no shift is applied.\n\nSee also BoxCoxEstimator a transformer for selected ordinals in a an iterable table.\n\n\n\n\n\n","category":"type"},{"location":"learning_networks/#Learning-Networks-1","page":"Learning Networks","title":"Learning Networks","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"MLJ has a flexible interface for building networks from multiple machine learning elements, whose complexity extend beyond the \"pipelines\" of other machine learning toolboxes. ","category":"page"},{"location":"learning_networks/#Overview-1","page":"Learning Networks","title":"Overview","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"In the future the casual MLJ user will be able to build common pipeline architetures, such as linear compositites and stacks, with simple macro invocations. Handcrafting a learning network, as outlined below, is an advanced MLJ feature, assuming familiarity with the basics outlined in Getting Started. The syntax for building a learning network is essentially an extension of the basic syntax but with data objects replaced with nodes (\"dynamic data\").","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"In MLJ, a learning network is a graph whose nodes apply an operation, such as predict or transform, using a fixed machine (requiring training) - or which, alternatively, applies a regular (untrained) mathematical operation to its input(s). In practice, a learning network works with fixed sources for its training/evaluation data, but can be built and tested in stages. By contrast, an exported learning network is a learning network exported as a stand-alone, re-usable Model object, to which all the MLJ Model meta-algorthims can be applied (ensembling, systematic tuning, etc).","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"As we shall see, exporting a learning network as a reusable model, is quite simple. While one can entirely skip the build-and-train steps, experimenting with raw learning networks may be the best way to understand how the stand-alone models work.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"In MLJ learning networks treat the flow of information during training and predicting separately. For this reason, simpler examples may appear more a little more complicated than in other approaches. However, in more sophisticated examples, such as stacking, this separation is essential.","category":"page"},{"location":"learning_networks/#Building-a-simple-learning-network-1","page":"Learning Networks","title":"Building a simple learning network","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"(Image: )","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"The diagram above depicts a learning network which standardises the input data X, learns an optimal Box-Cox transformation for the target y, predicts new target values using ridge regression, and then inverse-transforms those predictions, for later comparison with the original test data. The machines are labelled yellow.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"To implement the network, we begin by loading data needed for training and evaluation into source nodes. For testing purposes, we'll use a small synthetic data set:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"using MLJ # hide\nMLJ.color_off() # hide\nusing Statistics, DataFrames # hide\nx1 = rand(300)\nx2 = rand(300)\nx3 = rand(300)\ny = exp.(x1 - x2 -2x3 + 0.1*rand(300))\nX = DataFrame(x1=x1, x2=x2, x3=x3) # a column table\nys = source(y)\nXs = source(X)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"We label nodes we will construct according to their outputs in the diagram. Notice that the nodes z and yhat use the same machine, namely box, for different operations.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"To construct the W node we first need to define the machine stand that it will use to transform inputs.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"stand_model = Standardizer()\nstand = machine(stand_model, Xs)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Because Xs is a node, instead of concrete data, we can call transform on the machine without first training it, and the result is the new node W, instead of concrete transformed data:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"W = transform(stand, Xs)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"To get actual transformed data we call the node appropriately, which will require we first train the node. Training a node, rather than a machine, triggers training of all necessary machines in the network.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"test, train = partition(eachindex(y), 0.8)\nfit!(W, rows=train)\nW()           # transform all data\nW(rows=test ) # transform only test data\nW(X[3:4,:])   # transform any data, new or old","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"If you like, you can think of W (and the other nodes we will define) as \"dynamic data\": W is data, in the sense that it an be called (\"indexed\") on rows, but dynamic, in the sense the result depends on the outcome of training events.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"The other nodes of our network are defined similarly:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"box_model = UnivariateBoxCoxTransformer()  # for making data look normally-distributed\nbox = machine(box_model, ys)\nz = transform(box, ys)\n\nridge_model = RidgeRegressor(lambda=0.1)\nridge =machine(ridge_model, W, z)\nzhat = predict(ridge, W)\n\nyhat = inverse_transform(box, zhat)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"We are ready to train and evaluate the completed network. Notice that the standardizer, stand, is not retrained, as MLJ remembers that it was trained earlier:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"fit!(yhat, rows=train)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"rms(y[test], yhat(rows=test)) # evaluate","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"We can change a hyperparameters and retrain:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"ridge_model.lambda = 0.01\nfit!(yhat, rows=train) ","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"rms(y[test], yhat(rows=test))","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Notable feature. The machine, ridge::NodalMachine{RidgeRegressor}, is retrained, because its underlying model has been mutated. However, since the outcome of this training has no effect on the training inputs of the machines stand and box, these transformers are left untouched. (During construction, each node and machine in a learning network determines and records all machines on which it depends.) This behaviour, which extends to exported learning networks, means we can tune our wrapped regressor (using a holdout set) without re-computing transformations each time the hyperparameter is changed. ","category":"page"},{"location":"learning_networks/#Exporting-a-learning-network-as-a-stand-alone-model-1","page":"Learning Networks","title":"Exporting a learning network as a stand-alone model","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"To export a learning network:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Define a new mutable struct model type.\nWrap the learning network code in a model fit method.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"All learning networks that make determinisic (or, probabilistic) predictions export as models of subtype DeterministicNetwork (respectively, ProbabilisticNetwork):","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"mutable struct WrappedRidge <: DeterministicNetwork\n    ridge_model\nend\n\n# keyword constructor\nWrappedRidge(; ridge_model=RidgeRegressor) = WrappedRidge(ridge_model); ","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Now satisfied that the learning network we defined above works, we simply cut and paste its defining code into a fit method:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"function MLJ.fit(model::WrappedRidge, X, y)\n    Xs = source(X)\n    ys = source(y)\n\n    stand_model = Standardizer()\n    stand = machine(stand_model, Xs)\n    W = transform(stand, Xs)\n\n    box_model = UnivariateBoxCoxTransformer()  # for making data look normally-distributed\n    box = machine(box_model, ys)\n    z = transform(box, ys)\n\n    ridge_model = model.ridge_model ###\n    ridge =machine(ridge_model, W, z)\n    zhat = predict(ridge, W)\n\n    yhat = inverse_transform(box, zhat)\n    fit!(yhat, verbosity=0)\n    \n    return yhat\nend","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"The line marked ###, where the new exported model's hyperparameter ridge_model is spliced into the network, is the only modification.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"What's going on here? MLJ's machine interface is built atop a more primitive model interface, implemented for each algorithm. Each supervised model type (eg, RidgeRegressor) requires model fit and predict methods, which are called by the corresponding machine fit! and predict methods. We don't need to define a  model predict method here because MLJ provides a fallback which simply calls the node returned by fit on the data supplied: MLJ.predict(model::SupervisedNetwork, Xnew) = yhat(Xnew).","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"The export process is complete and we can wrap our exported model around any data or task we like, and evaluate like any other model:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"task = load_boston()\nwrapped_model = WrappedRidge(ridge_model=ridge_model)\nmach = machine(wrapped_model, task)\nevaluate!(mach, resampling=CV(), measure=rms, verbosity=0)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Another example of an exported learning network is given in the next subsection.","category":"page"},{"location":"learning_networks/#Static-operations-on-nodes-1","page":"Learning Networks","title":"Static operations on nodes","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Continuing to view nodes as \"dynamic data\", we can, in addition to applying \"dynamic\" operations like predict and transform to nodes, overload ordinary \"static\" operations as well. Common operations, like addition, scalar multiplication, exp and log work out-of-the box. To demonstrate this, consider the code below defining a composite model that: (i) One-hot encodes the input table X; (ii) Log transforms the continuous target y; (iii) Fits specified K-nearest neighbour and ridge regressor models to the data; (iv) Computes a weighted average of individual model predictions; and (v) Inverse transforms (exponentiates) the blended predictions.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Note, in particular, the lines defining zhat and yhat, which combine several static node operations.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"mutable struct KNNRidgeBlend <:DeterministicNetwork\n\n    knn_model\n    ridge_model\n    weights::Tuple{Float64, Float64}\n\nend\n\nfunction MLJ.fit(model::KNNRidgeBlend, X, y)\n    \n    Xs = source(X) \n    ys = source(y)\n\n    hot = machine(OneHotEncoder(), Xs)\n\n    # W, z, zhat and yhat are nodes in the network:\n    \n    W = transform(hot, Xs) # one-hot encode the input\n    z = log(ys) # transform the target\n    \n    ridge_model = model.ridge_model\n    knn_model = model.knn_model\n\n    ridge = machine(ridge_model, W, z) \n    knn = machine(knn_model, W, z)\n\n    # average the predictions of the KNN and ridge models\n    zhat = model.weights[1]*predict(ridge, W) + weights[2]*predict(knn, W) \n\n    # inverse the target transformation\n    yhat = exp(zhat) \n\n    fit!(yhat, verbosity=0)\n    \n    return yhat\nend\n","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"task = load_boston()\nknn_model = KNNRegressor(K=2)\nridge_model = RidgeRegressor(lambda=0.1)\nweights = (0.9, 0.1)\nblended_model = KNNRidgeBlend(knn_model, ridge_model, weights)\nmach = machine(blended_model, task)\nevaluate!(mach, resampling=Holdout(fraction_train=0.7), measure=rmsl) ","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"A node method allows us to overerload a given function to node arguments.  Here are some examples taken from MLJ source (at work in the example above):","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Base.log(v::Vector{<:Number}) = log.(v)\nBase.log(X::AbstractNode) = node(log, X)\n\nimport Base.+\n+(y1::AbstractNode, y2::AbstractNode) = node(+, y1, y2)\n+(y1, y2::AbstractNode) = node(+, y1, y2)\n+(y1::AbstractNode, y2) = node(+, y1, y2)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Here AbstractNode is the common supertype of Node and Source.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"As a final example, here's how to extend row shuffling to nodes:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"using Random\nRandom.shuffle(X::AbstractNode) = node(Y -> MLJ.selectrows(Y, Random.shuffle(1:nrows(Y))), X)\nX = (x1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n     x2 = [:one, :two, :three, :four, :five, :six, :seven, :eight, :nine, :ten])\nXs = source(X)\nW = shuffle(Xs)","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"W()","category":"page"},{"location":"learning_networks/#The-learning-network-API-1","page":"Learning Networks","title":"The learning network API","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Three types are part of learning networks: Source, Node and NodalMachine. A NodalMachine is returned by the machine constructor when given nodal arguments instead of concrete data.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"The definitions of Node and NodalMachine are coupled because every NodalMachine has Node objects in its args field (the training arguments specified in the constructor) and every Node must specify a NodalMachine, unless it is static (see below).","category":"page"},{"location":"learning_networks/#Source-nodes-1","page":"Learning Networks","title":"Source nodes","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"Only source nodes reference concrete data. A Source object has a single field, data. ","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"source(X)\nsources","category":"page"},{"location":"learning_networks/#MLJ.source-Tuple{Any}","page":"Learning Networks","title":"MLJ.source","text":"Xs = source(X)\n\nDefines a Source object out of data X. The data can be a vector, categorical vector, or table. The calling behaviour of a source node is this:\n\nXs() = X\nXs(rows=r) = selectrows(X, r)  # eg, X[r,:] for a DataFrame\nXs(Xnew) = Xnew\n\nSee also: sources, node\n\n\n\n\n\n","category":"method"},{"location":"learning_networks/#MLJ.sources","page":"Learning Networks","title":"MLJ.sources","text":"sources(N)\n\nReturn a list of all ultimate sources of  a node N. \n\nSee also: node, source\n\n\n\n\n\n","category":"function"},{"location":"learning_networks/#Nodal-machines-1","page":"Learning Networks","title":"Nodal machines","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"The key components of a NodalMachine object are:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"A model,  specifying a learning algorithm and hyperparameters.\nTraining arguments, which specify the nodes acting as proxies for training data on calls to fit!.\nA fit-result, for storing the outcomes of calls to fit!.\nA dependency tape (a vector or DAG) containing elements of type NodalMachine, obtained by merging the tapes of all training arguments.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"A nodal machine is trained in the same way as a regular machine with one difference: Instead of training the model on the wrapped data indexed on rows, it is trained on the wrapped nodes called on rows, with calling being a recursive operation on nodes within a learning network (see below).","category":"page"},{"location":"learning_networks/#Nodes-1","page":"Learning Networks","title":"Nodes","text":"","category":"section"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"The key components of a Node are:","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"An operation, which will either be static (a fixed function) or dynamic (such as predict or transform, dispatched on a nodal machine NodalMachine).\nA nodal machine on which to dispatch the operation (void if the operation is static).\nUpstream connections to other nodes (including source nodes) specified by arguments (one for each argument of the operation).\nA dependency tape, obtained by merging the the tapes of all arguments (nodal machines) and adding the present node's nodal machine.","category":"page"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"node","category":"page"},{"location":"learning_networks/#MLJ.node","page":"Learning Networks","title":"MLJ.node","text":"N = node(f::Function, args...)\n\nDefines a Node object N wrapping a static operation f and arguments args. Each of the n element of args must be a Node or Source object. The node N has the following calling behaviour:\n\nN() = f(args[1](), args[2](), ..., args[n]())\nN(rows=r) = f(args[1](rows=r), args[2](rows=r), ..., args[n](rows=r))\nN(X) = f(args[1](X), args[2](X), ..., args[n](X))\n\nJ = node(f, mach::NodalMachine, args...)\n\nDefines a dynamic Node object J wrapping a dynamic operation f (predict, predict_mean, transform, etc), a nodal machine mach and arguments args. Its calling behaviour, which depends on the outcome of training mach (and, implicitly, on training outcomes affecting its arguments) is this:\n\nJ() = f(mach, args[1](), args[2](), ..., args[n]())\nJ(rows=r) = f(mach, args[1](rows=r), args[2](rows=r), ..., args[n](rows=r))\nJ(X) = f(mach, args[1](X), args[2](X), ..., args[n](X))\n\nGenerally n=1 or n=2 in this latter case. \n\nCalling a node is a recursive operation which terminates in the call to a source node (or nodes). Calling nodes on new data X fails unless the number of source nodes is unique.  \n\nSee also: source, sources\n\n\n\n\n\n","category":"type"},{"location":"learning_networks/#","page":"Learning Networks","title":"Learning Networks","text":"fit!(N::Node; rows=nothing, verbosity=1, force=false)","category":"page"},{"location":"learning_networks/#StatsBase.fit!-Tuple{Node}","page":"Learning Networks","title":"StatsBase.fit!","text":"fit!(N::Node; rows=nothing, verbosity=1, force=false)\n\nWhen called for the first time, train all machines in the dependency tape of N, a necessary and sufficient condition for N() to be defined. Use only those rows with indices in rows, or use all rows if unspecified. \n\nIn subsequent calls to fit! the same machines are updated, but only if force=true, or if the rows specified for training are different from the last train, or if they are stale.\n\nA machine mach is stale if mach.model has changed since it was last trained, or if if one of its training arguments is stale. A node N is stale if N.machine is stale or one of its arguments is stale. A source node is never stale.\n\n\n\n\n\n","category":"method"},{"location":"simple_user_defined_models/#Simple-User-Defined-Models-1","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"","category":"section"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"To quickly implement a new supervised model in MLJ, it suffices to:","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"Define a mutable struct to store hyperparameters. This is either a subtype of Probabilistic or Deterministic, depending on whether probabilistic or ordinary point predictions are intended. This struct is the model.\nDefine a fit method, dispatched on the model, returning learned parameters, also known as the fit-result.\nDefine a predict method, dispatched on the model, and passed the fit-result, to return predictions on new patterns.","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"In the examples below, the training input X of fit, and the new input Xnew passed to predict, are tables. Each training target y is a AbstractVector.","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"The predicitions returned by predict have the same form as y for deterministic models, but are Vectors of distibutions for probabilistic models.","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"For your models to implement an optional update method, to buy into the MLJ logging protocol, or report training statistics or other model-specific functionality, a fit method with a slightly different signature and output is required. To enable checks of the scientific type of data passed to your model by MLJ's meta-algorithms, one needs to implement additional traits. A clean! method can be defined to check that hyperparameter values are within normal ranges. For details, see Adding Models for General Use.","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"For an unsupervised model, implement transform and, optionally, inverse_transform using the same signature at `predict below.","category":"page"},{"location":"simple_user_defined_models/#A-simple-deterministic-regressor-1","page":"Simple User Defined Models","title":"A simple deterministic regressor","text":"","category":"section"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"Here's a quick-and-dirty implementation of a ridge regressor with no intercept:","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"import MLJBase\nusing LinearAlgebra\n\nmutable struct SimpleRidgeRegressor <: MLJBase.Deterministic\n    lambda::Float64\nend\nSimpleRidgeRegressor(; lambda=0.1) = SimpleRidgeRegressor(lambda)\n\n# fit returns coefficients minimizing a penalized rms loss function:\nfunction MLJBase.fit(model::SimpleRidgeRegressor, X, y)\n    x = MLJBase.matrix(X)                     # convert table to matrix\n    fitresult = (x'x - model.lambda*I)\\(x'y)  # the coefficients\n    return fitresult\nend\n\n# predict uses coefficients to make new prediction:\nMLJBase.predict(model::SimpleRidgeRegressor, fitresult, Xnew) = MLJBase.matrix(Xnew)fitresult","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"After loading this code, all MLJ's basic meta-algorithms can be applied to SimpleRidgeRegressor:","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"julia> using MLJ\njulia> task = load_boston()\njulia> model = SimpleRidgeRegressor(lambda=1.0)\njulia> regressor = machine(model, task)\njulia> evaluate!(regressor, resampling=CV(), measure=rms) |> mean\n7.434221318358656\n","category":"page"},{"location":"simple_user_defined_models/#A-simple-probabilistic-classifier-1","page":"Simple User Defined Models","title":"A simple probabilistic classifier","text":"","category":"section"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"The following probabilistic model simply fits a probability distribution to the MultiClass training target (i.e., ignores X) and returns this pdf for any new pattern:","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"import MLJBase\nimport Distributions\n\nstruct MyClassifier <: MLJBase.Probabilistic\nend\n\n# `fit` ignores the inputs X and returns the training target y\n# probability distribution:\nfunction MLJBase.fit(model::MyClassifier, X, y)\n    fitresult = Distributions.fit(MLJBase.UnivariateFinite, y)\n    return fitresult\nend\n\n# `predict` retunrs the passed fitresult (pdf) for all new patterns:\nMLJBase.predict(model::MyClassifier, fitresult, Xnew) = \n    [fitresult for r in 1:nrows(Xnew)]","category":"page"},{"location":"simple_user_defined_models/#","page":"Simple User Defined Models","title":"Simple User Defined Models","text":"For more details on the UnivariateFinite distribution, query MLJBase.UnivariateFinite.","category":"page"},{"location":"adding_models_for_general_use/#Adding-New-Models-1","page":"Adding Models for General Use","title":"Adding New Models","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"This guide outlines in detail the specification of the MLJ model interface and provides guidelines for implementing the interface for models intended for general use. For sample implementations, see MLJModels/src.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The machine learning tools provided by MLJ can be applied to the models in any package that imports the package MLJBase and implements the API defined there, as outlined below. For a quick-and-dirty implementation of user-defined models see Simple User Defined Models.  To make new models available to all MLJ users, see Where to place code implementing new models.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"It is assumed the reader has read Getting Started. To implement the API described here, some familiarity with the following packages is also helpful:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Distributions.jl (for probabilistic predictions)\nCategoricalArrays.jl (essential if you are implementing a model handling data of Multiclass or OrderedFactor scitype)\nTables.jl (if you're algorithm needs input data in a novel format).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"In MLJ, the basic interface exposed to the user, built atop the model interface described here, is the machine interface. After a first reading of this document, the reader may wish to refer to MLJ Internals for context.","category":"page"},{"location":"adding_models_for_general_use/#Overview-1","page":"Adding Models for General Use","title":"Overview","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A model is an object storing hyperparameters associated with some machine learning algorithm.  In MLJ, hyperparameters include configuration parameters, like the number of threads, and special instructions, such as \"compute feature rankings\", which may or may not affect the final learning outcome.  However, the logging level (verbosity below) is excluded.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The name of the Julia type associated with a model indicates the associated algorithm (e.g., DecisionTreeClassifier). The outcome of training a learning algorithm is called a fitresult. For ordinary multilinear regression, for example, this would be the coefficients and intercept. For a general supervised model, it is the (generally minimal) information needed to make new predictions.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The ultimate supertype of all models is MLJBase.Model, which has two abstract subtypes:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"abstract type Supervised <: Model end\nabstract type Unsupervised <: Model end","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Supervised models are further divided according to whether they are able to furnish probabilistic predictions of the target (which they will then do so by default) or directly predict \"point\" estimates, for each new input pattern:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"abstract type Probabilistic <: Supervised end\nabstract type Deterministic <: Supervised end","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Further division of model types is realized through Trait declarations.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Associated with every concrete subtype of Model there must be a fit method, which implements the associated algorithm to produce the fitresult. Additionally, every Supervised model has a predict method, while Unsupervised models must have a transform method. More generally, methods such as these, that are dispatched on a model instance and a fitresult (plus other data), are called operations. Probabilistic supervised models optionally implement a predict_mode operation (in the case of classifiers) or a predict_mean and/or predict_median operations (in the case of regressors) although MLJBase also provides fallbacks that will suffice in most cases. Unsupervised models may implement an inverse_transform operation.","category":"page"},{"location":"adding_models_for_general_use/#New-model-type-declarations-and-optional-clean!-method-1","page":"Adding Models for General Use","title":"New model type declarations and optional clean! method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Here is an example of a concrete supervised model type declaration:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"import MLJ\n\nmutable struct RidgeRegressor <: MLJBase.Deterministic\n    lambda::Float64\nend","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Models (which are mutable) should not be given internal constructors. It is recommended that they be given an external lazy keyword constructor of the same name. This constructor defines default values for every field, and optionally corrects invalid field values by calling a clean! method (whose fallback returns an empty message string):","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"function MLJ.clean!(model::RidgeRegressor)\n    warning = \"\"\n    if model.lambda < 0\n        warning *= \"Need lambda ≥ 0. Resetting lambda=0. \"\n        model.lambda = 0\n    end\n    return warning\nend\n\n# keyword constructor\nfunction RidgeRegressor(; lambda=0.0)\n    model = RidgeRegressor(lambda)\n    message = MLJBase.clean!(model)\n    isempty(message) || @warn message\n    return model\nend","category":"page"},{"location":"adding_models_for_general_use/#Supervised-models-1","page":"Adding Models for General Use","title":"Supervised models","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The compulsory and optional methods to be implemented for each concrete type SomeSupervisedModel <: MLJBase.Supervised are summarized below. An = indicates the return value for a fallback version of the method.","category":"page"},{"location":"adding_models_for_general_use/#Summary-of-methods-1","page":"Adding Models for General Use","title":"Summary of methods","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Compulsory:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.fit(model::SomeSupervisedModel, verbosity::Integer, X, y) -> fitresult, cache, report\nMLJBase.predict(model::SomeSupervisedModel, fitresult, Xnew) -> yhat","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Fallback to be overridden if model input is univariate:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.input_is_multivariate(::Type{<:SomeSupervisedModel}) = true","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Optional, to check and correct invalid hyperparameter values:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.clean!(model::SomeSupervisedModel) = \"\" ","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Optional, to return user-friendly form of fitted parameters:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.fitted_params(model::SomeSupervisedModel, fitresult) = fitresult","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Optional, to avoid redundant calculations when re-fitting machines:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.update(model::SomeSupervisedModel, verbosity, old_fitresult, old_cache, X, y) =\n   MLJBase.fit(model, verbosity, X, y)","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Optional, if SomeSupervisedModel <: Probabilistic:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.predict_mode(model::SomeSupervisedModel, fitresult, Xnew) =\n    mode.(predict(model, fitresult, Xnew))\nMLJBase.predict_mean(model::SomeSupervisedModel, fitresult, Xnew) =\n    mean.(predict(model, fitresult, Xnew))\nMLJBase.predict_median(model::SomeSupervisedModel, fitresult, Xnew) =\n    median.(predict(model, fitresult, Xnew))","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Required, if model is to be registered (findable by general users):","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.load_path(::Type{<:SomeSupervisedModel})    = \"\"\nMLJBase.package_name(::Type{<:SomeSupervisedModel}) = \"Unknown\"\nMLJBase.package_uuid(::Type{<:SomeSupervisedModel}) = \"Unknown\"","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Recommended, to constrain the form of input data passed to fit and predict:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.input_scitype_union(::Type{<:SomeSupervisedModel}) = Union{Missing,Found}","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Recommended, to constrain the form of target data passed to fit (and compulsory if target is multivariate/sequential):","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.target_scitype_union(::Type{<:SomeSupervisedModel}) = Union{Found,NTuple{<:Found}}","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Optional but recommended:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.package_url(::Type{<:SomeSupervisedModel})  = \"Unknown\"\nMLJBase.is_pure_julia(::Type{<:SomeSupervisedModel}) = false","category":"page"},{"location":"adding_models_for_general_use/#The-form-of-data-for-fitting-and-predicting-1","page":"Adding Models for General Use","title":"The form of data for fitting and predicting","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The inputs X and Xnew for fit and predict are always tables, unless one defines","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.input_is_multivariate(::Type{<:SomeSupervisedModel}) = false","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The target y is always an AbstractVector (see the discussion in Getting Started). For multivariate or sequence-valued targets, a target_scitype_union declaration is required. This is discussed under Trait declarations below, which also describes how to constrain the element types of data.","category":"page"},{"location":"adding_models_for_general_use/#Additional-type-coercions-1","page":"Adding Models for General Use","title":"Additional type coercions","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"If the core algorithm being wrapped requires data in a different or more specific form, then fit will need to coerce the table into the form desired (and the same coercions applied to X will have to be repeated for Xnew in predict). To assist with common cases, MLJ provides the convenience method MLJBase.matrix. MLJBase.matrix(Xtable) has type Matrix{T} where T is the tightest common type of elements of Xtable, and Xtable is any table.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Other auxiliary methods provided by MLJBase for handling tabular data are: selectrows, selectcols, select and schema (for extracting the size, names and eltypes of a table). See Convenience methods below for details.","category":"page"},{"location":"adding_models_for_general_use/#Important-convention-1","page":"Adding Models for General Use","title":"Important convention","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"It is to be understood that the columns of the table X correspond to features and the rows to patterns.","category":"page"},{"location":"adding_models_for_general_use/#The-fit-method-1","page":"Adding Models for General Use","title":"The fit method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A compulsory fit method returns three objects:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.fit(model::SomeSupervisedModel, verbosity::Int, X, y) -> fitresult, cache, report","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Note: The Int typing of verbosity cannot be omitted.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"fitresult is the fitresult in the sense above (which becomes an  argument for predict discussed below).\nreport is a (possibly empty) NamedTuple, for example,  report=(deviance=..., dof_residual=..., stderror=..., vcov=...).  Any training-related statistics, such as internal estimates of the  generalization error, and feature rankings, should be returned in  the report tuple. How, or if, these are generated should be  controlled by hyperparameters (the fields of model). Fitted  parameters, such as the coefficients of a linear model, do not go  in the report as they will be extractable from fitresult (and  accessible to MLJ through the fitted_params method described below).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"3.\tThe value of cache can be nothing, unless one is also defining       an update method (see below). The Julia type of cache is not       presently restricted.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"It is not necessary for fit to provide dimension checks or to call clean! on the model; MLJ will carry out such checks.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The method fit should never alter hyperparameter values. If the package is able to suggest better hyperparameters, as a byproduct of training, return these in the report field.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The verbosity level (0 for silent) is for passing to learning algorithm itself. A fit method wrapping such an algorithm should generally avoid doing any of its own logging.","category":"page"},{"location":"adding_models_for_general_use/#The-fitted_params-method-1","page":"Adding Models for General Use","title":"The fitted_params method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A fitted_params method may be optionally overloaded. It's purpose is to provide MLJ access to a user-friendly representation of the learned parameters of the model (as opposed to the hyperparameters). They must be extractable from fitresult.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.fitted_params(model::SomeSupervisedModel, fitresult) -> friendly_fitresult::NamedTuple","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For a linear model, for example, one might declare something like friendly_fitresult=(coefs=[...], bias=...).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The fallback is to return (fitresult=fitresult,).","category":"page"},{"location":"adding_models_for_general_use/#The-predict-method-1","page":"Adding Models for General Use","title":"The predict method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"A compulsory predict method has the form","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.predict(model::SomeSupervisedModel, fitresult, Xnew) -> yhat","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Here Xnew will be have the same form as the X passed to fit.","category":"page"},{"location":"adding_models_for_general_use/#Prediction-types-for-deterministic-responses.-1","page":"Adding Models for General Use","title":"Prediction types for deterministic responses.","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"In the case of Deterministic models, yhat should be an AbstractVector (commonly a plain Vector) with the same element type as the target y passed to the fit method (see above). Any CategoricalValue or CategoricalString appearing in yhat must have the same levels in its pool as was present in the elements of the target y presented in training, even if not all levels appear in the training data or prediction itself. For example, in the univariate target case, this means MLJ.classes(yhat[i]) = MLJ.classes(y[j]) for all admissible i and j. (The method classes is described under Convenience methods below).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Unfortunately, code not written with the preservation of categorical levels in mind poses special problems. To help with this, MLJBase provides three utility methods: int (for converting a CategoricalValue or CategoricalString into an integer, the ordering of these integers being consistent with that of the pool), decoder (for constructing a callable object that decodes the integers back into CategoricalValue/CategoricalString objects), and classes, for extracting the complete pool from a single value. Refer to Convenience methods below for important details.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Note that a decoder created during fit may need to be bundled with fitresult to make it available to predict during re-encoding. So, for example, if the core algorithm being wrapped by fit expects a nominal target yint of type Vector{<:Integer} then a fit method may look something like this:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"function MLJBase.fit(model::SomeSupervisedModel, verbosity, X, y)\n    yint = MLJBase.int(y) \n    a_target_element = y[1]                    # a CategoricalValue/String\n    decode = MLJBase.decoder(a_target_element) # can be called on integers\n\t\n    core_fitresult = SomePackage.fit(X, yint, verbosity=verbosity)\n\n    fitresult = (decode, core_fitresult)\n    cache = nothing\n    report = nothing\n    return fitresult, cache, report\nend","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"while a corresponding deterministic predict operation might look like this:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"function MLJBase.predict(model::SomeSupervisedModel, fitresult, Xnew)\n    decode, core_fitresult = fitresult\n    yhat = SomePackage.predict(core_fitresult, Xnew)\n    return decode.(yhat)  # or decode(yhat) also works\nend","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For a concrete example, refer to the code for SVMClassifier.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Of course, if you are coding a learning algorithm from scratch, rather than wrapping an existing one, these extra measures may be unnecessary.","category":"page"},{"location":"adding_models_for_general_use/#Prediction-types-for-probabilistic-responses-1","page":"Adding Models for General Use","title":"Prediction types for probabilistic responses","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"In the case of Probabilistic models with univariate targets, yhat must be a Vector whose elements are distributions (one distribution per row of Xnew).","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Presently, a distribution is any object d for which MLJBase.isdistribution(::d) = true, which includes all objects of type Distributions.Distribution from the package Distributions.jl. (Soon any Distributions.Sampleable will be included.) The declaration MLJBase.isdistribution(::d) = true implies that at least Base.rand(d) is implemented, but the rest of this API is still a work-in-progress.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Use the distribution MLJBase.UnivariateFinite for Probabilistic models predicting a target with Finite scitype (classifiers). In this case each element of the training target y is a CategoricalValue or CategoricalString, as in this contrived example:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"using CategoricalArrays\ny = identity.(categorical([:yes, :no, :no, :maybe, :maybe]))","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Note that, as in this case, we cannot assume y is a CategoricalVector, and we rely on elements for pool information (if we need it); this is accessible using the convenience method MLJ.classes:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"julia> yes = y[1]\njulia> levels = MLJBase.classes(yes)\n3-element Array{CategoricalValue{Symbol,UInt32},1}:\n :maybe\n :no\n :yes","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Now supposing that, for some new input pattern, the elements yes = y[1] and no = y[2] are to be assigned respective probabilities of 0.2 and 0.8. Then the corresponding distribution d is constructed as follows:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"julia> d = MLJBase.UnivariateFinite([yes, no], [0.2, 0.8])\nUnivariateFinite{CategoricalValue{Symbol,UInt32},Float64}(Dict(:yes=>0.2,:maybe=>0.0,:no=>0.8))\n\njulia> pdf(d, yes)\n0.2\n\njulia> maybe = y[4]; pdf(d, maybe)\n0.0","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Alternatively, a dictionary can be passed to the constructor. ","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.UnivariateFinite","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.UnivariateFinite","page":"Adding Models for General Use","title":"MLJBase.UnivariateFinite","text":"UnivariateFinite(levels, p)\n\nA discrete univariate distribution whose finite support is the elements of the vector levels, and whose corresponding probabilities are elements of the vector p, which must sum to one.\n\nIn the special case that levels has type AbstractVector{L} where L <: CategoricalValue or L <: CategoricalString (for example levels is a CategoricalVector) the constructor adds the unobserved classes (from the common pool) with probability zero.\n\nUnivariateFinite(prob_given_level)\n\nA discrete univariate distribution whose finite support is the set of keys of the provided dictionary, prob_given_level. The dictionary values specify the corresponding probabilities, which must be nonnegative and sum to one. \n\nIn the special case that keys(prob_given_level) has type AbstractVector{L} where L <: CategoricalValue or L <: CategoricalString (for example it is a CategoricalVector) the constructor adds the unobserved classes from the common pool with probability zero.\n\nlevels(d::UnivariateFinite)\n\nReturn the levels of d.\n\nd = UnivariateFinite([\"yes\", \"no\", \"maybe\"], [0.1, 0.2, 0.7])\npdf(d, \"no\") # 0.2\nmode(d) # \"maybe\"\nrand(d, 5) # [\"maybe\", \"no\", \"maybe\", \"maybe\", \"no\"]\nd = fit(UnivariateFinite, [\"maybe\", \"no\", \"maybe\", \"yes\"])\npdf(d, \"maybe\") ≈ 0.5 # true\nlevels(d) # [\"yes\", \"no\", \"maybe\"]\n\nIf the element type of v is a CategoricalValue or CategoricalString, then fit(UnivariateFinite, v) assigns a probability of zero to unobserved classes from the common pool.\n\nSee also classes\n\n\n\n\n\n","category":"type"},{"location":"adding_models_for_general_use/#Trait-declarations-1","page":"Adding Models for General Use","title":"Trait declarations","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Two trait functions allow the implementer to restrict the types of elements appearing in the inputs X, and Xnew passed to fit and predict, and the elements appearing in the training target y. The MLJ task interface also uses these traits to match models to tasks. So if they are omitted (and your model is registered) then a general user may attempt to use your model with inappropriately typed data.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The trait functions input_scitype_union and target_scitype_union take scientific data types as values (see Getting Started for scitype basics). These types are organized in the following hierarchy:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"(Image: )","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For example,  to ensure that elements of X presented to the DecisionTreeClassifier fit method all have Continuous scitype (and hence AbstractFloat machine type), one declares","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.input_scitype_union(::Type{<:DecisionTreeClassifier}) = MLJBase.Continuous","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"For, in general, MLJ will never call fit(model::SomeSuperivsedModel, verbosity, X, y) unless Union{scitypes(X)...} <: inputs_scitype_union(SomeSupervisedModel) holds. (See Convenience methods below for more on the scitypes and related scitype_union methods.)","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Similarly, one declares","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.target_scitype_union(::Type{<:DecisionTreeClassifier}) = MLJBase.Finite","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"to ensure that all elements of the target y (which is always an AbstractVector) have Finite scitype (and hence CategoricalValue or CategoricalString machine type). This is because, in the general case, MLJ guarantees that scitype_union(y) <: target_scitype_union(SomeSupervisedModel).","category":"page"},{"location":"adding_models_for_general_use/#Multivariate-targets-1","page":"Adding Models for General Use","title":"Multivariate targets","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The above remarks continue to hold unchanged for the case multivariate targets.  In this case the elements of the AbstractVector y are now tuples. If, for example, you declare","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"target_scitype_union(SomeSupervisedModel) = Tuple{Continuous,Count}","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"then each element of y will be a tuple of type Tuple{AbstractFloat,Integer}. For predicting variable length sequences of, say, binary values, use","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"target_scitype_union(SomeSupervisedModel) = NTuple{<:Multiclass{2}}","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"The trait functions controlling the form of data are summarized as follows:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"method return type declarable return values default value\ninput_is_multivariate Bool true or false true\ninput_scitype_union DataType subtype of Union{Missing,Found} Union{Missing,Found}\ntarget_scitype_union DataType subtype of Found or tuple of such types Union{Found,NTuple{<:Found}}","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Additional trait functions tell MLJ's @load macro how to find your model if it is registered, and provide other self-explanatory metadata about the model:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"method return type declarable return values default value\nload_path String unrestricted \"unknown\"\npackage_name String unrestricted \"unknown\"\npackage_uuid String unrestricted \"unknown\"\npackage_url String unrestricted \"unknown\"\nis_pure_julia Bool true or false false","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Here is the complete list of trait function declarations for DecistionTreeClassifier  (source):","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.input_is_multivariate(::Type{<:DecisionTreeClassifier}) = true\nMLJBase.input_scitype_union(::Type{<:DecisionTreeClassifier}) = MLJBase.Continuous\nMLJBase.target_scitype_union(::Type{<:DecisionTreeClassifier}) = MLJBase.Finite\nMLJBase.load_path(::Type{<:DecisionTreeClassifier}) = \"MLJModels.DecisionTree_.DecisionTreeClassifier\" \nMLJBase.package_name(::Type{<:DecisionTreeClassifier}) = \"DecisionTree\"\nMLJBase.package_uuid(::Type{<:DecisionTreeClassifier}) = \"7806a523-6efd-50cb-b5f6-3fa6f1930dbb\"\nMLJBase.package_url(::Type{<:DecisionTreeClassifier}) = \"https://github.com/bensadeghi/DecisionTree.jl\"\nMLJBase.is_pure_julia(::Type{<:DecisionTreeClassifier}) = true","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"You can test all your declarations of traits by calling info(SomeModel).","category":"page"},{"location":"adding_models_for_general_use/#Iterative-models-and-the-update!-method-1","page":"Adding Models for General Use","title":"Iterative models and the update! method","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"An update method may be optionally overloaded to enable a call by MLJ to retrain a model (on the same training data) to avoid repeating computations unnecessarily.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.update(model::SomeSupervisedModel, verbosity, old_fitresult, old_cache, X, y) -> fitresult, cache, report","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"If an MLJ Machine is being fit! and it is not the first time, then update is called instead of fit, unless the machine fit! has been called with a new rows keyword argument. However, MLJBase defines a fallback for update which just calls fit. For context, see MLJ Internals.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Learning networks wrapped as models constitute one use-case (see Learning Networks): One would like each component model to be retrained only when hyperparameter changes \"upstream\" make this necessary. In this case MLJ provides a fallback (specifically, the fallback is for any subtype of SupervisedNetwork = Union{DeterministicNetwork,ProbabilisticNetwork}). A second more generally relevant use-case is iterative models, where calls to increase the number of iterations only restarts the iterative procedure if other hyperparameters have also changed. For an example, see the MLJ ensemble code.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"In the event that the argument fitresult (returned by a preceding call to fit) is not sufficient for performing an update, the author can arrange for fit to output in its cache return value any additional information required, as this is also passed as an argument to the update method.","category":"page"},{"location":"adding_models_for_general_use/#Unsupervised-models-1","page":"Adding Models for General Use","title":"Unsupervised models","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"TODO","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"transform should return a table unless output_is_multivariate is","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"set false. Convenience method: table (for materializing an AbstractMatrix, or named tuple of vectors, as a table matching a given prototype)","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"instead of target_scitype_union have output_scitype_union\ninput_is_multivariate and input_scitype_union are the same ","category":"page"},{"location":"adding_models_for_general_use/#Convenience-methods-1","page":"Adding Models for General Use","title":"Convenience methods","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.int","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.int","page":"Adding Models for General Use","title":"MLJBase.int","text":"int(x)\n\nThe positional integer of the CategoricalString or CategoricalValue x, in the ordering defined by the pool of x. The type of int(x) is the refrence type of x.\n\nNot to be confused with x.ref, which is unchanged by reordering of the pool of x, but has the same type.\n\nint(X::CategoricalArray)\nint(W::Array{<:CategoricalString})\nint(W::Array{<:CategoricalValue})\n\nBroadcasted versions of int.\n\njulia> v = categorical([:c, :b, :c, :a])\njulia> levels(v)\n3-element Array{Symbol,1}:\n :a\n :b\n :c\njulia> int(v)\n4-element Array{UInt32,1}:\n 0x00000003\n 0x00000002\n 0x00000003\n 0x00000001\n\nSee also: decoder\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.classes","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.classes","page":"Adding Models for General Use","title":"MLJBase.classes","text":"classes(x)\n\nAll the categorical values in the same pool as x (including x), returned as a list, with an ordering consistent with the pool. Here x has CategoricalValue or CategoricalString type, and classes(x) is a vector of the same eltype. \n\nNot to be confused with the levels of x.pool which have a different type. In particular, while x in classes(x) is always true, x in x.pool.levels is not true.\n\njulia> v = categorical([:c, :b, :c, :a])\njulia> levels(v)\n3-element Array{Symbol,1}:\n :a\n :b\n :c\njulia> classes(v[4])\n3-element Array{CategoricalValue{Symbol,UInt32},1}:\n :a\n :b\n :c\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.decoder","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.decoder","page":"Adding Models for General Use","title":"MLJBase.decoder","text":"d = decoder(x)\n\nA callable object for decoding the integer representation of a CategoricalString or CategoricalValue sharing the same pool as x. (Here x is of one of these two types.) Specifically, one has d(int(y)) == y for all y in classes(x). One can also call d on integer arrays, in which case d is broadcast over all elements.\n\njulia> v = categorical([:c, :b, :c, :a])\njulia> int(v)\n4-element Array{UInt32,1}:\n 0x00000003\n 0x00000002\n 0x00000003\n 0x00000001\njulia> d = decoder(v[3])\njulia> d(int(v)) == v\ntrue\n\nSee also: int, classes\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.matrix","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.matrix","page":"Adding Models for General Use","title":"MLJBase.matrix","text":"MLJBase.matrix(X)\n\nConvert a table source X into an Matrix; or, if X is a AbstractMatrix, return X. Optimized for column-based sources.\n\nIf instead X is a sparse table, then a SparseMatrixCSC object is returned. The integer relabelling of column names follows the lexicographic ordering (as indicated by schema(X).names).\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.table","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.table","page":"Adding Models for General Use","title":"MLJBase.table","text":"MLJBase.table(cols; prototype=cols)\n\nConvert a named tuple of vectors cols, into a table. The table type returned is the \"preferred sink type\" for prototype (see the Tables.jl documentation). \n\nMLJBase.table(X::AbstractMatrix; names=nothing, prototype=nothing)\n\nConvert an abstract matrix X into a table with names (a tuple of symbols) as column names, or with labels (:x1, :x2, ..., :xn) where n=size(X, 2), if names is not specified.  If prototype=nothing, then a named tuple of vectors is returned.\n\nEquivalent to table(cols, prototype=prototype) where cols is the named tuple of columns of X, with keys(cols) = names.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.select","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.select","page":"Adding Models for General Use","title":"MLJBase.select","text":"select(X, r, c)\n\nSelect element of a table or sparse table at row r and column c. In the case of sparse data where the key (r, c), zero or missing is returned, depending on the value type.\n\nSee also: selectrows, selectcols\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.selectrows","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.selectrows","page":"Adding Models for General Use","title":"MLJBase.selectrows","text":"selectrows(X, r)\n\nSelect single or multiple rows from any table, sparse table, or abstract vector X.  If X is tabular, the object returned is a table of the preferred sink type of typeof(X), even a single row is selected.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.selectcols","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.selectcols","page":"Adding Models for General Use","title":"MLJBase.selectcols","text":"selectcols(X, c)\n\nSelect single or multiple columns from any table or sparse table X. If c is an abstract vector of integers or symbols, then the object returned is a table of the preferred sink type of typeof(X). If c is a single integer or column, then a Vector or CategoricalVector is returned.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.schema","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.schema","page":"Adding Models for General Use","title":"MLJBase.schema","text":"schema(X)\n\nReturns a struct with properties names, types with the obvious meanings. Here X is any table or sparse table.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.nrows","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.nrows","page":"Adding Models for General Use","title":"MLJBase.nrows","text":"nrows(X)\n\nReturn the number of rows in a table, sparse table, or abstract vector.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.scitype","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.scitype","page":"Adding Models for General Use","title":"MLJBase.scitype","text":"scitype(x)\n\nReturn the scientific type for scalar values that object x can represent. If x is a tuple, then Tuple{scitype.(x)...} is returned. \n\njulia> scitype(4.5)\nContinous\n\njulia> scitype(\"book\")\nUnknown\n\njulia> scitype((1, 4.5))\nTuple{Count,Continuous}\n\njulia> using CategoricalArrays\njulia> v = categorical([:m, :f, :f])\njulia> scitype(v[1])\nMulticlass{2}\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.scitype_union","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.scitype_union","page":"Adding Models for General Use","title":"MLJBase.scitype_union","text":"scitype_union(A)\n\nReturn the type union, over all elements x generated by the iterable A, of scitype(x).\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"MLJBase.scitypes","category":"page"},{"location":"adding_models_for_general_use/#MLJBase.scitypes","page":"Adding Models for General Use","title":"MLJBase.scitypes","text":"scitypes(X)\n\nReturns a named tuple keyed on the column names of the table X with values the corresponding scitype unions over a column's entries.\n\n\n\n\n\n","category":"function"},{"location":"adding_models_for_general_use/#Where-to-place-code-implementing-new-models-1","page":"Adding Models for General Use","title":"Where to place code implementing new models","text":"","category":"section"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Note that different packages can implement models having the same name without causing conflicts, although an MLJ user cannot simultaneously load two such models.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"There are two options for making a new model implementation available to all MLJ users:","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Native implementations (preferred option). The implementation code lives in the same package that contains the learning algorithms implementing the interface. In this case, it is sufficient to open an issue at MLJRegistry requesting the package to be registered with MLJ. Registering a package allows the MLJ user to access its models' metadata and to selectively load them.\nExternal implementations (short-term alternative). The model implementation code is necessarily separate from the package SomePkg defining the learning algorithm being wrapped. In this case, the recommended procedure is to include the implementation code at MLJModels/src via a pull-request, and test code at MLJModels/test. Assuming SomePkg is the only package imported by the implementation code, one needs to: (i) register SomePkg at MLJRegistry as explained above; and (ii) add a corresponding @require line in the PR to MLJModels/src/MLJModels.jl to enable lazy-loading of that package by MLJ (following the pattern of existing additions). If other packages must be imported, add them to the MLJModels project file after checking they are not already there. If it is really necessary, packages can be also added to Project.toml for testing purposes.","category":"page"},{"location":"adding_models_for_general_use/#","page":"Adding Models for General Use","title":"Adding Models for General Use","text":"Additionally, one needs to ensure that the implementation code defines the package_name and load_path model traits appropriately, so that MLJ's @load macro can find the necessary code (see MLJModels/src for examples). The @load command can only be tested after registration. If changes are made, lodge an issue at MLJRegistry to make the changes available to MLJ.  ","category":"page"},{"location":"internals/#Internals-1","page":"Internals","title":"Internals","text":"","category":"section"},{"location":"internals/#The-machine-interface,-simplified-1","page":"Internals","title":"The machine interface, simplified","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"The following is simplified description of the Machine interface. See also the Glossary","category":"page"},{"location":"internals/#The-Machine-type-1","page":"Internals","title":"The Machine type","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"mutable struct Machine{M<Model}\n\n    model::M\n    fitresult\n    cache\n    args::Tuple    # e.g., (X, y) for supervised models\n    report\n    rows # remember last rows used \n    \n    function Machine{M}(model::M, args...) where M<:Model\n        machine = new{M}(model)\n        machine.args = args\n        machine.report = Dict{Symbol,Any}()\n        return machine\n    end\n\nend","category":"page"},{"location":"internals/#Constructor-1","page":"Internals","title":"Constructor","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"machine(model::M, Xtable, y) = Machine{M}(model, Xtable, y)","category":"page"},{"location":"internals/#fit!-and-predict/transform-1","page":"Internals","title":"fit! and predict/transform","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"function fit!(machine::Machine; rows=nothing, verbosity=1) \n\n    warning = clean!(mach.model)\n    isempty(warning) || verbosity < 0 || @warn warning \n\n    if rows == nothing\n        rows = (:) \n    end\n\n    rows_have_changed  = (!isdefined(mach, :rows) || rows != mach.rows)\n\n    args = [MLJ.selectrows(arg, rows) for arg in mach.args]\n\t\n    if !isdefined(mach, :fitresult) || rows_have_changed || force \n        mach.fitresult, mach.cache, report =\n            fit(mach.model, verbosity, args...)\n    else # call `update`:\n        mach.fitresult, mach.cache, report =\n            update(mach.model, verbosity, mach.fitresult, mach.cache, args...)\n    end\n\n    if rows_have_changed\n        mach.rows = deepcopy(rows)\n    end\n\n    if report != nothing\n        merge!(mach.report, report)\n    end\n\n    return mach\n\nend\n\nfunction predict(machine::Machine{<:Supervised}, Xnew)\n    if isdefined(machine, :fitresult)\n        return predict(machine.model, machine.fitresult, Xnew))\n    else\n        throw(error(\"$machine is not trained and so cannot predict.\"))\n    end\nend\n\nfunction transform(machine::Machine{<:Unsupervised}, Xnew)\n    if isdefined(machine, :fitresult)\n        return transform(machine.model, machine.fitresult, Xnew))\n    else\n        throw(error(\"$machine is not trained and so cannot transform.\"))\n    end\nend","category":"page"},{"location":"glossary/#Glossary-1","page":"Glossary","title":"Glossary","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Note: This glossary includes some detail intended mainly for MLJ developers.","category":"page"},{"location":"glossary/#Basics-1","page":"Glossary","title":"Basics","text":"","category":"section"},{"location":"glossary/#task-(object-of-type-Task)-1","page":"Glossary","title":"task (object of type Task)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Data plus a learning objective (e.g., \"probabilistic prediction of Sales\"). In MLJ a task does not include a description of how the completed task is to be evaluated.","category":"page"},{"location":"glossary/#hyperparameters-1","page":"Glossary","title":"hyperparameters","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Parameters on which some learning algorithm depends, specified before the algorithm is applied, and where learning is interpreted in the broadest sense. For example, PCA feature reduction is a \"preprocessing\" transformation \"learning\" a projection from training data, governed by a dimension hyperparameter. Hyperparameters in our sense may specify configuration (eg, number of parallel processes) even when this does not effect the end-product of learning. (But we exlcude verbosity level.)","category":"page"},{"location":"glossary/#model-(object-of-abstract-type-Model)-1","page":"Glossary","title":"model (object of abstract type Model)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Object collecting together hyperameters of a single algorithm. Most models are classified either as supervised or unsupervised models (generally, \"transformers\").","category":"page"},{"location":"glossary/#fit-result-(type-generally-defined-outside-of-MLJ)-1","page":"Glossary","title":"fit-result (type generally defined outside of MLJ)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Also known as \"learned\" or \"fitted\" parameters, these are \"weights\", \"coefficients\", or similar paramaters learned by an algorithm, after adopting the prescribed hyperparameters. For example, decision trees of a random forest, the coefficients and intercept of a linear model, or the rotation and projection matrices of PCA reduction scheme.","category":"page"},{"location":"glossary/#operation-1","page":"Glossary","title":"operation","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Data-manipulating operations (methods) parameterized by some fit-result. For supervised learners, the predict or predict_mode methods, for transformers, the transform or inverse_transform method. In some contexts, such an operation might be replaced by an ordinary operation (method) that does not depend on an fit-result, which are then then called static operations for clarity. An operation that is not static is dynamic.","category":"page"},{"location":"glossary/#machine-(object-of-type-Machine)-1","page":"Glossary","title":"machine (object of type Machine)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"An object consisting of:","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(1) A model ","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(2) A fit-result (undefined until training)","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(3) Training arguments (one for each data argument of the model's associated fit method). A training argument is data used for training. Generally, there are two training arguments for supervised models, and just one for unsuperivsed models.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"In addition machines store \"report\" metadata, for recording algorithm-specific statistics of training (eg, internal estimate of generalization error, feature importances); and they cache information allowing the fit-result to be updated without repeating unnecessary information.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Trainable models are trained by calls to a fit method which may be passed an optional argument specifying the rows of data to be used in training.","category":"page"},{"location":"glossary/#Learning-Networks-and-Composite-Models-1","page":"Glossary","title":"Learning Networks and Composite Models","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Note: Multiple nodal machines may share the same model, and multiple learning nodes may share the same nodal machine.","category":"page"},{"location":"glossary/#source-node-(object-of-type-Source)-1","page":"Glossary","title":"source node (object of type Source)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"A container for training data and point of entry for new data in a learning network (see below).","category":"page"},{"location":"glossary/#nodal-machine-(object-of-type-NodalMachine)-1","page":"Glossary","title":"nodal machine (object of type NodalMachine)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Like a machine with the following exceptions:","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(1) Training arguments are source nodes or regular nodes (see below) in the learning network, instead of data.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(2) The object internally records dependencies on other other nodal machines, as implied by the training arguments, and so on. ","category":"page"},{"location":"glossary/#node-(object-of-type-Node)-1","page":"Glossary","title":"node (object of type Node)","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Essentially a nodal machine wrapped in an associated operation (e.g., predict or inverse_transform). It detail, it consists of:","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(1) An operation, static or dynamic.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(2) A nodal machine, void if the operation is static.","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(3) Upstream connections to other learning or source nodes, specified by a list    of arguments (one for each argument of the operation).","category":"page"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"(4) Metadata recording the dependencies of the object's machine, and the dependecies on other nodal machines implied by its arguments.","category":"page"},{"location":"glossary/#learning-network-1","page":"Glossary","title":"learning network","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"An acyclic directed graph implicit in the connections of a collection of source(s) and nodes. Each connected component is ordinarily restricted to have a unique source.","category":"page"},{"location":"glossary/#wrapper-1","page":"Glossary","title":"wrapper","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Any model with one or more other models as hyperparameters.","category":"page"},{"location":"glossary/#composite-model-1","page":"Glossary","title":"composite model","text":"","category":"section"},{"location":"glossary/#","page":"Glossary","title":"Glossary","text":"Any wrapper, or any learning network \"exported\" as a model (see Learning Networks).","category":"page"},{"location":"api/#API-1","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Functions-1","page":"API","title":"Functions","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"Modules = [MLJ,MLJBase,MLJModels]","category":"page"},{"location":"api/#MLJ.EnsembleModel-Tuple{}","page":"API","title":"MLJ.EnsembleModel","text":"EnsembleModel(atom=nothing, \n              weights=Float64[],\n              bagging_fraction=0.8,\n              rng=GLOBAL_RNG, n=100,\n              parallel=true,\n              out_of_bag_measure=[])\n\nCreate a model for training an ensemble of n learners, with optional bagging, each with associated model atom. Ensembling is useful if fit!(machine(atom, data...)) does not create identical models on repeated calls (ie, is a stochastic model, such as a decision tree with randomized node selection criteria), or if bagging_fraction is set to a value less than 1.0, or both. The constructor fails if no atom is specified.\n\nIf rng is an integer, then MersenneTwister(rng) is the random number generator used for bagging. Otherwise some AbstractRNG object is expected.\n\nPredictions are weighted according to the vector weights (to allow for external optimization) except in the case that atom is a Deterministic classifier. Uniform weights are used if weight has zero length.\n\nThe ensemble model is Deterministic or Probabilistic, according to the corresponding supertype of atom. In the case of deterministic classifiers (target_scitype_union(atom) <: Finite), the predictions are majority votes, and for regressors (target_scitype_union(atom)<: Continuous) they are ordinary averages. Probabilistic predictions are obtained by averaging the atomic probability distribution/mass functions; in particular, for regressors, the ensemble prediction on each input pattern has the type MixtureModel{VF,VS,D} from the Distributions.jl package, where D is the type of predicted distribution for atom.\n\nIf a single measure or non-empty vector of measusres is specified by out_of_bag_measure, then out-of-bag estimates of performance are reported.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.TunedModel-Tuple{}","page":"API","title":"MLJ.TunedModel","text":"tuned_model = TunedModel(; model=nothing,\n                         tuning=Grid(),\n                         resampling=Holdout(),\n                         measure=nothing,\n                         operation=predict,\n                         nested_ranges=NamedTuple(),\n                         minimize=true,\n                         full_report=true)\n\nConstruct a model wrapper for hyperparameter optimization of a supervised learner.\n\nCalling fit!(mach) on a machine mach=machine(tuned_model, X, y) will: (i) Instigate a search, over clones of model with the hyperparameter mutations specified by nested_ranges, for that model optimizing the specified measure, according to evaluations carried out using the specified tuning strategy and resampling strategy; and (ii) Fit a machine, mach_optimal = mach.fitresult, wrapping the optimal model object in all the provided data X, y. Calling predict(mach, Xnew) then returns predictions on Xnew of the machine mach_optimal.\n\nIf measure is a score, rather than a loss, specify minimize=false.\n\nThe optimal clone of model is accessible as fitted_params(mach).best_model. In the case of two-parameter tuning, a Plots.jl plot of performance estimates is returned by plot(mach) or heatmap(mach).\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.coerce-Tuple{Dict,Any}","page":"API","title":"MLJ.coerce","text":"coerce(d::Dict, X)\n\nReturn a copy of the table X with columns named in the keys of d coerced to have scitype_union equal to the corresponding value. \n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.coerce-Tuple{Type{Continuous},AbstractArray{#s77,1} where #s77<:Number}","page":"API","title":"MLJ.coerce","text":"coerce(T, v::AbstractVector)\n\nCoerce the machine types of elements of v to ensure the returned vector has T as its scitype_union, or Union{Missing,T}, if v has missing values.\n\njulia> v = coerce(Continuous, [1, missing, 5])\n3-element Array{Union{Missing, Float64},1}:\n 1.0     \n missing\n 5.0  \n\njulia> scitype_union(v)\nUnion{Missing,Continuous}\n\nSee also scitype, scitype_union, scitypes\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.evaluate!-Tuple{Machine}","page":"API","title":"MLJ.evaluate!","text":"evaluate!(mach, resampling=CV(), measure=nothing, operation=predict, verbosity=1)\n\nEstimate the performance of a machine mach using the specified resampling strategy (defaulting to 6-fold cross-validation) and measure, which can be a single measure or vector. \n\nAlthough evaluate! is mutating, mach.model and mach.args are preserved.\n\nResampling and testing is based exclusively on data in rows, when specified.\n\nIf no measure is specified, then default_measure(mach.model) is used, unless this default is nothing and an error is thrown.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.iterator-Union{Tuple{M}, Tuple{M,NamedTuple}} where M<:Model","page":"API","title":"MLJ.iterator","text":"iterator(model::Model, param_iterators::NamedTuple)\n\nIterator over all models of type typeof(model) defined by param_iterators.\n\nEach name in the nested :name => value pairs of param_iterators should be the name of a (possibly nested) field of model; and each element of flat_values(param_iterators) (the corresponding final values) is an iterator over values of one of those fields.\n\nSee also iterator and params.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.learning_curve!-Tuple{Machine{#s74} where #s74<:Supervised}","page":"API","title":"MLJ.learning_curve!","text":"curve = learning_curve!(mach; resolution=30, resampling=Holdout(), measure=rms, operation=predict, nested_range=nothing, n=1)\n\nGiven a supervised machine mach, returns a named tuple of objects needed to generate a plot of performance measurements, as a function of the single hyperparameter specified in nested_range. The tuple curve has the following keys: :parameter_name, :parameter_scale, :parameter_values, :measurements.\n\nFor n not equal to 1, multiple curves are computed, and the value of curve.measurements is an array, one column for each run. This is useful in the case of models with indeterminate fit-results, such as a random forest.\n\nX, y = datanow()\natom = RidgeRegressor()\nensemble = EnsembleModel(atom=atom)\nmach = machine(ensemble, X, y)\nr_lambda = range(atom, :lambda, lower=0.1, upper=100, scale=:log10)\ncurve = MLJ.learning_curve!(mach; nested_range=(atom=(lambda=r_lambda,),))\nusing Plots\nplot(curve.parameter_values, curve.measurements, xlab=curve.parameter_name, xscale=curve.parameter_scale)\n\nSmart fitting applies. For example, if the model is an ensemble model, and the hyperparemeter parameter is n, then atomic models are progressively added to the ensemble, not recomputed from scratch for each new value of n.\n\natom.lambda=1.0\nr_n = range(ensemble, :n, lower=2, upper=150)\ncurves = MLJ.learning_curve!(mach; nested_range=(n=r_n,), verbosity=3, n=5)\nplot(curves.parameter_values, curves.measurements, xlab=curves.parameter_name)\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.rmsp-Tuple{AbstractArray{#s12,1} where #s12<:Real,Any}","page":"API","title":"MLJ.rmsp","text":"Root mean squared percentage loss \n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.sources-Tuple{MLJ.Source}","page":"API","title":"MLJ.sources","text":"sources(N)\n\nReturn a list of all ultimate sources of  a node N. \n\nSee also: node, source\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.supervised-Tuple{}","page":"API","title":"MLJ.supervised","text":"task = supervised(data=nothing, \n                  types=Dict(), \n                  target=nothing,  \n                  ignore=Symbol[], \n                  is_probabilistic=false, \n                  verbosity=1)\n\nConstruct a supervised learning task with input features X and target y, where: y is the column vector from data named target, if this is a single symbol, or a vector of tuples, if target is a vector; X consists of all remaining columns of data not named in ignore, and is a table unless it has only one column, in which case it is a vector.\n\nThe data types of elements in a column of data named as a key of the dictionary types are coerced to have a scientific type given by the corresponding value. Possible values are Continuous, Multiclass, OrderedFactor and Count. So, for example, types=Dict(:x1=>Count) means elements of the column of data named :x1 will be coerced into integers (whose scitypes are always Count).\n\ntask = supervised(X, y; \n                  input_is_multivariate=true, \n                  is_probabilistic=false, \n                  verbosity=1)\n\nA more customizable constructor, this returns a supervised learning task with input features X and target y, where: X must be a table or vector, according to whether it is multivariate or univariate, while y must be a vector whose elements are scalars, or tuples scalars (of constant length for ordinary multivariate predictions, and of variable length for sequence prediction). Table rows must correspond to patterns and columns to features. Type coercion is not available for this constructor (but see also coerce).\n\nX, y = task()\n\nReturns the input X and target y of the task, also available as task.X and task.y.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.unsupervised-Tuple{}","page":"API","title":"MLJ.unsupervised","text":"task = unsupervised(data=nothing, types=Dict(), ignore=Symbol[], verbosity=1)\n\nConstruct an unsupervised learning task with given input data, which should be a table or, in the case of univariate inputs, a single vector. \n\nThe data types of elements in a column of data named as a key of the dictionary types are coerced to have a scientific type given by the corresponding value. Possible values are Continuous, Multiclass, OrderedFactor and Count. So, for example, types=Dict(:x1=>Count) means elements of the column of data named :x1 will be coerced into integers (whose scitypes are always Count).\n\nRows of data must correspond to patterns and columns to features. Columns in data whose names appear in ignore are ignored.\n\nX = task()\n\nReturn the input data in form to be used in models.\n\nSee also scitype, scitype_union, scitypes\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.info-Tuple{String}","page":"API","title":"MLJBase.info","text":"info(model, pkg=nothing)\n\nReturn the dictionary of metadata associated with model::String. If more than one package implements model then pkg::String will need to be specified.\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsBase.fit!-Tuple{MLJ.AbstractMachine}","page":"API","title":"StatsBase.fit!","text":"fit!(mach::Machine; rows=nothing, verbosity=1)\n\nTrain the machine mach using the algorithm and hyperparameters specified by mach.model, using those rows of the wrapped data having indices in rows.\n\nfit!(mach::NodalMachine; rows=nothing, verbosity=1)\n\nA nodal machine is trained in the same way as a regular machine with one difference: Instead of training the model on the wrapped data indexed on rows, it is trained on the wrapped nodes called on rows, with calling being a recursive operation on nodes within a learning network.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.@curve-Tuple{Any,Any,Any}","page":"API","title":"MLJ.@curve","text":"@curve\n\nThe code,\n\n@curve var range code\n\nevaluates code, replacing appearances of var therein with each value in range. The range and corresponding evaluations are returned as a tuple of arrays. For example,\n\n@curve  x 1:3 (x^2 + 1)\n\nevaluates to\n\n([1,2,3], [2, 5, 10])\n\nThis is convenient for plotting functions using, eg, the Plots package:\n\nplot(@curve x 1:3 (x^2 + 1))\n\nA macro @pcurve parallelizes the same behaviour.  A two-variable implementation is also available, operating as in the following example:\n\njulia> @curve x [1,2,3] y [7,8] (x + y)\n([1,2,3],[7 8],[8.0 9.0; 9.0 10.0; 10.0 11.0])\n\njulia> ans[3]\n3×2 Array{Float64,2}:\n  8.0   9.0\n  9.0  10.0\n 10.0  11.0\n\nN.B. The second range is returned as a row vector for consistency with the output matrix. This is also helpful when plotting, as in:\n\njulia> u1, u2, A = @curve x range(0, stop=1, length=100) α [1,2,3] x^α\njulia> u2 = map(u2) do α \"α = \"*string(α) end\njulia> plot(u1, A, label=u2)\n\nwhich generates three superimposed plots - of the functions x, x^2 and x^3 - each labels with the exponents α = 1, 2, 3 in the legend.\n\n\n\n\n\n","category":"macro"},{"location":"api/#MLJ.SimpleDeterministicCompositeModel","page":"API","title":"MLJ.SimpleDeterministicCompositeModel","text":"SimpleDeterministicCompositeModel(;regressor=ConstantRegressor(), \n                          transformer=FeatureSelector())\n\nConstruct a composite model consisting of a transformer (Unsupervised model) followed by a Deterministic model. Mainly intended for internal testing .\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.copy","page":"API","title":"Base.copy","text":"copy(params::NamedTuple, values=nothing)\n\nReturn a copy of params with new values. That is, flat_values(copy(params, values)) == values is true, while the nested keys remain unchanged.\n\nIf values is not specified a deep copy is returned. \n\n\n\n\n\n","category":"function"},{"location":"api/#Base.merge!-Tuple{Array{T,1} where T,Array{T,1} where T}","page":"API","title":"Base.merge!","text":"merge!(tape1, tape2)\n\nIncrementally appends to tape1 all elements in tape2, excluding any element previously added (or any element of tape1 in its initial state).\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.range-Union{Tuple{D}, Tuple{MLJType,Symbol}} where D","page":"API","title":"Base.range","text":"r = range(model, :hyper; values=nothing)\n\nDefines a NominalRange object for a field hyper of model. Note that r is not directly iterable but iterator(r) iterates over values.\n\nr = range(model, :hyper; upper=nothing, lower=nothing, scale=:linear)\n\nDefines a NumericRange object for a field hyper of model.  Note that r is not directly iteratable but iterator(r, n) iterates over n values between lower and upper values, according to the specified scale. The supported scales are :linear, :log, :log10, :log2. Values for Integer types are rounded (with duplicate values removed, resulting in possibly less than n values).\n\nAlternatively, if a function f is provided as scale, then iterator(r, n) iterates over the values [f(x1), f(x2), ... , f(xn)], where x1, x2, ..., xn are linearly spaced between lower and upper.\n\nSee also: iterator\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.flat_keys-Tuple{Pair{Symbol,B} where B}","page":"API","title":"MLJ.flat_keys","text":" flat_keys(params::NamedTuple)\n\nUse dot-concatentation to express each possibly nested key of params in string form.\n\nExample\n\njulia> flat_keys((A=(x=2, y=3), B=9)))\n[\"A.x\", \"A.y\", \"B\"]\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.get_type-Tuple{Any,Symbol}","page":"API","title":"MLJ.get_type","text":"get_type(T, field::Symbol)\n\nReturns the type of the field field of DataType T. Not a type-stable function.  \n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.scale-Tuple{MLJ.NominalRange}","page":"API","title":"MLJ.scale","text":"MLJ.scale(r::ParamRange)\n\nReturn the scale associated with the ParamRange object r. The possible return values are: :none (for a NominalRange), :linear, :log, :log10, :log2, or :custom (if r.scale is function).\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJ.unwind-Tuple","page":"API","title":"MLJ.unwind","text":"unwind(iterators...)\n\nRepresent all possible combinations of values generated by iterators as rows of a matrix A. In more detail, A has one column for each iterator in iterators and one row for each distinct possible combination of values taken on by the iterators. Elements in the first column cycle fastest, those in the last clolumn slowest. \n\nExample\n\njulia> iterators = ([1, 2], [\"a\",\"b\"], [\"x\", \"y\", \"z\"]);\njulia> MLJ.unwind(iterators...)\n12×3 Array{Any,2}:\n 1  \"a\"  \"x\"\n 2  \"a\"  \"x\"\n 1  \"b\"  \"x\"\n 2  \"b\"  \"x\"\n 1  \"a\"  \"y\"\n 2  \"a\"  \"y\"\n 1  \"b\"  \"y\"\n 2  \"b\"  \"y\"\n 1  \"a\"  \"z\"\n 2  \"a\"  \"z\"\n 1  \"b\"  \"z\"\n 2  \"b\"  \"z\"\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.StratifiedKFold","page":"API","title":"MLJBase.StratifiedKFold","text":"StratifiedKFold(strata,k)\n\nStruct for StratifiedKFold provide strata's and number of partitions(k) and simply collect the object for the indices.  Taken from MLBase (https://github.com/JuliaStats/MLBase.jl).\n\n\n\n\n\n","category":"type"},{"location":"api/#MLJBase.classes-Tuple{Union{CategoricalString, CategoricalValue}}","page":"API","title":"MLJBase.classes","text":"classes(x)\n\nAll the categorical values in the same pool as x (including x), returned as a list, with an ordering consistent with the pool. Here x has CategoricalValue or CategoricalString type, and classes(x) is a vector of the same eltype. \n\nNot to be confused with the levels of x.pool which have a different type. In particular, while x in classes(x) is always true, x in x.pool.levels is not true.\n\njulia> v = categorical([:c, :b, :c, :a])\njulia> levels(v)\n3-element Array{Symbol,1}:\n :a\n :b\n :c\njulia> classes(v[4])\n3-element Array{CategoricalValue{Symbol,UInt32},1}:\n :a\n :b\n :c\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.color_off-Tuple{}","page":"API","title":"MLJBase.color_off","text":"color_off()\n\nSuppress color and bold output at the REPL for displaying MLJ objects. \n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.color_on-Tuple{}","page":"API","title":"MLJBase.color_on","text":"color_on()\n\nEnable color and bold output at the REPL, for enhanced display of MLJ objects.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.container_type-Tuple{Any}","page":"API","title":"MLJBase.container_type","text":"container_type(X)\n\nReturn :table, :sparse, or :other, according to whether X is a supported table format, a supported sparse table format, or something else.\n\nThe first two formats, together abstract vectors, support the MLJBase accessor methods selectrows, selectcols, select, nrows, schema, and union_scitypes.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.datanow-Tuple{}","page":"API","title":"MLJBase.datanow","text":"Get some supervised data now!!\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.decoder-Tuple{Union{CategoricalString, CategoricalValue}}","page":"API","title":"MLJBase.decoder","text":"d = decoder(x)\n\nA callable object for decoding the integer representation of a CategoricalString or CategoricalValue sharing the same pool as x. (Here x is of one of these two types.) Specifically, one has d(int(y)) == y for all y in classes(x). One can also call d on integer arrays, in which case d is broadcast over all elements.\n\njulia> v = categorical([:c, :b, :c, :a])\njulia> int(v)\n4-element Array{UInt32,1}:\n 0x00000003\n 0x00000002\n 0x00000003\n 0x00000001\njulia> d = decoder(v[3])\njulia> d(int(v)) == v\ntrue\n\nSee also: int, classes\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.int-Tuple{Union{CategoricalString, CategoricalValue}}","page":"API","title":"MLJBase.int","text":"int(x)\n\nThe positional integer of the CategoricalString or CategoricalValue x, in the ordering defined by the pool of x. The type of int(x) is the refrence type of x.\n\nNot to be confused with x.ref, which is unchanged by reordering of the pool of x, but has the same type.\n\nint(X::CategoricalArray)\nint(W::Array{<:CategoricalString})\nint(W::Array{<:CategoricalValue})\n\nBroadcasted versions of int.\n\njulia> v = categorical([:c, :b, :c, :a])\njulia> levels(v)\n3-element Array{Symbol,1}:\n :a\n :b\n :c\njulia> int(v)\n4-element Array{UInt32,1}:\n 0x00000003\n 0x00000002\n 0x00000003\n 0x00000001\n\nSee also: decoder\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.load_ames-Tuple{}","page":"API","title":"MLJBase.load_ames","text":"Load the full version of the well-known Ames Housing task.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.load_boston-Tuple{}","page":"API","title":"MLJBase.load_boston","text":"Load a well-known public regression dataset with nominal features.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.load_crabs-Tuple{}","page":"API","title":"MLJBase.load_crabs","text":"Load a well-known crab classification dataset with nominal features.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.load_iris-Tuple{}","page":"API","title":"MLJBase.load_iris","text":"Load a well-known public classification task with nominal features.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.load_reduced_ames-Tuple{}","page":"API","title":"MLJBase.load_reduced_ames","text":"Load a reduced version of the well-known Ames Housing task, having six numerical and six categorical features.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.matrix-Tuple{Any}","page":"API","title":"MLJBase.matrix","text":"MLJBase.matrix(X)\n\nConvert a table source X into an Matrix; or, if X is a AbstractMatrix, return X. Optimized for column-based sources.\n\nIf instead X is a sparse table, then a SparseMatrixCSC object is returned. The integer relabelling of column names follows the lexicographic ordering (as indicated by schema(X).names).\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.nrows-Tuple{Any}","page":"API","title":"MLJBase.nrows","text":"nrows(X)\n\nReturn the number of rows in a table, sparse table, or abstract vector.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.params-Tuple{Any}","page":"API","title":"MLJBase.params","text":"params(m)\n\nRecursively convert any object of subtype MLJType into a named tuple, keyed on the fields of m. The named tuple is possibly nested because params is recursively applied to the field values, which themselves might be MLJType objects. \n\nUsed, in particluar, in the case that m is a model, to inspect its nested hyperparameters:\n\njulia> params(EnsembleModel(atom=ConstantClassifier()))\n(atom = (target_type = Bool,),\n weights = Float64[],\n bagging_fraction = 0.8,\n rng_seed = 0,\n n = 100,\n parallel = true,)\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.partition-Tuple{AbstractArray{Int64,1},Vararg{Any,N} where N}","page":"API","title":"MLJBase.partition","text":"partition(rows::AbstractVector{Int}, fractions...; shuffle=false)\n\nSplits the vector rows into a tuple of vectors whose lengths are given by the corresponding fractions of length(rows). The last fraction is not provided, as it is inferred from the preceding ones. So, for example,\n\njulia> partition(1:1000, 0.2, 0.7)\n(1:200, 201:900, 901:1000)\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.schema-Tuple{Any}","page":"API","title":"MLJBase.schema","text":"schema(X)\n\nReturns a struct with properties names, types with the obvious meanings. Here X is any table or sparse table.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.scitype-Tuple{Any}","page":"API","title":"MLJBase.scitype","text":"scitype(x)\n\nReturn the scientific type for scalar values that object x can represent. If x is a tuple, then Tuple{scitype.(x)...} is returned. \n\njulia> scitype(4.5)\nContinous\n\njulia> scitype(\"book\")\nUnknown\n\njulia> scitype((1, 4.5))\nTuple{Count,Continuous}\n\njulia> using CategoricalArrays\njulia> v = categorical([:m, :f, :f])\njulia> scitype(v[1])\nMulticlass{2}\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.scitype_union-Tuple{Any}","page":"API","title":"MLJBase.scitype_union","text":"scitype_union(A)\n\nReturn the type union, over all elements x generated by the iterable A, of scitype(x).\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.scitypes-Tuple{Any}","page":"API","title":"MLJBase.scitypes","text":"scitypes(X)\n\nReturns a named tuple keyed on the column names of the table X with values the corresponding scitype unions over a column's entries.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.select-Tuple{Any,Any,Any}","page":"API","title":"MLJBase.select","text":"select(X, r, c)\n\nSelect element of a table or sparse table at row r and column c. In the case of sparse data where the key (r, c), zero or missing is returned, depending on the value type.\n\nSee also: selectrows, selectcols\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.selectcols-Tuple{Any,Any}","page":"API","title":"MLJBase.selectcols","text":"selectcols(X, c)\n\nSelect single or multiple columns from any table or sparse table X. If c is an abstract vector of integers or symbols, then the object returned is a table of the preferred sink type of typeof(X). If c is a single integer or column, then a Vector or CategoricalVector is returned.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.selectrows-Tuple{Any,Any}","page":"API","title":"MLJBase.selectrows","text":"selectrows(X, r)\n\nSelect single or multiple rows from any table, sparse table, or abstract vector X.  If X is tabular, the object returned is a table of the preferred sink type of typeof(X), even a single row is selected.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.table-Tuple{NamedTuple}","page":"API","title":"MLJBase.table","text":"MLJBase.table(cols; prototype=cols)\n\nConvert a named tuple of vectors cols, into a table. The table type returned is the \"preferred sink type\" for prototype (see the Tables.jl documentation). \n\nMLJBase.table(X::AbstractMatrix; names=nothing, prototype=nothing)\n\nConvert an abstract matrix X into a table with names (a tuple of symbols) as column names, or with labels (:x1, :x2, ..., :xn) where n=size(X, 2), if names is not specified.  If prototype=nothing, then a named tuple of vectors is returned.\n\nEquivalent to table(cols, prototype=prototype) where cols is the named tuple of columns of X, with keys(cols) = names.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.@constant-Tuple{Any}","page":"API","title":"MLJBase.@constant","text":"@constant x = value\n\nEquivalent to const x = value but registers the binding thus:\n\nMLJBase.HANDLE_GIVEN_ID[objectid(value)] = :x\n\nRegistered objects get displayed using the variable name to which it was bound in calls to show(x), etc.\n\nWARNING: As with any const declaration, binding x to new value of the same type is not prevented and the registration will not be updated.\n\n\n\n\n\n","category":"macro"},{"location":"api/#MLJBase.@more-Tuple{}","page":"API","title":"MLJBase.@more","text":"@more\n\nEntered at the REPL, equivalent to show(ans, 100). Use to get a recursive description of all fields of the last REPL value.\n\n\n\n\n\n","category":"macro"},{"location":"api/#MLJBase._cummulative-Union{Tuple{UnivariateFinite{L,T}}, Tuple{T}, Tuple{L}} where T<:Real where L","page":"API","title":"MLJBase._cummulative","text":"_cummulative(d::UnivariateFinite)\n\nReturn the cummulative probability vector [0, ..., 1] for the distribution d, using whatever ordering is used in the dictionary d.prob_given_level. Used only for to implement random sampling from d.\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase._rand-Tuple{Any}","page":"API","title":"MLJBase._rand","text":"rand(pcummulative)\n\nRandomly sample the distribution with discrete support 1:n which has cummulative probability vector p_cummulative=[0, ..., 1] (of length n+1). Does not check the first and last elements of p_cummulative but does not use them either. \n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase._recursive_show-Tuple{IO,MLJType,Any,Any}","page":"API","title":"MLJBase._recursive_show","text":"_recursive_show(stream, object, current_depth, depth)\n\nGenerate a table of the field values of the MLJType object, dislaying each value by calling the method _show on it. The behaviour of _show(stream, f) is as follows:\n\nIf f is itself a MLJType object, then its short form is shown\n\nand _recursive_show generates as separate table for each of its field values (and so on, up to a depth of argument depth).\n\nOtherwise f is displayed as \"(omitted T)\" where T = typeof(f),\n\nunless istoobig(f) is false (the istoobig fall-back for arbitrary types being true). In the latter case, the long (ie, MIME\"plain/text\") form of f is shown. To override this behaviour, overload the _show method for the type in question. \n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.abbreviated-Tuple{Any}","page":"API","title":"MLJBase.abbreviated","text":"to display abbreviated versions of integers\n\n\n\n\n\n","category":"method"},{"location":"api/#MLJBase.handle-Tuple{Any}","page":"API","title":"MLJBase.handle","text":"return abbreviated object id (as string)  or it's registered handle (as string) if this exists\n\n\n\n\n\n","category":"method"},{"location":"api/#Index-1","page":"API","title":"Index","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"","category":"page"},{"location":"frequently_asked_questions/#Frequently-Asked-Questions-1","page":"FAQ","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"frequently_asked_questions/#Julia-already-has-a-great-machine-learning-toolbox,-ScitkitLearn.jl.-Why-MLJ?-1","page":"FAQ","title":"Julia already has a great machine learning toolbox, ScitkitLearn.jl. Why MLJ?","text":"","category":"section"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"An alternative machine learning toolbox for Julia users is ScikitLearn.jl. Initially intended as a Julia wrapper for the popular python library scikit-learn, ML algorithms written in Julia can also implement the ScikitLearn.jl API. Meta-algorithms (systematic tuning, pipelining, etc) remain python wrapped code, however.","category":"page"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"While ScitkiLearn.jl provides the Julia user with access to a mature and large library of machine learning models, the scikit-learn API on which it is modeled, dating back to 2007, is not likely to evolve significantly in the future. MLJ enjoys (or will enjoy) several features that should make it an attractive alternative in the longer term:","category":"page"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"One language. ScikitLearn.jl wraps python code, which in turn wraps C code for performance-critical routines. A Julia machine learning algorithm that implements the MLJ model interface is 100% Julia. Writing code in Julia is almost as fast as python and well-written Julia code runs almost as fast as C. Additionally, a single language design provides superior interoperability. For example, one can implement: (i) gradient-descent tuning of hyperparameters, using automatic differentiation libraries such as Flux.jl; and (ii) GPU performance boosts without major code refactoring, using CuArrays.jl.\nRegistry for model metadata. In ScikitLearn.jl the list of available models, as well as model metadata (whether a model handles categorical inputs, whether is can make probabilistic predictions, etc) must be gleaned from documentation. In MLJ, this information is more structured and is accessible to MLJ via an external model registry (without the models needing to be loaded). This forms the basis of a \"task\" interface and facilitates model composition (see below).\nTask interface. Once the MLJ user specifies a \"task\" (e.g., \"make probabilistic predictions of home value, based on features x, y, z\") then MLJ can automatically search for models matching that task, assisting in systematic benchmarking and model selection.\nFlexible API for model composition. Pipelines in scikit-learn are more of an afterthought than an integral part of the original design. By contrast, MLJ's user-interaction API was predicated on the requirements of a flexible \"learning network\" API, one that allows models to be connected in essentially arbitrary ways (including target transforming and inverse-transforming). Networks can be built and tested in stages before being exported as first-class stand-alone models. Networks feature \"smart\" training (only necessary components are retrained after parameter changes) and will eventually be trainable using a DAG scheduler. With the help of Julia's meta-programming features, constructing common architectures, such as linear pipelines and stacks, will be one-line operations.\nClean probabilistic API. The scikit-learn API does not specify a universal standard for the form of probabilistic predictions. By fixing a probabilistic API along the lines of the skpro project, MLJ aims to improve support for Bayesian statistics and probabilistic graphical models.\nUniversal adoption of categorical data types. Python's scientific array library NumPy has no dedicated data type for representing categorical data (i.e., no type that tracks the pool of all possible classes). Generally scikit-learn models deal with this by requiring data to be relabeled as integers. However, the naive user trains a model on relabeled categorical data only to discover that evaluation on a test set crashes his code because a categorical feature takes on a value not observed in training. MLJ mitigates such issues by insisting on the use of categorical data types, and by insisting that MLJ model implementations preserve the class pools. If, for example, a training target contains classes in the pool that do not actually appear in the training set, a probabilistic prediction will nevertheless predict a distribution whose support includes the missing class, but which is appropriately weighted with probability zero.","category":"page"},{"location":"frequently_asked_questions/#","page":"FAQ","title":"FAQ","text":"Finally, we note that there is a project underway to implement (some of) the ScikitLearn.jl models as MLJ models, as an temporary expedient.","category":"page"},{"location":"NEWS/#MLJ-News-1","page":"MLJ News","title":"MLJ News","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Development news for MLJ and its satellite packages,  MLJBase, MLJRegistry and MLJModels","category":"page"},{"location":"NEWS/#MLJModels-v0.2.1-1","page":"MLJ News","title":"MLJModels v0.2.1","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"ScikitLearn wraps ElasticNet and ElasticNetCV now available (and registered at MLJRegistry). Resolves: MLJ #112","category":"page"},{"location":"NEWS/#MLJ-v0.2.1-1","page":"MLJ News","title":"MLJ v0.2.1","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Fix a bug and related problem in \"Getting Started\" docs: [#126](https://github.com/alan-turing-institute/MLJ.jl/issues/126 .","category":"page"},{"location":"NEWS/#MLJBase-0.2.0,-MLJModels-0.2.0,-MLJ-0.2.0-1","page":"MLJ News","title":"MLJBase 0.2.0, MLJModels 0.2.0, MLJ 0.2.0","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Model API refactored to resolve #93 and #119 and hence simplify the model interface. This breaks all implementations of supervised models, and some scitype methods. However, for the regular user the effects are restricted to: (i) no more target_type hyperparameter for some models; (ii) Deterministic{Node} is now DeterministicNetwork and Probabillistic{Node} is now ProbabilisticNetwork when exporting learning networks as models.\nNew feature: Task constructors now allow the user to explicitly specify scitypes of features/target. There is a coerce method for vectors and tables for the user who wants to do this manually. Resolves: #119","category":"page"},{"location":"NEWS/#Official-registered-versions-of-MLJBase-0.1.1,-MLJModels-0.1.1,-MLJ-0.1.1-released-1","page":"MLJ News","title":"Official registered versions of MLJBase 0.1.1, MLJModels 0.1.1, MLJ 0.1.1 released","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Minor revisions to the repos, doc updates, and a small breaking change around scitype method names and associated traits. Resolves: #119","category":"page"},{"location":"NEWS/#unversioned-commits-12-April-2019-(around-00:10,-GMT)-1","page":"MLJ News","title":"unversioned commits 12 April 2019 (around 00:10, GMT)","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Added out-of-bag estimates for performance in homogeneous ensembles. Resolves: #77","category":"page"},{"location":"NEWS/#unversioned-commits-11-April-2019-(before-noon,-GMT)-1","page":"MLJ News","title":"unversioned commits 11 April 2019 (before noon, GMT)","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Removed dependency on unregistered package TOML.jl (using, Pkg.TOML instead). Resolves #113","category":"page"},{"location":"NEWS/#unversioned-commits-8-April-2019-(some-time-after-20:00-GMT)-1","page":"MLJ News","title":"unversioned commits 8 April 2019 (some time after 20:00 GMT)","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Addition of XGBoost models XGBoostRegressor, XGBoostClassifier and XGBoostCount. Resolves #65.\nDocumentation reorganized as GitHub pages. Includes some additions but still a work in progress.","category":"page"},{"location":"NEWS/#unversioned-commits-1-March-2019-(some-time-after-03:50-GMT)-1","page":"MLJ News","title":"unversioned commits 1 March 2019 (some time after 03:50 GMT)","text":"","category":"section"},{"location":"NEWS/#","page":"MLJ News","title":"MLJ News","text":"Addition of \"scientific type\" hierarchy, including Continuous, Discrete, Multiclass, and Other subtypes of Found (to complement Missing). See Getting Started for more one this.  Resolves: #86\nRevamp of model traits to take advantage of scientific types, with output_kind replaced with target_scitype_union, input_kind replaced with input_scitype. Also, output_quantity dropped, input_quantity replaced with Bool-valued input_is_multivariate, and is_pure_julia made Bool-valued. Trait definitions in all model implementations and effected meta-algorithms have been updated. Related: #81\nSubstantial update of the core guide Adding New Models to reflect above changes and in response to new model implementer queries. Some design \"decisions\" regarding multivariate targets now explict there.\nthe order the y and yhat arguments of measures (aka loss functions) have been reversed. Progress on: #91\nUpdate of Standardizer and OneHotEncoder to mesh with new scitypes.\nNew improved task constructors infer task metadata from data scitypes. This brings us close to a simple implementation of basic task-model matching. Query the doc-strings for SupervisedTask and UnsupervisedTask for details.  Machines can now dispatch on tasks instead of X and y. A task, task, is now callable: task() returns (X, y) for supervised models, and X for unsupervised models.  Progress on:  #86\nthe data in the load_ames() test task has been replaced by the full data set, and load_reduced_ames() now loads a reduced set.","category":"page"},{"location":"julia_blogpost/#Beyond-machine-learning-pipelines-with-MLJ-1","page":"Julia BlogPost","title":"Beyond machine learning pipelines with MLJ","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Anthony Blaom, Diego Arenas, Franz Kiraly, Yiannis Simillides, Sebastian Vollmer","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"May 1st, 2019. Blog post also posted on the Julia Language Blog","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"(Image: ) (Image: )\n(Image: ) (Image: )","category":"page"},{"location":"julia_blogpost/#Introduction-1","page":"Julia BlogPost","title":"Introduction","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"MLJ is an open-source machine learning toolbox written in pure Julia. It provides a uniform interface for interacting with supervised and unsupervised learning models currently scattered in different Julia packages.","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Building on a earlier proof-of-concept, development began in earnest at The Alan Turing Institute in December 2018. In a short time interest grew and the project is now the Institute's most starred software repository.","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"After outlining MLJ's current functionality, this post introduces MLJ learning networks, a super-charged pipelining feature for model composition.","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Quick links:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"MLJ vs ScikitLearn.jl  \nVideo from London Julia User Group meetup in March 2019 (skip to demo at 21'39) &nbsp; \n(Image: London Julia User Group)\nThe MLJ tour \nBuilding a self-tuning random forest","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"An MLJ docker image (including tour)\nImplementing the MLJ interface for a new model \nHow to contribute\nJulia Slack channel: #mlj.\nStar'ing us to show support for MLJ would be greatly appreciated!","category":"page"},{"location":"julia_blogpost/#MLJ-features-1","page":"Julia BlogPost","title":"MLJ features","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"MLJ already has substantial functionality:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Learning networks. Flexible model composition beyond traditional pipelines (more on this below).\nAutomatic tuning. Automated tuning of hyperparameters, including composite models. Tuning implemented as a model wrapper for composition with other meta-algorithms.\nHomogeneous model ensembling.\nRegistry for model metadata. Metadata available without loading model code. Basis of a \"task\" interface and facilitates model composition.\nTask interface. Automatically match models to specified learning tasks, to streamline benchmarking and model selection.\nClean probabilistic API. Improves support for Bayesian statistics and probabilistic graphical models.\nData container agnostic. Present and manipulate data in your favorite Tables.jl format.\nUniversal adoption of categorical data types. Enables model implementations to properly account for classes seen in training but not in evaluation.","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Enhancements planned for the near future include integration of Flux.jl deep learning models, and gradient descent tuning of continuous hyperparameters using automatic differentiation.","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"While a relatively small number of machine learning models currently implement the MLJ interface, work in progress aims to wrap models supported by the popular python framework, scikit-learn, as a temporary expedient. For a comparison of the MLJ's design with the Julia wrap ScitLearn.jl, see this FAQ.","category":"page"},{"location":"julia_blogpost/#Learning-networks-1","page":"Julia BlogPost","title":"Learning networks","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"MLJ's model composition interface is flexible enough to implement, for example, the model stacks popular in data science competitions. To treat examples of this kind, the interface design must account for the fact that information flow in prediction and training modes is different. This can be seen from the following schematic of a simple two-model stack, viewed as a network:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"(Image: )","category":"page"},{"location":"julia_blogpost/#Building-a-simple-network-1","page":"Julia BlogPost","title":"Building a simple network","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"In MLJ, networks of models are built using a declarative syntax already familiar from basic use of the package. For example, the ordinary syntax for training a decision tree in MLJ, after one-hot encoding the categorical features, looks like this:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"using MLJ\n@load DecisionTreeRegressor\n\n# load some data:\ntask = load_reduced_ames();\nX, y = task();\n\n# one-hot encode the inputs, X:\nhot_model = OneHotEncoder()\nhot = machine(hot_model, X)\nfit!(hot)\nXt = transform(hot, X)\n\n# fit a decision tree to the transformed data:\ntree_model = DecisionTreeRegressor()\ntree = machine(tree_model, Xt, y)\nfit!(tree, rows = 1:1300)","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Note that a model in MLJ is just a struct containing hyperparameters. Wrapping a model in data delivers a machine struct, which will additionally record the results of training.","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Without a pipeline, each time we want to present new data for prediction we must first apply one-hot encoding:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Xnew = X[1301:1400,:];\nXnewt = transform(hot, Xnew);\nyhat = predict(tree, Xnewt);\nyhat[1:3]\n 3-element Array{Float64,1}:\n  223956.9999999999 \n  320142.85714285733\n  161227.49999999994","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"To build a pipeline one simply wraps the supplied data in source nodes and repeats similar declarations, omitting calls to fit!. The difference now is that each \"variable\" (e.g., Xt, yhat) is a node of our pipeline, instead of concrete data:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Xs = source(X)\nys = source(y)\n\nhot = machine(hot_model, Xs)\nXt = transform(hot, Xs);\n\ntree = machine(tree_model, Xt, ys)\nyhat = predict(tree, Xt)","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"If we like, we can think of a node as dynamic data - \"data\" because it can be called (indexed) on rows, but \"dynamic\" because the result depends on the outcome of training events, which in turn depend on  hyperparameter values. For example, after fitting the completed pipeline, we can make new predictions like this:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"fit!(yhat, rows=1:1300)\n [ Info: Training NodalMachine @ 1…51.\n [ Info: Spawned 1300 sub-features to one-hot encode feature :Neighborhood.\n [ Info: Spawned 1300 sub-features to one-hot encode feature :MSSubClass.\n [ Info: Training NodalMachine @ 1…17.\n Node @ 1…79 = predict(1…17, transform(1…51, 1…07))\n\nyhat(rows=1301:1302) # to predict on rows of source node\nyhat(Xnew)           # to predict on new data\n156-element Array{Float64,1}:\n 223956.9999999999 \n 320142.85714285733\n ...","category":"page"},{"location":"julia_blogpost/#Exporting-and-retraining-1","page":"Julia BlogPost","title":"Exporting and retraining","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Once a pipeline like this has been built and tested on sample data, it can be exported as a stand-alone model, ready to be trained on any dataset. For details, see the MLJ documentation. In the future, Julia macros will allow common architectures (e.g., linear pipelines) to be built in a couple of lines.","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Finally, we mention that MLJ learning networks, and their exported counterparts, are \"smart\" in the sense that changing a hyperparameter does not trigger retraining of component models upstream of the change:","category":"page"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"tree_model.max_depth = 4\nfit!(yhat, rows=1:1300)\n [ Info: Not retraining NodalMachine @ 1…51. It is up-to-date.\n [ Info: Updating NodalMachine @ 1…17.\n Node @ 1…79 = predict(1…17, transform(1…51, 1…07))","category":"page"},{"location":"julia_blogpost/#Just-\"Write-the-math!\"-1","page":"Julia BlogPost","title":"Just \"Write the math!\"","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"Because of Julia's generic programming features, any kind of operation you would normally apply to data (arithmetic, row selection, column concatenation, etc) can be overloaded to work with nodes. In this way, MLJ's network-building syntax is economical, intuitive and easy to read. In this respect we have been inspired by On Machine Learning and Programming Languages.","category":"page"},{"location":"julia_blogpost/#Invitation-to-the-community-1","page":"Julia BlogPost","title":"Invitation to the community","text":"","category":"section"},{"location":"julia_blogpost/#","page":"Julia BlogPost","title":"Julia BlogPost","text":"We now invite the community to try out our newly registered packages, MLJalongside MLJModels, and provide any feedback or suggestions you may have going forward. We are also particularly interested in hearing how you would use our package, and what features it may be lacking. ","category":"page"}]
}
