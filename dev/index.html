<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Getting Started · MLJ</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MLJ</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Getting Started</a><ul class="internal"></ul></li><li><a class="toctext" href="evaluating_model_performance/">Evaluating model performance</a></li><li><a class="toctext" href="measures/">Measures</a></li><li><a class="toctext" href="tuning_models/">Tuning models</a></li><li><a class="toctext" href="built_in_transformers/">Built-in Transformers</a></li><li><a class="toctext" href="learning_networks/">Learning Networks</a></li><li><a class="toctext" href="simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="toctext" href="adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="toctext" href="working_with_tasks/">Working with Tasks</a></li><li><a class="toctext" href="benchmarking/">Benchmarking</a></li><li><a class="toctext" href="internals/">Internals</a></li><li><a class="toctext" href="glossary/">Glossary</a></li><li><a class="toctext" href="api/">API</a></li><li><a class="toctext" href="mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="toctext" href="NEWS/">MLJ News</a></li><li><a class="toctext" href="frequently_asked_questions/">FAQ</a></li><li><a class="toctext" href="julia_blogpost/">Julia BlogPost</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Getting Started</a></li></ul><a class="edit-page" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/index.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Getting Started</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Getting-Started-1" href="#Getting-Started-1">Getting Started</a></h1><h4><a class="nav-anchor" id="[Installation-instructions](https://github.com/alan-turing-institute/MLJ.jl/blob/master/README.md)-1" href="#[Installation-instructions](https://github.com/alan-turing-institute/MLJ.jl/blob/master/README.md)-1"><a href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/README.md">Installation instructions</a></a></h4><h4><a class="nav-anchor" id="[Cheatsheet](mlj_cheatsheet.md)-1" href="#[Cheatsheet](mlj_cheatsheet.md)-1"><a href="mlj_cheatsheet/">Cheatsheet</a></a></h4><h4><a class="nav-anchor" id="[Glossary](glossary.md)-1" href="#[Glossary](glossary.md)-1"><a href="glossary/">Glossary</a></a></h4><h3><a class="nav-anchor" id="Plug-and-play-model-evaluation-1" href="#Plug-and-play-model-evaluation-1">Plug-and-play model evaluation</a></h3><p>To load some data install the <a href="https://github.com/JuliaStats/RDatasets.jl">RDatasets</a> in your load path and enter</p><pre><code class="language-julia-repl">julia&gt; using RDatasets

julia&gt; iris = dataset(&quot;datasets&quot;, &quot;iris&quot;); # a DataFrame</code></pre><p>and then split the data into input and target parts:</p><pre><code class="language-julia-repl">julia&gt; X = iris[:, 1:4];

julia&gt; y = iris[:, 5];</code></pre><p>In MLJ a <em>model</em> is a struct storing the hyperparameters of the learning algorithm indicated by the struct name.  Assuming the DecisionTree package is in your load path, we can instantiate a DecisionTreeClassifier model like this:</p><pre><code class="language-julia-repl">julia&gt; using MLJ

julia&gt; @load DecisionTreeClassifier verbosity=1
import MLJModels ✔
import DecisionTree ✔
import MLJModels.DecisionTree_.DecisionTreeClassifier ✔

julia&gt; tree_model = DecisionTreeClassifier(max_depth=2)
MLJModels.DecisionTree_.DecisionTreeClassifier(pruning_purity = 1.0,
                                               max_depth = 2,
                                               min_samples_leaf = 1,
                                               min_samples_split = 2,
                                               min_purity_increase = 0.0,
                                               n_subfeatures = 0,
                                               display_depth = 5,
                                               post_prune = false,
                                               merge_purity_threshold = 0.9,
                                               pdf_smoothing = 0.05,) @ 1…47</code></pre><p><em>Important:</em> DecisionTree and most other packages implementing machine learning algorithms for use in MLJ are not MLJ dependencies. If such a package is not in your load path you will receive an error explaining how to add the package to your current environment.</p><p>Once loaded, a model is evaluated with the <code>evaluate</code> method:</p><pre><code class="language-julia-repl">julia&gt; evaluate(tree_model, X, y,
                resampling=CV(shuffle=true), measure=cross_entropy, verbosity=0)
(measure = MLJ.CrossEntropy[cross_entropy],
 measurement = [0.317715],
 per_fold = Array{Float64,1}[[0.0327898, 0.0327898, 0.355982, 0.54328, 0.38746, 0.553985]],
 per_observation = Array{Array{Float64,1},1}[[[0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898  …  0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898], [0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898  …  0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898], [0.198851, 0.198851, 0.198851, 0.198851, 0.198851, 0.198851, 0.198851, 0.198851, 0.198851, 0.198851  …  0.198851, 0.198851, 0.198851, 0.198851, 0.198851, 4.12713, 0.198851, 0.198851, 0.198851, 0.198851], [0.231641, 0.231641, 4.12713, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 4.12713, 0.231641  …  0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641], [0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 4.12713, 0.231641, 0.231641, 0.231641  …  0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641], [0.096572, 0.096572, 0.096572, 0.096572, 3.90835, 0.096572, 0.096572, 0.096572, 3.90835, 3.90835  …  0.096572, 0.096572, 0.096572, 0.096572, 0.096572, 0.096572, 0.096572, 0.096572, 0.096572, 0.096572]]],)</code></pre><p>Evaluating against multiple performance measures is also possible. See <a href="evaluating_model_performance/">Evaluating model performance</a> for details.</p><h3><a class="nav-anchor" id="Training-and-testing-by-hand-1" href="#Training-and-testing-by-hand-1">Training and testing by hand</a></h3><p>Wrapping the model in data creates a <em>machine</em> which will store training outcomes:</p><pre><code class="language-julia-repl">julia&gt; tree = machine(tree_model, X, y)
Machine{DecisionTreeClassifier} @ 1…52</code></pre><p>Training and testing on a hold-out set:</p><pre><code class="language-julia-repl">julia&gt; train, test = partition(eachindex(y), 0.7, shuffle=true); # 70:30 split

julia&gt; fit!(tree, rows=train);
[ Info: Training Machine{DecisionTreeClassifier} @ 1…52.

julia&gt; yhat = predict(tree, X[test,:]);

julia&gt; yhat[3:5]
3-element Array{UnivariateFinite{String,UInt8,Float64},1}:
 UnivariateFinite(setosa=&gt;0.01612903225806452, versicolor=&gt;0.9677419354838711, virginica=&gt;0.01612903225806452)
 UnivariateFinite(setosa=&gt;0.9677419354838709, versicolor=&gt;0.016129032258064516, virginica=&gt;0.016129032258064516)
 UnivariateFinite(setosa=&gt;0.9677419354838709, versicolor=&gt;0.016129032258064516, virginica=&gt;0.016129032258064516)

julia&gt; cross_entropy(yhat, y[test]) |&gt; mean
0.39288444293059455</code></pre><p>Notice that <code>yhat</code> is a vector of <code>Distribution</code> objects (because DecisionTreeClassifier makes probabilistic predictions). The methods of the <a href="https://github.com/JuliaStats/Distributions.jl">Distributions</a> package can be applied to such distributions:</p><pre><code class="language-julia-repl">julia&gt; broadcast(pdf, yhat[3:5], &quot;virginica&quot;) # predicted probabilities of virginica
3-element Array{Float64,1}:
 0.01612903225806452
 0.016129032258064516
 0.016129032258064516

julia&gt; mode.(yhat[3:5])
3-element Array{CategoricalArrays.CategoricalString{UInt8},1}:
 &quot;versicolor&quot;
 &quot;setosa&quot;
 &quot;setosa&quot;</code></pre><p>One can explicitly get modes by using <code>predict_mode</code> instead of <code>predict</code>:</p><pre><code class="language-julia-repl">julia&gt; predict_mode(tree, rows=test[3:5])
3-element Array{CategoricalArrays.CategoricalString{UInt8},1}:
 &quot;versicolor&quot;
 &quot;setosa&quot;
 &quot;setosa&quot;</code></pre><p>Machines have an internal state which allows them to avoid redundant calculations when retrained, in certain conditions - for example when increasing the number of trees in a random forest, or the number of epochs in a neural network. The machine building syntax also anticipates a more general syntax for composing multiple models, as explained in <a href="learning_networks/">Learning Networks</a>.</p><p>There is a version of <code>evaluate</code> for machines as well as models:</p><pre><code class="language-julia-repl">julia&gt; evaluate!(tree, resampling=Holdout(fraction_train=0.5, shuffle=true),
                       measure=cross_entropy,
                       verbosity=0)
(measure = MLJ.CrossEntropy[cross_entropy],
 measurement = [0.359694],
 per_fold = Array{Float64,1}[[0.359694]],
 per_observation = Array{Array{Float64,1},1}[[[0.0609811, 0.0327898, 0.0327898, 0.0609811, 0.0327898, 0.0327898, 4.12713, 3.15202, 0.0609811, 0.0327898  …  0.0609811, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 3.15202, 0.0327898, 0.0327898, 0.0609811, 0.0609811]]],)</code></pre><p>Changing a hyperparameter and re-evaluating:</p><pre><code class="language-julia-repl">julia&gt; tree_model.max_depth = 3
3

julia&gt; evaluate!(tree, resampling=Holdout(fraction_train=0.5, shuffle=true),
                 measure=cross_entropy,
                 verbosity=0)
(measure = MLJ.CrossEntropy[cross_entropy],
 measurement = [0.0688904],
 per_fold = Array{Float64,1}[[0.0688904]],
 per_observation = Array{Array{Float64,1},1}[[[0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898  …  0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898]]],)</code></pre><h3><a class="nav-anchor" id="Next-steps-1" href="#Next-steps-1">Next steps</a></h3><p>To learn a little more about what MLJ can do, take the MLJ <a href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/examples/tour/tour.ipynb">tour</a>, and then return to the manual as needed. Read at least the remainder of this page before considering serious use of MLJ.</p><h3><a class="nav-anchor" id="Prerequisites-1" href="#Prerequisites-1">Prerequisites</a></h3><p>MLJ assumes some familiarity with the <code>CategoricalValue</code> and <code>CategoricalString</code> types from <a href="https://github.com/JuliaData/CategoricalArrays.jl">CategoricalArrays.jl</a>, used here for representing categorical data. For probabilistic predictors, a basic acquaintance with <a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a> is also assumed.</p><h3><a class="nav-anchor" id="Data-containers-and-scientific-types-1" href="#Data-containers-and-scientific-types-1">Data containers and scientific types</a></h3><p>The MLJ user should acquaint themselves with some basic assumptions about the form of data expected by MLJ, as outlined below. </p><pre><code class="language-none">machine(model::Supervised, X, y) 
machine(model::Unsupervised, X)</code></pre><p>Each supervised model in MLJ declares the permitted <em>scientific type</em> of the inputs <code>X</code> and targets <code>y</code> that can be bound to it in the first constructor above, rather than specifying specific machine types (such as <code>Array{Float32, 2}</code>). Similar remarks apply to the input <code>X</code> of an unsupervised model. Scientific types are julia types defined in the package <a href="https://github.com/alan-turing-institute/ScientificTypes.jl">ScientificTypes.jl</a>, which also defines the convention used here (and there called <em>mlj</em>) for assigning a specific scientific type (interpretation) to each julia object (see the <code>scitype</code> examples below).</p><p>The basic &quot;scalar&quot; scientific types are <code>Continuous</code>, <code>Multiclass{N}</code>, <code>OrderedFactor{N}</code> and <code>Count</code>. Be sure you read <a href="#Container-element-types-1">Container element types</a> below to be guarantee your scalar data is interpreted correctly. Most containers also have a scientific type.</p><p><img src="scitypes.png" alt/></p><pre><code class="language-julia-repl">julia&gt; scitype(4.6)
Continuous

julia&gt; scitype(42)
Count

julia&gt; x1 = categorical([&quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;maybe&quot;]);

julia&gt; scitype(x1)
AbstractArray{Multiclass{3},1}

julia&gt; X = (x1=x1, x2=rand(4), x3=rand(4))  # a &quot;column table&quot;
(x1 = CategoricalArrays.CategoricalString{UInt32}[&quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;maybe&quot;],
 x2 = [0.338647, 0.199328, 0.916576, 0.0110741],
 x3 = [0.860289, 0.263816, 0.987185, 0.417746],)

julia&gt; scitype(X)
Table{Union{AbstractArray{Continuous,1}, AbstractArray{Multiclass{3},1}}}</code></pre><h4><a class="nav-anchor" id="Tabular-data-1" href="#Tabular-data-1">Tabular data</a></h4><p>All data containers compatible with the <a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> interface (which includes all source formats listed <a href="https://github.com/queryverse/IterableTables.jl">here</a>) have the scientific type <code>Table{K}</code>, where <code>K</code> depends on the scientific types of the columns, which can be individually inspected using <code>schema</code>:</p><pre><code class="language-julia-repl">julia&gt; schema(X)
(names = (:x1, :x2, :x3),
 types = (CategoricalArrays.CategoricalString{UInt32}, Float64, Float64),
 scitypes = (Multiclass{3}, Continuous, Continuous),
 nrows = 4,)</code></pre><p><em>Figure 1. Part of the scientific type heirarchy in</em> ScientificTypes.jl.</p><h4><a class="nav-anchor" id="Inputs-1" href="#Inputs-1">Inputs</a></h4><p>Since an MLJ model only specifies the scientific type of data, if that type is <code>Table</code> - which is the case for the majority of MLJ models - then any Tables.jl format is permitted. However, the Tables.jl API excludes matrices. If <code>Xmatrix</code> is a matrix, convert it to a column table using <code>X = MLJ.table(Xmatrix)</code>.</p><p>Specifically, the requirement for an arbitrary model&#39;s input is <code>scitype(X) &lt;: input_scitype(model)</code>.</p><h4><a class="nav-anchor" id="Targets-1" href="#Targets-1">Targets</a></h4><p>The target <code>y</code> expected by MLJ models is generally an <code>AbstractVector</code>. A multivariate target <code>y</code> will generally be a vector of <em>tuples</em>. The tuples need not have uniform length, so some forms of sequence prediction are supported. Only the element types of <code>y</code> matter (the types of <code>y[j]</code> for each <code>j</code>). </p><p>Specifically, the type requirement for a model target is <code>scitype(y) &lt;: target_scitype(model)</code>.</p><h4><a class="nav-anchor" id="Querying-a-model-for-data-types-1" href="#Querying-a-model-for-data-types-1">Querying a model for data types</a></h4><p>One can inspect the admissible scientific types of a model&#39;s input and target. If one has a <code>Model</code> instance <code>model</code>, one can use <code>scitype(model)</code>:</p><pre><code class="language-julia-repl">julia&gt; tree = DecisionTreeClassifier();

julia&gt; scitype(tree)
(input = Table{#s13} where #s13&lt;:(AbstractArray{#s12,1} where #s12&lt;:Continuous),
 target = AbstractArray{#s30,1} where #s30&lt;:Finite,)</code></pre><p>If, however, the relevant model code has not been loaded, one can nevertheless extract the scitypes from the model type&#39;s MLJ registry entry:</p><pre><code class="language-julia-repl">julia&gt; info(&quot;DecisionTreeClassifier&quot;)
OrderedCollections.LittleDict{Any,Any,Array{Any,1},Array{Any,1}} with 13 entries:
  :package_uuid     =&gt; &quot;7806a523-6efd-50cb-b5f6-3fa6f1930dbb&quot;
  :package_license  =&gt; &quot;unkown&quot;
  :load_path        =&gt; &quot;MLJModels.DecisionTree_.DecisionTreeClassifier&quot;
  :is_pure_julia    =&gt; true
  :is_supervised    =&gt; true
  :package_url      =&gt; &quot;https://github.com/bensadeghi/DecisionTree.jl&quot;
  :package_name     =&gt; &quot;DecisionTree&quot;
  :name             =&gt; &quot;DecisionTreeClassifier&quot;
  :is_probabilistic =&gt; true
  :target_scitype   =&gt; AbstractArray{_s163,1} where _s163&lt;:Finite
  :supports_weights =&gt; false
  :input_scitype    =&gt; Table{_s13} where _s13&lt;:(AbstractArray{_s12,1} where _s1…
  :is_wrapper       =&gt; false</code></pre><p>See also <a href="working_with_tasks/">Working with tasks</a> on searching for models solving a specified task.</p><h4><a class="nav-anchor" id="Container-element-types-1" href="#Container-element-types-1">Container element types</a></h4><p>Models in MLJ will always apply the <em>mlj</em> convention described <a href="https://github.com/alan-turing-institute/ScientificTypes.jl">ScientificTypes.jl</a> to decide how to interpret the elements of your container types. Here are the key aspects of that convention:</p><ul><li><p>Any <code>AbstractFloat</code> is interpreted as <code>Continuous</code>.</p></li><li><p>Any <code>Integer</code> is interpreted as <code>Count</code>. </p></li><li><p>Any <code>CategoricalValue</code> or <code>CategoricalString</code>, <code>x</code>, is interpreted as <code>Multiclass</code> or <code>OrderedFactor</code>, depending on the value of <code>x.pool.ordered</code>.</p></li><li><p><code>String</code>s and <code>Char</code>s are <em>not</em> interpreted as <code>Finite</code>; they have <code>Unknown</code> scitype. Coerce vectors of strings or characters to <code>CategoricalVector</code>s if they represent <code>Multiclass</code> or <code>OrderedFactor</code> data.</p></li><li><p>In particular, <em>integers</em> (including <code>Bool</code>s) <em>cannot be used to represent categorical data.</em></p></li></ul><p>To coerce the scientific type of a vector or table, use the <code>coerce</code> method (re-exported from <a href="https://github.com/alan-turing-institute/ScientificTypes.jl">ScientificTypes.jl</a>).</p><footer><hr/><a class="next" href="evaluating_model_performance/"><span class="direction">Next</span><span class="title">Evaluating model performance</span></a></footer></article></body></html>
