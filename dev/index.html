<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Getting Started · MLJ</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MLJ</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Getting Started</a><ul class="internal"></ul></li><li><a class="toctext" href="evaluating_model_performance/">Evaluating model performance</a></li><li><a class="toctext" href="measures/">Measures</a></li><li><a class="toctext" href="tuning_models/">Tuning models</a></li><li><a class="toctext" href="built_in_transformers/">Built-in Transformers</a></li><li><a class="toctext" href="learning_networks/">Learning Networks</a></li><li><a class="toctext" href="simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="toctext" href="adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="toctext" href="working_with_tasks/">Working with Tasks</a></li><li><a class="toctext" href="benchmarking/">Benchmarking</a></li><li><a class="toctext" href="internals/">Internals</a></li><li><a class="toctext" href="glossary/">Glossary</a></li><li><a class="toctext" href="api/">API</a></li><li><a class="toctext" href="mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="toctext" href="NEWS/">MLJ News</a></li><li><a class="toctext" href="frequently_asked_questions/">FAQ</a></li><li><a class="toctext" href="julia_blogpost/">Julia BlogPost</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Getting Started</a></li></ul><a class="edit-page" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/index.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Getting Started</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Getting-Started-1" href="#Getting-Started-1">Getting Started</a></h1><h3><a class="nav-anchor" id="[Installation-instructions](https://github.com/alan-turing-institute/MLJ.jl/blob/master/README.md)-1" href="#[Installation-instructions](https://github.com/alan-turing-institute/MLJ.jl/blob/master/README.md)-1"><a href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/README.md">Installation instructions</a></a></h3><h3><a class="nav-anchor" id="[Glossary](glossary.md)-1" href="#[Glossary](glossary.md)-1"><a href="glossary/">Glossary</a></a></h3><h3><a class="nav-anchor" id="Plug-and-play-model-evaluation-1" href="#Plug-and-play-model-evaluation-1">Plug-and-play model evaluation</a></h3><p>To load some data install the <a href="https://github.com/JuliaStats/RDatasets.jl">RDatasets</a> in your load path and enter</p><pre><code class="language-julia-repl">julia&gt; using RDatasets

julia&gt; iris = dataset(&quot;datasets&quot;, &quot;iris&quot;); # a DataFrame</code></pre><p>and then split the data into input and target parts:</p><pre><code class="language-julia-repl">julia&gt; X = iris[:, 1:4];

julia&gt; y = iris[:, 5];</code></pre><p>In MLJ a <em>model</em> is a struct storing the hyperparameters of the learning algorithm indicated by the struct name.  Assuming the DecisionTree package is in your load path, we can instantiate a DecisionTreeClassifier model like this:</p><pre><code class="language-julia-repl">julia&gt; using MLJ

julia&gt; @load DecisionTreeClassifier verbosity=1
import MLJModels ✔
import DecisionTree ✔
import MLJModels.DecisionTree_.DecisionTreeClassifier ✔

julia&gt; tree_model = DecisionTreeClassifier(max_depth=2)
MLJModels.DecisionTree_.DecisionTreeClassifier(pruning_purity = 1.0,
                                               max_depth = 2,
                                               min_samples_leaf = 1,
                                               min_samples_split = 2,
                                               min_purity_increase = 0.0,
                                               n_subfeatures = 0,
                                               display_depth = 5,
                                               post_prune = false,
                                               merge_purity_threshold = 0.9,
                                               pdf_smoothing = 0.05,) @ 7…28</code></pre><p><em>Important:</em> DecisionTree and most other packages implementing machine learning algorithms for use in MLJ are not MLJ dependencies. If such a package is not in your load path you will receive an error explaining how to add the package to your current environment.</p><p>Once loaded, a model is evaluated with the <code>evaluate</code> method:</p><pre><code class="language-julia-repl">julia&gt; evaluate(tree_model, X, y,
                resampling=CV(shuffle=true), measure=cross_entropy, verbosity=0)
(measure = MLJ.CrossEntropy[cross_entropy],
 measurement = [0.317715],
 per_fold = Array{Float64,1}[[0.0327898, 0.0327898, 0.355982, 0.54328, 0.38746, 0.553985]],
 per_observation = Array{Array{Float64,1},1}[[[0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898  …  0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898], [0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898  …  0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898], [0.198851, 0.198851, 0.198851, 0.198851, 0.198851, 0.198851, 0.198851, 0.198851, 0.198851, 0.198851  …  0.198851, 0.198851, 0.198851, 0.198851, 0.198851, 4.12713, 0.198851, 0.198851, 0.198851, 0.198851], [0.231641, 0.231641, 4.12713, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 4.12713, 0.231641  …  0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641], [0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 4.12713, 0.231641, 0.231641, 0.231641  …  0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641, 0.231641], [0.096572, 0.096572, 0.096572, 0.096572, 3.90835, 0.096572, 0.096572, 0.096572, 3.90835, 3.90835  …  0.096572, 0.096572, 0.096572, 0.096572, 0.096572, 0.096572, 0.096572, 0.096572, 0.096572, 0.096572]]],)</code></pre><p>Evaluating against multiple performance measures is also possible. See <a href="evaluating_model_performance/">Evaluating model performance</a> for details.</p><h3><a class="nav-anchor" id="Training-and-testing-by-hand-1" href="#Training-and-testing-by-hand-1">Training and testing by hand</a></h3><p>Wrapping the model in data creates a <em>machine</em> which will store training outcomes:</p><pre><code class="language-julia-repl">julia&gt; tree = machine(tree_model, X, y)
Machine{DecisionTreeClassifier} @ 2…73</code></pre><p>Training and testing on a hold-out set:</p><pre><code class="language-julia-repl">julia&gt; train, test = partition(eachindex(y), 0.7, shuffle=true); # 70:30 split

julia&gt; fit!(tree, rows=train);
[ Info: Training Machine{DecisionTreeClassifier} @ 2…73.

julia&gt; yhat = predict(tree, X[test,:]);

julia&gt; yhat[3:5]
3-element Array{UnivariateFinite{CategoricalArrays.CategoricalString{UInt8},Float64},1}:
 UnivariateFinite{CategoricalArrays.CategoricalString{UInt8},Float64}(
prob_given_level: Dict(&quot;virginica&quot;=&gt;0.016129,&quot;setosa&quot;=&gt;0.967742,&quot;versicolor&quot;=&gt;0.016129)
)

 UnivariateFinite{CategoricalArrays.CategoricalString{UInt8},Float64}(
prob_given_level: Dict(&quot;virginica&quot;=&gt;0.967742,&quot;setosa&quot;=&gt;0.016129,&quot;versicolor&quot;=&gt;0.016129)
)

 UnivariateFinite{CategoricalArrays.CategoricalString{UInt8},Float64}(
prob_given_level: Dict(&quot;virginica&quot;=&gt;0.016129,&quot;setosa&quot;=&gt;0.967742,&quot;versicolor&quot;=&gt;0.016129)
)

julia&gt; cross_entropy(yhat, y[test]) |&gt; mean
0.1942735846639985</code></pre><p>Notice that <code>yhat</code> is a vector of <code>Distribution</code> objects (because DecisionTreeClassifier makes probabilistic predictions). The methods of the <a href="https://github.com/JuliaStats/Distributions.jl">Distributions</a> package can be applied to such distributions:</p><pre><code class="language-julia-repl">julia&gt; broadcast(pdf, yhat[3:5], &quot;virginica&quot;) # predicted probabilities of virginica
3-element Array{Float64,1}:
 0.01612903225806452
 0.9677419354838711
 0.01612903225806452

julia&gt; mode.(yhat[3:5])
3-element Array{CategoricalArrays.CategoricalString{UInt8},1}:
 &quot;setosa&quot;
 &quot;virginica&quot;
 &quot;setosa&quot;</code></pre><p>One can explicitly get modes by using <code>predict_mode</code> instead of <code>predict</code>:</p><pre><code class="language-julia-repl">julia&gt; predict_mode(tree, rows=test[3:5])
3-element Array{CategoricalArrays.CategoricalString{UInt8},1}:
 &quot;setosa&quot;
 &quot;virginica&quot;
 &quot;setosa&quot;</code></pre><p>Machines have an internal state which allows them to avoid redundant calculations when retrained, in certain conditions - for example when increasing the number of trees in a random forest, or the number of epochs in a neural network. The machine building syntax also anticaptes a more general syntax for composing multiple models, as explained in <a href="learning_networks/">Learning Networks</a>.</p><p>There is a version of <code>evaluate</code> for machines as well as models:</p><pre><code class="language-julia-repl">julia&gt; evaluate!(tree, resampling=Holdout(fraction_train=0.5, shuffle=true),
                       measure=cross_entropy,
                       verbosity=0)
(measure = MLJ.CrossEntropy[cross_entropy],
 measurement = [0.398643],
 per_fold = Array{Float64,1}[[0.398643]],
 per_observation = Array{Array{Float64,1},1}[[[0.0542696, 0.0327898, 0.0327898, 4.12713, 0.0327898, 0.0327898, 0.0542696, 0.0327898, 0.0542696, 0.0327898  …  3.31237, 0.0542696, 0.0327898, 0.0327898, 0.0542696, 0.0327898, 0.0327898, 4.12713, 0.0327898, 0.0327898]]],)</code></pre><p>Changing a hyperparameter and re-evaluating:</p><pre><code class="language-julia-repl">julia&gt; tree_model.max_depth = 3
3

julia&gt; evaluate!(tree, resampling=Holdout(fraction_train=0.5, shuffle=true),
                 measure=cross_entropy,
                 verbosity=0)
(measure = MLJ.CrossEntropy[cross_entropy],
 measurement = [0.330556],
 per_fold = Array{Float64,1}[[0.330556]],
 per_observation = Array{Array{Float64,1},1}[[[0.421994, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898  …  0.0327898, 4.12713, 0.0327898, 4.12713, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898, 0.0327898]]],)</code></pre><h3><a class="nav-anchor" id="Next-steps-1" href="#Next-steps-1">Next steps</a></h3><p>To learn a little more about what MLJ can do, take the MLJ <a href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/examples/tour/tour.ipynb">tour</a>, and then return to the manual as needed. Read at least the remainder of this page before considering serious use of MLJ.</p><h3><a class="nav-anchor" id="Prerequisites-1" href="#Prerequisites-1">Prerequisites</a></h3><p>MLJ assumes some familiarity with the <code>CategoricalValue</code> and <code>CategoricalString</code> types from <a href="https://github.com/JuliaData/CategoricalArrays.jl">CategoricalArrays.jl</a>, used here for representing categorical data. For probabilistic predictors, a basic acquaintance with <a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a> is also assumed.</p><h3><a class="nav-anchor" id="Data-containers-and-scientific-types-1" href="#Data-containers-and-scientific-types-1">Data containers and scientific types</a></h3><p>The MLJ user should acquaint themselves with some basic assumptions about the form of data expected by MLJ, as outlined below. </p><pre><code class="language-none">machine(model::Supervised, X, y) 
machine(model::Unsupervised, X)</code></pre><p><strong>Multivariate input.</strong> The input <code>X</code> in the above machine constructors can be any table, where <em>table</em> means any data type supporting the <a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> interface.</p><p>In particular, <code>DataFrame</code>, <code>JuliaDB.IndexedTable</code> and <code>TypedTables.Table</code> objects are supported, as are two Julia native formats: <em>column tables</em> (named tuples of equal length vectors) and <em>row tables</em> (vectors of named tuples sharing the same keys).</p><p><strong>Univariate input.</strong> For models which handle only univariate inputs (<code>input_is_multivariate(model)=false</code>) <code>X</code> cannot be a table but is expected to be some <code>AbstractVector</code> type.</p><p><strong>Targets.</strong> The target <code>y</code> in the first constructor above must be an <code>AbstractVector</code>. A multivariate target <code>y</code> will be a vector of <em>tuples</em>. The tuples need not have uniform length, so some forms of sequence prediction are supported. Only the element types of <code>y</code> matter (the types of <code>y[j]</code> for each <code>j</code>). Indeed if a machine accepts <code>y</code> as an argument it will be just as happy with <code>identity.(y)</code>.</p><p><strong>Element types.</strong> The types of input and target <em>elements</em> has strict consequences for MLJ&#39;s behaviour. </p><p>To articulate MLJ&#39;s conventions about data representation, MLJ distinguishes between <em>machine</em> data types on the one hand (<code>Float64</code>, <code>Bool</code>, <code>String</code>, etc) and <em>scientific data types</em> on the other, represented by new Julia types: <code>Continuous</code>, <code>Count</code>, <code>Multiclass{N}</code>, <code>OrderedFactor{N}</code> and <code>Unknown</code>, with obvious interpretations.  These types are organized in a type <a href="scitypes.png">hierarchy</a> rooted in a new abstract type <code>Found</code>.</p><p>A <em>scientific type</em> is any subtype of <code>Union{Missing,Found}</code>. Scientific types have no instances. (They are used behind the scenes is values for model trait functions.) Such types appear, for example, when querying model metadata:</p><pre><code class="language-julia">julia&gt; info(&quot;DecisionTreeClassifier&quot;)[:target_scitype_union]</code></pre><pre><code class="language-julia">Finite</code></pre><pre><code class="language-julia">subtypes(Finite)</code></pre><pre><code class="language-julia">2-element Array{Any,1}:
 Multiclass   
 OrderedFactor</code></pre><p>This means that the scitype of all elements of <code>DecisionTreeClassier</code> target must be <code>Multiclass</code> or <code>OrderedFactor</code>.</p><p>To see how MLJ will interpret an object <code>x</code> appearing in table or vector input <code>X</code>, or target vector <code>y</code>, call <code>scitype(x)</code>. The fallback this function is <code>scitype(::Any) = Unknown</code>. </p><pre><code class="language-julia">julia&gt; (scitype(42), scitype(float(π)), scitype(&quot;Julia&quot;))</code></pre><pre><code class="language-julia">(Count, Continuous, Unknown)</code></pre><p>The table below shows machine types that have scientific types different from <code>Unknown</code>:</p><table><tr><th style="text-align: right"><code>T</code></th><th style="text-align: left"><code>scitype(x)</code> for <code>x::T</code></th></tr><tr><td style="text-align: right"><code>AbstractFloat</code></td><td style="text-align: left"><code>Continuous</code></td></tr><tr><td style="text-align: right"><code>Integer</code></td><td style="text-align: left"><code>Count</code></td></tr><tr><td style="text-align: right"><code>CategoricalValue</code></td><td style="text-align: left"><code>Multiclass{N}</code> where <code>N = nlevels(x)</code>, provided <code>x.pool.ordered == false</code></td></tr><tr><td style="text-align: right"><code>CategoricalString</code></td><td style="text-align: left"><code>Multiclass{N}</code> where <code>N = nlevels(x)</code>, provided <code>x.pool.ordered == false</code></td></tr><tr><td style="text-align: right"><code>CategoricalValue</code></td><td style="text-align: left"><code>OrderedFactor{N}</code> where <code>N = nlevels(x)</code>, provided <code>x.pool.ordered == true</code></td></tr><tr><td style="text-align: right"><code>CategoricalString</code></td><td style="text-align: left"><code>OrderedFactor{N}</code> where <code>N = nlevels(x)</code> provided <code>x.pool.ordered == true</code></td></tr><tr><td style="text-align: right"><code>Missing</code></td><td style="text-align: left"><code>Missing</code></td></tr></table><p>Here <code>nlevels(x) = length(levels(x.pool))</code>.</p><p><strong>Special note on using integers.</strong> According to the above, integers cannot be used to represent <code>Multiclass</code> or <code>OrderedFactor</code> data. These can be represented by an unordered or ordered <code>CategoricalValue</code> or <code>CategoricalString</code> (automatic if they are elements of a <code>CategoricalArray</code>).</p><p>Methods exist to coerce the scientific type of a vector or table (see below). <a href="working_with_tasks/">Task</a> constructors also allow one to force the data being wrapped to have the desired scientific type.</p><p>For more about scientific types and their role, see <a href="adding_models_for_general_use/">Adding Models for General Use</a></p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.coerce" href="#MLJ.coerce"><code>MLJ.coerce</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">coerce(T, v::AbstractVector)</code></pre><p>Coerce the machine types of elements of <code>v</code> to ensure the returned vector has <code>T</code> as its <code>scitype_union</code>, or <code>Union{Missing,T}</code>, if <code>v</code> has missing values.</p><pre><code class="language-none">julia&gt; v = coerce(Continuous, [1, missing, 5])
3-element Array{Union{Missing, Float64},1}:
 1.0
 missing
 5.0

julia&gt; scitype_union(v)
Union{Missing,Continuous}</code></pre><p>See also <a href="adding_models_for_general_use/#MLJBase.scitype"><code>scitype</code></a>, <a href="adding_models_for_general_use/#MLJBase.scitype_union"><code>scitype_union</code></a>, <a href="adding_models_for_general_use/#MLJBase.scitypes"><code>scitypes</code></a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/2b28a2d01c76369736f29abd48fcb61503131288/src/tasks.jl#L51-L69">source</a><div><div><pre><code class="language-none">coerce(d::Dict, X)</code></pre><p>Return a copy of the table <code>X</code> with columns named in the keys of <code>d</code> coerced to have <code>scitype_union</code> equal to the corresponding value.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/2b28a2d01c76369736f29abd48fcb61503131288/src/tasks.jl#L131-L137">source</a></section><footer><hr/><a class="next" href="evaluating_model_performance/"><span class="direction">Next</span><span class="title">Evaluating model performance</span></a></footer></article></body></html>
