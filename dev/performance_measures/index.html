<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Performance Measures · MLJ</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MLJ</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Getting Started</a></li><li><a class="toctext" href="../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="toctext" href="../model_search/">Model Search</a></li><li><a class="toctext" href="../machines/">Machines</a></li><li><a class="toctext" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li class="current"><a class="toctext" href>Performance Measures</a><ul class="internal"></ul></li><li><a class="toctext" href="../tuning_models/">Tuning Models</a></li><li><a class="toctext" href="../learning_curves/">Learning Curves</a></li><li><a class="toctext" href="../built_in_transformers/">Built-in Transformers</a></li><li><a class="toctext" href="../composing_models/">Composing Models</a></li><li><a class="toctext" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="toctext" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="toctext" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="toctext" href="../benchmarking/">Benchmarking</a></li><li><a class="toctext" href="../working_with_tasks/">Working with Tasks</a></li><li><a class="toctext" href="../internals/">Internals</a></li><li><a class="toctext" href="../glossary/">Glossary</a></li><li><a class="toctext" href="../api/">API</a></li><li><a class="toctext" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="toctext" href="../NEWS/">MLJ News</a></li><li><a class="toctext" href="../frequently_asked_questions/">FAQ</a></li><li><a class="toctext" href="../julia_blogpost/">Julia BlogPost</a></li><li><a class="toctext" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Performance Measures</a></li></ul><a class="edit-page" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/performance_measures.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Performance Measures</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Performance-Measures-1" href="#Performance-Measures-1">Performance Measures</a></h1><p>In MLJ loss functions, scoring rules, sensitivities, and so on, are collectively referred to as <em>measures</em>. Presently, MLJ includes a few built-in measures, provides support for the loss functions in the <a href="https://github.com/JuliaML/LossFunctions.jl">LossFunctions.jl</a> library, and allows for users to define their own custom measures. </p><p>Providing further measures for probabilistic predictors, such as proper scoring rules, and for constructing multi-target product measures, is a work in progress.</p><p><em>Note for developers:</em> The measures interface and the built-in measures  described here are defined in MLJBase.</p><h3><a class="nav-anchor" id="Built-in-measures-1" href="#Built-in-measures-1">Built-in measures</a></h3><p>These measures all have the common calling syntax</p><pre><code class="language-julia">measure(ŷ, y)</code></pre><p>or</p><pre><code class="language-julia">measure(ŷ, y, w)</code></pre><p>where <code>y</code> iterates over observations of some target variable, and <code>ŷ</code> iterates over predictions (<code>Distribution</code> or <code>Sampler</code> objects in the probabilistic case). Here <code>w</code> is an optional vector of sample weights, which can be provided when the measure supports this.</p><pre><code class="language-julia-repl">julia&gt; using MLJ

julia&gt; y = [1, 2, 3, 4];

julia&gt; ŷ = [2, 3, 3, 3];

julia&gt; w = [1, 2, 2, 1];

julia&gt; rms(ŷ, y) # reports an aggregrate loss
0.8660254037844386

julia&gt; l1(ŷ, y, w) # reports per observation losses
4-element Array{Float64,1}:
 0.6666666666666666
 1.3333333333333333
 0.0
 0.6666666666666666

julia&gt; y = categorical([&quot;male&quot;, &quot;female&quot;, &quot;female&quot;])
3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:
 &quot;male&quot;
 &quot;female&quot;
 &quot;female&quot;

julia&gt; male = y[1]; female = y[2];

julia&gt; d = UnivariateFinite([male, female], [0.55, 0.45]);

julia&gt; ŷ = [d, d, d];

julia&gt; cross_entropy(ŷ, y)
3-element Array{Float64,1}:
 0.5978370007556204
 0.7985076962177716
 0.7985076962177716</code></pre><h3><a class="nav-anchor" id="Traits-and-custom-measures-1" href="#Traits-and-custom-measures-1">Traits and custom measures</a></h3><p>Notice that <code>l1</code> reports per-sample evaluations, while <code>rms</code> only reports an aggregated result. This and other behavior can be gleaned from measure <em>traits</em> which are summarized by the <code>info</code> method:</p><pre><code class="language-julia-repl">julia&gt; info(l1)
absolute deviations; aliases: `l1`
(name = &quot;l1&quot;,
 target_scitype = Union{AbstractArray{Continuous,1}, AbstractArray{Count,1}},
 supports_weights = true,
 prediction_type = :deterministic,
 orientation = :loss,
 reports_each_observation = true,
 aggregation = MLJBase.Mean(),
 is_feature_dependent = false,
 docstring = &quot;absolute deviations; aliases: `l1`&quot;,
 distribution_type = missing,)</code></pre><p>Use <code>measures()</code> to list all measures and <code>measures(conditions...)</code> to search for measures with given traits (as you would <a href="../model_search/">query models</a>).</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.measures-Tuple" href="#MLJBase.measures-Tuple"><code>MLJBase.measures</code></a> — <span class="docstring-category">Method</span>.</div><div><div><pre><code class="language-julia">measures()</code></pre><p>List all measures as named-tuples keyed on measure traits.</p><pre><code class="language-none">measures(conditions...)</code></pre><p>List all measures satisifying the specified <code>conditions</code>. A <em>condition</em> is any <code>Bool</code>-valued function on the named-tuples.</p><p><strong>Example</strong></p><p>Find all classification measures supporting sample weights:</p><pre><code class="language-none">measures(m -&gt; m.target_scitype &lt;: AbstractVector{&lt;:Finite} &amp;&amp;
              m.supports_weights)</code></pre></div></div></section><p>A user-defined measure in MLJ can be passed to the <code>evaluate!</code> method, and elsewhere in MLJ, provided it is a function or callable object conforming to the above syntactic conventions. By default, a custom measure is understood to:</p><ul><li><p>be a loss function (rather than a score) </p></li><li><p>report an aggregated value (rather than per-sample evaluations)</p></li><li><p>be feature-independent</p></li></ul><p>To override this behavior one simply overloads the appropriate trait, as shown in the following examples:</p><pre><code class="language-julia-repl">julia&gt; y = [1, 2, 3, 4];

julia&gt; ŷ = [2, 3, 3, 3];

julia&gt; w = [1, 2, 2, 1];

julia&gt; my_loss(ŷ, y) = maximum((ŷ - y).^2);

julia&gt; my_loss(ŷ, y)
1

julia&gt; my_per_sample_loss(ŷ, y) = abs.(ŷ - y);

julia&gt; MLJ.reports_each_observation(::typeof(my_per_sample_loss)) = true;

julia&gt; my_per_sample_loss(ŷ, y)
4-element Array{Int64,1}:
 1
 1
 0
 1

julia&gt; my_weighted_score(ŷ, y) = 1/mean(abs.(ŷ - y));

julia&gt; my_weighted_score(ŷ, y, w) = 1/mean(abs.((ŷ - y).^w));

julia&gt; MLJ.supports_weights(::typeof(my_weighted_score)) = true;

julia&gt; MLJ.orientation(::typeof(my_weighted_score)) = :score;

julia&gt; my_weighted_score(ŷ, y)
1.3333333333333333

julia&gt; X = (x=rand(4), penalty=[1, 2, 3, 4]);

julia&gt; my_feature_dependent_loss(ŷ, X, y) = sum(abs.(ŷ - y) .* X.penalty)/sum(X.penalty);

julia&gt; MLJ.is_feature_dependent(::typeof(my_feature_dependent_loss)) = true

julia&gt; my_feature_dependent_loss(ŷ, X, y)
0.7</code></pre><p>The possible signatures for custom measures are: <code>measure(ŷ, y)</code>, <code>measure(ŷ, y, w)</code>, <code>measure(ŷ, X, y)</code> and <code>measure(ŷ, X, y, w)</code>, each measure implementing one non-weighted version, and possibly a second weighted version.</p><p><em>Implementation detail:</em> Internally, every measure is evaluated using the syntax </p><pre><code class="language-julia">MLJ.value(measure, ŷ, X, y, w)</code></pre><p>and the traits determine what can be ignored and how <code>measure</code> is actually called. If <code>w=nothing</code> then the non-weighted form of <code>measure</code> is dipatched. </p><h3><a class="nav-anchor" id="Using-LossFunctions.jl-1" href="#Using-LossFunctions.jl-1">Using LossFunctions.jl</a></h3><p>The <a href="https://github.com/JuliaML/LossFunctions.jl">LossFunctions.jl</a> package includes &quot;distance loss&quot; functions for <code>Continuous</code> targets, and &quot;marginal loss&quot; functins for <code>Binary</code> targets. While the LossFunctions,jl interface differs from the present one (for, example <code>Binary</code> observations must be +1 or -1), one can safely pass the loss functions defined there to any MLJ algorithm, which re-interprets it under the hood. Note that the &quot;distance losses&quot; in the package apply to deterministic predictions, while the &quot;marginal losses&quot; apply to probabilistic predictions.</p><pre><code class="language-julia-repl">julia&gt; using LossFunctions

julia&gt; X = (x1=rand(5), x2=rand(5)); y = categorical([&quot;y&quot;, &quot;y&quot;, &quot;y&quot;, &quot;n&quot;, &quot;y&quot;]); w = [1, 2, 1, 2, 3];

julia&gt; mach = machine(ConstantClassifier(), X, y);

julia&gt; holdout = Holdout(fraction_train=0.6);

julia&gt; evaluate!(mach,
                 measure=[ZeroOneLoss(), L1HingeLoss(), L2HingeLoss(), SigmoidLoss()],
                 resampling=holdout,
                 operation=predict,
                 weights=w,
                 verbosity=0)
┌─────────────┬───────────────┬────────────┐
│ _.measure   │ _.measurement │ _.per_fold │
├─────────────┼───────────────┼────────────┤
│ ZeroOneLoss │ 0.4           │ [0.4]      │
│ L1HingeLoss │ 0.8           │ [0.8]      │
│ L2HingeLoss │ 1.6           │ [1.6]      │
│ SigmoidLoss │ 0.848         │ [0.848]    │
└─────────────┴───────────────┴────────────┘
_.per_observation = [[[0.8, 0.0]], [[1.6, 0.0]], [[3.2, 0.0]], [[1.409275324764612, 0.2860870128530822]]]</code></pre><p><em>Note:</em> Although <code>ZeroOneLoss(ŷ, y)</code> makes no sense (neither <code>ŷ</code> nor <code>y</code> have a type expected by LossFunctions.jl), one can instead use the adaptor <code>MLJ.value</code> as discussed above:</p><pre><code class="language-julia-repl">julia&gt; ŷ = predict(mach, X);

julia&gt; loss = MLJ.value(ZeroOneLoss(), ŷ, X, y, w) # X is ignored here
5-element Array{Float64,1}:
 0.0
 0.0
 0.0
 1.1111111111111112
 0.0

julia&gt; mean(loss) ≈ misclassification_rate(mode.(ŷ), y, w)
true</code></pre><h3><a class="nav-anchor" id="List-of-built-in-measures-(excluding-LossFunctions.jl-losses)-1" href="#List-of-built-in-measures-(excluding-LossFunctions.jl-losses)-1">List of built-in measures (excluding LossFunctions.jl losses)</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.l1" href="#MLJBase.l1"><code>MLJBase.l1</code></a> — <span class="docstring-category">Constant</span>.</div><div><div><pre><code class="language-julia">l1(ŷ, y)
l1(ŷ, y, w)</code></pre><p>L1 per-observation loss.</p><p>For more information, run <code>info(l1)</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.l2" href="#MLJBase.l2"><code>MLJBase.l2</code></a> — <span class="docstring-category">Constant</span>.</div><div><div><pre><code class="language-julia">l2(ŷ, y)
l2(ŷ, y, w)</code></pre><p>L2 per-observation loss.</p><p>For more information, run <code>info(l2)</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.mav" href="#MLJBase.mav"><code>MLJBase.mav</code></a> — <span class="docstring-category">Constant</span>.</div><div><div><pre><code class="language-julia">mav(ŷ, y)
mav(ŷ, y, w)</code></pre><p>Mean absolute error (also known as MAE).</p><p><span>$\text{MAV} =  n^{-1}∑ᵢ|yᵢ-ŷᵢ|$</span> or <span>$\text{MAV} =  ∑ᵢwᵢ|yᵢ-ŷᵢ|/∑ᵢwᵢ$</span></p><p>For more information, run <code>info(mav)</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.misclassification_rate" href="#MLJBase.misclassification_rate"><code>MLJBase.misclassification_rate</code></a> — <span class="docstring-category">Constant</span>.</div><div><div><p>misclassification<em>rate(ŷ, y) misclassification</em>rate(ŷ, y, w) misclassification<em>rate(conf</em>mat)</p><p>Returns the rate of misclassification of the (point) predictions <code>ŷ</code>, given true observations <code>y</code>, optionally weighted by the weights <code>w</code>. All three arguments must be abstract vectors of the same length. A confusion matrix can also be passed as argument. This metric is invariant to class labelling and can be used for multiclass classification.</p><p>For more information, run <code>info(misclassification_rate)</code>. You can also equivalently use <code>mcr</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.rms" href="#MLJBase.rms"><code>MLJBase.rms</code></a> — <span class="docstring-category">Constant</span>.</div><div><div><pre><code class="language-julia">rms(ŷ, y)
rms(ŷ, y, w)</code></pre><p>Root mean squared error:</p><p><span>$\text{RMS} = \sqrt{n^{-1}∑ᵢ|yᵢ-ŷᵢ|^2}$</span> or <span>$\text{RMS} = \sqrt{\frac{∑ᵢwᵢ|yᵢ-ŷᵢ|^2}{∑ᵢwᵢ}}$</span></p><p>For more information, run <code>info(rms)</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.rmsl" href="#MLJBase.rmsl"><code>MLJBase.rmsl</code></a> — <span class="docstring-category">Constant</span>.</div><div><div><pre><code class="language-julia">rmsl(ŷ, y)</code></pre><p>Root mean squared logarithmic error:</p><p><span>$\text{RMSL} = n^{-1}∑ᵢ\log\left({yᵢ \over ŷᵢ}\right)$</span></p><p>For more information, run <code>info(rmsl)</code>.</p><p>See also <a href="#MLJBase.rmslp1"><code>rmslp1</code></a>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.rmslp1" href="#MLJBase.rmslp1"><code>MLJBase.rmslp1</code></a> — <span class="docstring-category">Constant</span>.</div><div><div><pre><code class="language-julia">rmslp1(ŷ, y)</code></pre><p>Root mean squared logarithmic error with an offset of 1:</p><p><span>$\text{RMSLP1} = n^{-1}∑ᵢ\log\left({yᵢ + 1 \over ŷᵢ + 1}\right)$</span></p><p>For more information, run <code>info(rmslp1)</code>.</p><p>See also <a href="#MLJBase.rmsl"><code>rmsl</code></a>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.rmsp" href="#MLJBase.rmsp"><code>MLJBase.rmsp</code></a> — <span class="docstring-category">Constant</span>.</div><div><div><pre><code class="language-julia">rmsp(ŷ, y)</code></pre><p>Root mean squared percentage loss:</p><p><span>$\text{RMSP} = m^{-1}∑ᵢ \left({yᵢ-ŷᵢ \over yᵢ}\right)^2$</span></p><p>where the sum is over indices such that <code>yᵢ≂̸0</code> and <code>m</code> is the number of such indices.</p><p>For more information, run <code>info(rmsp)</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.cross_entropy" href="#MLJBase.cross_entropy"><code>MLJBase.cross_entropy</code></a> — <span class="docstring-category">Constant</span>.</div><div><div><p>cross_entropy(ŷ, y::AbstractVector{&lt;:Finite})</p><p>Given an abstract vector of <code>UnivariateFinite</code> distributions <code>ŷ</code> (ie, probabilistic predictions) and an abstract vector of true observations <code>y</code>, return the negative log-probability that each observation would occur, according to the corresponding probabilistic prediction.</p><p>For more information, run <code>info(cross_entropy)</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.BrierScore" href="#MLJBase.BrierScore"><code>MLJBase.BrierScore</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">brier = BrierScore(; distribution=UnivariateFinite)
brier(ŷ, y)</code></pre><p>Given an abstract vector of distributions <code>ŷ</code> and an abstract vector of true observations <code>y</code>, return the corresponding Brier (aka quadratic) scores.</p><p>Currently only <code>distribution=UnivariateFinite</code> is supported, which is applicable to superivised models with <code>Finite</code> target scitype. In this case, if <code>p(y)</code> is the predicted probability for a <em>single</em> observation <code>y</code>, and <code>C</code> all possible classes, then the corresponding Brier score for that observation is given by</p><p><span>$2p(y) - \left(\sum_{η ∈ C} p(η)^2\right) - 1$</span></p><p>For more information, run <code>info(brier_score)</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.accuracy" href="#MLJBase.accuracy"><code>MLJBase.accuracy</code></a> — <span class="docstring-category">Constant</span>.</div><div><div><p>accuracy(ŷ, y) accuracy(ŷ, y, w) accuracy(conf_mat)</p><p>Returns the accuracy of the (point) predictions <code>ŷ</code>, given true observations <code>y</code>, optionally weighted by the weights <code>w</code>. All three arguments must be abstract vectors of the same length. This metric is invariant to class labelling and can be used for multiclass classification.</p><p>For more information, run <code>info(accuracy)</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.balanced_accuracy" href="#MLJBase.balanced_accuracy"><code>MLJBase.balanced_accuracy</code></a> — <span class="docstring-category">Constant</span>.</div><div><div><p>balanced<em>accuracy(ŷ, y [, w]) bacc(ŷ, y [, w]) bac(ŷ, y [, w]) balanced</em>accuracy(conf_mat)</p><p>Return the balanced accuracy of the point prediction <code>ŷ</code>, given true observations <code>y</code>, optionally weighted by <code>w</code>. The balanced accuracy takes into consideration class imbalance. All  three arguments must have the same length. This metric is invariant to class labelling and can be used for multiclass classification.</p><p>For more information, run <code>info(balanced_accuracy)</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.matthews_correlation" href="#MLJBase.matthews_correlation"><code>MLJBase.matthews_correlation</code></a> — <span class="docstring-category">Constant</span>.</div><div><div><p>matthews<em>correlation(ŷ, y) mcc(ŷ, y) matthews</em>correlation(conf_mat)</p><p>Return Matthews&#39; correlation coefficient corresponding to the point prediction <code>ŷ</code>, given true observations <code>y</code>. This metric is invariant to class labelling and can be used for multiclass classification.</p><p>For more information, run <code>info(matthews_correlation)</code>.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.auc" href="#MLJBase.auc"><code>MLJBase.auc</code></a> — <span class="docstring-category">Constant</span>.</div><div><div><p>auc(ŷ, y)</p><p>Return the Area Under the (ROC) Curve for probabilistic prediction <code>ŷ</code> given true observations <code>y</code>. This metric is invariant to class labelling and can be used only for binary classification.</p><p>For more information, run <code>info(auc)</code>.</p></div></div></section><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>tp</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>tn</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>fp</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>fn</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>tpr</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>tnr</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>fpr</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>fnr</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>FScore</code>. Check Documenter&#39;s build log for details.</p></div></div><h3><a class="nav-anchor" id="Other-performance-related-tools-1" href="#Other-performance-related-tools-1">Other performance related tools</a></h3><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>ConfusionMatrix</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.confusion_matrix" href="#MLJBase.confusion_matrix"><code>MLJBase.confusion_matrix</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>confusion_matrix(ŷ, y; rev=false)</p><p>Computes the confusion matrix given a predicted <code>ŷ</code> with categorical elements and the actual <code>y</code>. Rows are the predicted class, columns the ground truth. The ordering follows that of <code>levels(y)</code>.</p><p><strong>Keywords</strong></p><ul><li><code>rev=false</code>: in the binary case, this keyword allows to swap the ordering of              classes.</li><li><code>perm=[]</code>:   in the general case, this keyword allows to specify a permutation              re-ordering the classes.</li><li><code>warn=true</code>: whether to show a warning in case <code>y</code> does not have scientific type              <code>OrderedFactor{2}</code> (see note below).</li></ul><p><strong>Note</strong></p><p>To decrease the risk of unexpected errors, if <code>y</code> does not have scientific type <code>OrderedFactor{2}</code> (and so does not have a &quot;natural ordering&quot; negative-positive), a warning is shown indicating the current order unless the user specifies, explicitly either <code>rev</code> or <code>perm</code> in which case it&#39;s assumed the user is aware of the class ordering.</p></div></div></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.roc_curve" href="#MLJBase.roc_curve"><code>MLJBase.roc_curve</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>tprs, fprs, ts = roc_curve(ŷ, y) = roc(ŷ, y)</p><p>Return the ROC curve for a two-class probabilistic prediction <code>ŷ</code> given the ground  truth <code>y</code>. The true positive rates, false positive rates over a range of thresholds <code>ts</code> are returned. Note that if there are <code>k</code> unique scores, there are correspondingly  <code>k</code> thresholds and <code>k+1</code> &quot;bins&quot; over which the FPR and TPR are constant:</p><ul><li>[0.0 - thresh[1]]</li><li>[thresh[1] - thresh[2]]</li><li>...</li><li>[thresh[k] - 1]</li></ul><p>consequently, <code>tprs</code> and <code>fprs</code> are of length <code>k+1</code> if <code>ts</code> is of length <code>k</code>.</p><p>To draw the curve using your favorite plotting backend, do <code>plot(fprs, tprs)</code>.</p></div></div></section><footer><hr/><a class="previous" href="../evaluating_model_performance/"><span class="direction">Previous</span><span class="title">Evaluating Model Performance</span></a><a class="next" href="../tuning_models/"><span class="direction">Next</span><span class="title">Tuning Models</span></a></footer></article></body></html>
