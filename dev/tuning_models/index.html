<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tuning Models · MLJ</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MLJ</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Getting Started</a></li><li><a class="toctext" href="../common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="toctext" href="../model_search/">Model Search</a></li><li><a class="toctext" href="../machines/">Machines</a></li><li><a class="toctext" href="../evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="toctext" href="../performance_measures/">Performance Measures</a></li><li class="current"><a class="toctext" href>Tuning Models</a><ul class="internal"></ul></li><li><a class="toctext" href="../built_in_transformers/">Built-in Transformers</a></li><li><a class="toctext" href="../composing_models/">Composing Models</a></li><li><a class="toctext" href="../homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="toctext" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="toctext" href="../adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="toctext" href="../benchmarking/">Benchmarking</a></li><li><a class="toctext" href="../working_with_tasks/">Working with Tasks</a></li><li><a class="toctext" href="../internals/">Internals</a></li><li><a class="toctext" href="../glossary/">Glossary</a></li><li><a class="toctext" href="../api/">API</a></li><li><a class="toctext" href="../mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="toctext" href="../NEWS/">MLJ News</a></li><li><a class="toctext" href="../frequently_asked_questions/">FAQ</a></li><li><a class="toctext" href="../julia_blogpost/">Julia BlogPost</a></li><li><a class="toctext" href="../acceleration_and_parallelism/">Acceleration and Parallelism</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Tuning Models</a></li></ul><a class="edit-page" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/tuning_models.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Tuning Models</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Tuning-models-1" href="#Tuning-models-1">Tuning models</a></h1><p>In MLJ tuning is implemented as a model wrapper. After wrapping a model in a tuning strategy and binding the wrapped model to data in a machine, fitting the machine instigates a search for optimal model hyperparameters, within the specified range, and then uses all supplied data to train the best model. Making predictions using this fitted machine then amounts to predicting using a machine based on the unwrapped model with the specified hyperparameters optimized. In this way the wrapped model may be viewed as a &quot;self-tuning&quot; version of the unwrapped model.</p><h3><a class="nav-anchor" id="Tuning-a-single-hyperparameter-1" href="#Tuning-a-single-hyperparameter-1">Tuning a single hyperparameter</a></h3><div></div><pre><code class="language-julia-repl">julia&gt; using MLJ

julia&gt; X = (x1=rand(100), x2=rand(100), x3=rand(100));

julia&gt; y = 2X.x1 - X.x2 + 0.05*rand(100);

julia&gt; tree_model = @load DecisionTreeRegressor;</code></pre><p>Let&#39;s tune <code>min_purity_increase</code> in the model above, using a grid-search. Defining hyperparameter ranges and wrapping the model:</p><pre><code class="language-julia-repl">julia&gt; r = range(tree_model, :min_purity_increase, lower=0.001, upper=1.0, scale=:log);

julia&gt; self_tuning_tree_model = TunedModel(model=tree_model,
                                           resampling = CV(nfolds=3),
                                           tuning = Grid(resolution=10),
                                           ranges = r,
                                           measure = rms);</code></pre><p>Incidentally, for a numeric hyperparameter, the object returned by <code>range</code> can be iterated after specifying a resolution:</p><pre><code class="language-julia-repl">julia&gt; iterator(r, 5)
5-element Array{Float64,1}:
 0.0010000000000000002
 0.005623413251903492
 0.0316227766016838
 0.1778279410038923
 1.0</code></pre><p>Non-numeric hyperparameters are handled a little differently:</p><pre><code class="language-julia-repl">julia&gt; selector_model = FeatureSelector();

julia&gt; r2 = range(selector_model, :features, values = [[:x1,], [:x1, :x2]]);

julia&gt; iterator(r2)
2-element Array{Array{Symbol,1},1}:
 [:x1]
 [:x1, :x2]</code></pre><p>Returning to the wrapped tree model:</p><pre><code class="language-julia-repl">julia&gt; self_tuning_tree = machine(self_tuning_tree_model, X, y);

julia&gt; fit!(self_tuning_tree, verbosity=0);</code></pre><p>We can inspect the detailed results of the grid search with <code>report(self_tuning_model)</code> or just retrieve the optimal model, as here:</p><pre><code class="language-julia-repl">julia&gt; fitted_params(self_tuning_tree).best_model
MLJModels.DecisionTree_.DecisionTreeRegressor(pruning_purity_threshold = 0.0,
                                              max_depth = -1,
                                              min_samples_leaf = 5,
                                              min_samples_split = 2,
                                              min_purity_increase = 0.0010000000000000002,
                                              n_subfeatures = 0,
                                              post_prune = false,) @ 8…28</code></pre><p>Predicting on new input observations using the optimal model:</p><pre><code class="language-julia-repl">julia&gt; predict(self_tuning_tree, (x1=rand(3), x2=rand(3), x3=rand(3)))
3-element Array{Float64,1}:
 0.68693420317309
 1.517905733355572
 0.12574044598261233</code></pre><h3><a class="nav-anchor" id="Tuning-multiple-nested-hyperparameters-1" href="#Tuning-multiple-nested-hyperparameters-1">Tuning multiple nested hyperparameters</a></h3><p>The following model has another model, namely a <code>DecisionTreeRegressor</code>, as a hyperparameter:</p><div></div><pre><code class="language-julia">julia&gt; tree_model = DecisionTreeRegressor()
julia&gt; forest_model = EnsembleModel(atom=tree_model); </code></pre><p>Nested hyperparameters can be inspected using <code>params</code> (or just type <code>@more</code> in the REPL after instantiating <code>forest_model</code>):</p><pre><code class="language-julia-repl">julia&gt; params(forest_model)
(atom = (pruning_purity_threshold = 0.0,
         max_depth = -1,
         min_samples_leaf = 5,
         min_samples_split = 2,
         min_purity_increase = 0.0,
         n_subfeatures = 0,
         post_prune = false,),
 weights = Float64[],
 bagging_fraction = 0.8,
 rng = MersenneTwister(UInt32[0x4d1f026e, 0x8d7fec25, 0x78202f12, 0x6a6a0831]),
 n = 100,
 acceleration = ComputationalResources.CPU1{Nothing}(nothing),
 out_of_bag_measure = Any[],)</code></pre><p>Ranges for nested hyperparameters are specified using dot syntax:</p><pre><code class="language-julia-repl">julia&gt; r1 = range(forest_model, :(atom.n_subfeatures), lower=1, upper=3);

julia&gt; r2 = range(forest_model, :bagging_fraction, lower=0.4, upper=1.0);

julia&gt; self_tuning_forest_model = TunedModel(model=forest_model,
                                             tuning=Grid(resolution=12),
                                             resampling=CV(nfolds=6),
                                             ranges=[r1, r2],
                                             measure=rms);

julia&gt; self_tuning_forest = machine(self_tuning_forest_model, X, y);

julia&gt; fit!(self_tuning_forest, verbosity=0)
Machine{DeterministicTunedModel} @ 1…70

julia&gt; report(self_tuning_forest)
(parameter_names = [&quot;atom.n_subfeatures&quot; &quot;bagging_fraction&quot;],
 parameter_scales = Symbol[:linear :linear],
 parameter_values = Any[1 0.4; 2 0.4; … ; 2 1.0; 3 1.0],
 measurements = [0.3306113484239433, 0.2410619955308142, 0.24285283259276638, 0.3232432359786891, 0.2165770103233904, 0.21524342232329963, 0.33105672779858647, 0.21322543409793124, 0.20534289995322078, 0.3095275236909751  …  0.17105414439129266, 0.2670133704457056, 0.16361605134000778, 0.17082613537484173, 0.2690531469775497, 0.16829749089840243, 0.1789624182752565, 0.2633414351297431, 0.16479972100160267, 0.20330763046523934],
 best_measurement = 0.16361605134000778,)</code></pre><p>In this two-parameter case, a plot of the grid search results is also available:</p><pre><code class="language-julia">using Plots
plot(self_tuning_forest)</code></pre><p><img src="../tuning_plot.png" alt/></p><p>It is also possible to specify different resolutions for each dimension of the grid. See <a href="@ref">Grid</a> below for details.</p><h3><a class="nav-anchor" id="API-1" href="#API-1">API</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Base.range" href="#Base.range"><code>Base.range</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">r = range(model, :hyper; values=nothing)</code></pre><p>Defines a <code>NominalRange</code> object for a field <code>hyper</code> of <code>model</code>, assuming the field is a not a subtype of <code>Real</code>. Note that <code>r</code> is not directly iterable but <code>iterator(r)</code> iterates over <code>values</code>.</p><p>A nested hyperparameter is specified using dot notation. For example, <code>:(atom.max_depth)</code> specifies the <code>:max_depth</code> hyperparameter of the hyperparameter <code>:atom</code> of <code>model</code>.</p><pre><code class="language-none">r = range(model, :hyper; upper=nothing, lower=nothing, scale=:linear)</code></pre><p>Defines a <code>NumericRange</code> object for a <code>Real</code> field <code>hyper</code> of <code>model</code>. Note that <code>r</code> is not directly iteratable but <code>iterator(r, n)</code> iterates over <code>n</code> values between <code>lower</code> and <code>upper</code> values, according to the specified <code>scale</code>. The supported scales are <code>:linear, :log, :log10, :log2</code>. Values for <code>Integer</code> types are rounded (with duplicate values removed, resulting in possibly less than <code>n</code> values).</p><p>Alternatively, if a function <code>f</code> is provided as <code>scale</code>, then <code>iterator(r, n)</code> iterates over the values <code>[f(x1), f(x2), ... , f(xn)]</code>, where <code>x1, x2, ..., xn</code> are linearly spaced between <code>lower</code> and <code>upper</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/d1752d4ea00438acba07d86de65fc8876cc518a4/src/parameters.jl#L63-L88">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.Grid" href="#MLJ.Grid"><code>MLJ.Grid</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">Grid(resolution=10, parallel=true)</code></pre><p>Define a grid-based hyperparameter tuning strategy, using the specified <code>resolution</code> for numeric hyperparameters. For use with a <code>TunedModel</code> object.</p><p>Individual hyperparameter resolutions can also be specified, as in</p><pre><code class="language-none">Grid(resolution=[:n =&gt; r1, :(atom.max_depth) =&gt; r2])</code></pre><p>where <code>r1</code> and <code>r2</code> are <code>NumericRange</code> objects.</p><p>See also <a href="@ref">TunedModel</a>, <a href="@ref">range</a>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/d1752d4ea00438acba07d86de65fc8876cc518a4/src/tuning.jl#L4-L19">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJ.TunedModel" href="#MLJ.TunedModel"><code>MLJ.TunedModel</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">tuned_model = TunedModel(; model=nothing,
                         tuning=Grid(),
                         resampling=Holdout(),
                         measure=nothing,
                         weights=nothing,
                         operation=predict,
                         ranges=ParamRange[],
                         full_report=true)</code></pre><p>Construct a model wrapper for hyperparameter optimization of a supervised learner.</p><p>Calling <code>fit!(mach)</code> on a machine <code>mach=machine(tuned_model, X, y)</code> or <code>mach=machine(tuned_model, task)</code> will:</p><ul><li><p>Instigate a search, over clones of <code>model</code>, with the hyperparameter mutations specified by <code>ranges</code>, for a model optimizing the specified <code>measure</code>, using performance evaluations carried out using the specified <code>tuning</code> strategy and <code>resampling</code> strategy.</p></li><li><p>Fit an internal machine, based on the optimal model <code>fitted_params(mach).best_model</code>, wrapping the optimal <code>model</code> object in <em>all</em> the provided data <code>X, y</code> (or in <code>task</code>). Calling <code>predict(mach, Xnew)</code> then returns predictions on <code>Xnew</code> of this internal machine.</p></li></ul><p><em>Important.</em> If a custom measure <code>measure</code> is used, and the measure is a score, rather than a loss, be sure to check that <code>MLJ.orientation(measure) == :score</code> to ensure maximization of the measure, rather than minimization. Overide an incorrect value with <code>MLJ.orientation(::typeof(measure)) = :score</code>.</p><p>If <code>measure</code> supports sample weights (<code>MLJ.supports_weights(measure) == true</code>) then these can be passed to the measure as <code>weights</code>.</p><p>In the case of two-parameter tuning, a Plots.jl plot of performance estimates is returned by <code>plot(mach)</code> or <code>heatmap(mach)</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJ.jl/blob/d1752d4ea00438acba07d86de65fc8876cc518a4/src/tuning.jl#L70-L108">source</a></section><footer><hr/><a class="previous" href="../performance_measures/"><span class="direction">Previous</span><span class="title">Performance Measures</span></a><a class="next" href="../built_in_transformers/"><span class="direction">Next</span><span class="title">Built-in Transformers</span></a></footer></article></body></html>
