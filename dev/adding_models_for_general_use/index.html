<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Adding Models for General Use · MLJ</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MLJ</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Getting Started</a></li><li><a class="toctext" href="../working_with_tasks/">Working with Tasks</a></li><li><a class="toctext" href="../built_in_transformers/">Built-in Transformers</a></li><li><a class="toctext" href="../learning_networks/">Learning Networks</a></li><li><a class="toctext" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li class="current"><a class="toctext" href>Adding Models for General Use</a><ul class="internal"></ul></li><li><a class="toctext" href="../internals/">Internals</a></li><li><a class="toctext" href="../glossary/">Glossary</a></li><li><a class="toctext" href="../api/">API</a></li><li><a class="toctext" href="../frequently_asked_questions/">FAQ</a></li><li><a class="toctext" href="../NEWS/">MLJ News</a></li><li><a class="toctext" href="../julia_blogpost/">Julia BlogPost</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Adding Models for General Use</a></li></ul><a class="edit-page" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/adding_models_for_general_use.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Adding Models for General Use</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Adding-New-Models-1" href="#Adding-New-Models-1">Adding New Models</a></h1><p>This guide outlines in detail the specification of the MLJ model interface and provides guidelines for implementing the interface for models intended for general use. For sample implementations, see <a href="https://github.com/alan-turing-institute/MLJModels.jl/tree/master/src">MLJModels/src</a>.</p><p>The machine learning tools provided by MLJ can be applied to the models in any package that imports the package <a href="https://github.com/alan-turing-institute/MLJBase.jl">MLJBase</a> and implements the API defined there, as outlined below. For a quick-and-dirty implementation of user-defined models see <a href="../simple_user_defined_models/">Simple User Defined Models</a>.  To make new models available to all MLJ users, see <a href="#Where-to-place-code-implementing-new-models-1">Where to place code implementing new models</a>.</p><p>It is assumed the reader has read <a href="../">Getting Started</a>. To implement the API described here, some familiarity with the following packages is also helpful:</p><ul><li><p><a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a> (for probabilistic predictions)</p></li><li><p><a href="https://github.com/JuliaData/CategoricalArrays.jl">CategoricalArrays.jl</a> (essential if you are implementing a model handling data of <code>Multiclass</code> or <code>OrderedFactor</code> scitype)</p></li><li><p><a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> (if you&#39;re algorithm needs input data in a novel format).</p></li></ul><p>In MLJ, the basic interface exposed to the user, built atop the model interface described here, is the <em>machine interface</em>. After a first reading of this document, the reader may wish to refer to <a href="../internals/">MLJ Internals</a> for context.</p><h3><a class="nav-anchor" id="Overview-1" href="#Overview-1">Overview</a></h3><p>A <em>model</em> is an object storing hyperparameters associated with some machine learning algorithm.  In MLJ, hyperparameters include configuration parameters, like the number of threads, and special instructions, such as &quot;compute feature rankings&quot;, which may or may not affect the final learning outcome.  However, the logging level (<code>verbosity</code> below) is excluded.</p><p>The name of the Julia type associated with a model indicates the associated algorithm (e.g., <code>DecisionTreeClassifier</code>). The outcome of training a learning algorithm is called a <em>fitresult</em>. For ordinary multilinear regression, for example, this would be the coefficients and intercept. For a general supervised model, it is the (generally minimal) information needed to make new predictions.</p><p>The ultimate supertype of all models is <code>MLJBase.Model</code>, which has two abstract subtypes:</p><pre><code class="language-julia">abstract type Supervised &lt;: Model end
abstract type Unsupervised &lt;: Model end</code></pre><p><code>Supervised</code> models are further divided according to whether they are able to furnish probabilistic predictions of the target (which they will then do so by default) or directly predict &quot;point&quot; estimates, for each new input pattern:</p><pre><code class="language-julia">abstract type Probabilistic &lt;: Supervised end
abstract type Deterministic &lt;: Supervised end</code></pre><p>Further division of model types is realized through <a href="#Trait-declarations-1">Trait declarations</a>.</p><p>Associated with every concrete subtype of <code>Model</code> there must be a <code>fit</code> method, which implements the associated algorithm to produce the fitresult. Additionally, every <code>Supervised</code> model has a <code>predict</code> method, while <code>Unsupervised</code> models must have a <code>transform</code> method. More generally, methods such as these, that are dispatched on a model instance and a fitresult (plus other data), are called <em>operations</em>. <code>Probabilistic</code> supervised models optionally implement a <code>predict_mode</code> operation (in the case of classifiers) or a <code>predict_mean</code> and/or <code>predict_median</code> operations (in the case of regressors) although MLJBase also provides fallbacks that will suffice in most cases. <code>Unsupervised</code> models may implement an <code>inverse_transform</code> operation.</p><h3><a class="nav-anchor" id="New-model-type-declarations-and-optional-clean!-method-1" href="#New-model-type-declarations-and-optional-clean!-method-1">New model type declarations and optional clean! method</a></h3><p>Here is an example of a concrete supervised model type declaration:</p><pre><code class="language-julia">import MLJ

mutable struct RidgeRegressor &lt;: MLJBase.Deterministic
    lambda::Float64
end</code></pre><p>Models (which are mutable) should not be given internal constructors. It is recommended that they be given an external lazy keyword constructor of the same name. This constructor defines default values for every field, and optionally corrects invalid field values by calling a <code>clean!</code> method (whose fallback returns an empty message string):</p><pre><code class="language-julia">function MLJ.clean!(model::RidgeRegressor)
    warning = &quot;&quot;
    if model.lambda &lt; 0
        warning *= &quot;Need lambda ≥ 0. Resetting lambda=0. &quot;
        model.lambda = 0
    end
    return warning
end

# keyword constructor
function RidgeRegressor(; lambda=0.0)
    model = RidgeRegressor(lambda)
    message = MLJBase.clean!(model)
    isempty(message) || @warn message
    return model
end</code></pre><h3><a class="nav-anchor" id="Supervised-models-1" href="#Supervised-models-1">Supervised models</a></h3><p>The compulsory and optional methods to be implemented for each concrete type <code>SomeSupervisedModel &lt;: MLJBase.Supervised</code> are summarized below. An <code>=</code> indicates the return value for a fallback version of the method.</p><h4><a class="nav-anchor" id="Summary-of-methods-1" href="#Summary-of-methods-1">Summary of methods</a></h4><p>Compulsory:</p><pre><code class="language-julia">MLJBase.fit(model::SomeSupervisedModel, verbosity::Integer, X, y) -&gt; fitresult, cache, report
MLJBase.predict(model::SomeSupervisedModel, fitresult, Xnew) -&gt; yhat</code></pre><p>Fallback to be overridden if model input is univariate:</p><pre><code class="language-julia">MLJBase.input_is_multivariate(::Type{&lt;:SomeSupervisedModel}) = true</code></pre><p>Optional, to check and correct invalid hyperparameter values:</p><pre><code class="language-julia">MLJBase.clean!(model::SomeSupervisedModel) = &quot;&quot; </code></pre><p>Optional, to return user-friendly form of fitted parameters:</p><pre><code class="language-julia">MLJBase.fitted_params(model::SomeSupervisedModel, fitresult) = fitresult</code></pre><p>Optional, to avoid redundant calculations when re-fitting machines:</p><pre><code class="language-julia">MLJBase.update(model::SomeSupervisedModel, verbosity, old_fitresult, old_cache, X, y) =
   MLJBase.fit(model, verbosity, X, y)</code></pre><p>Optional, if <code>SomeSupervisedModel &lt;: Probabilistic</code>:</p><pre><code class="language-julia">MLJBase.predict_mode(model::SomeSupervisedModel, fitresult, Xnew) =
    mode.(predict(model, fitresult, Xnew))
MLJBase.predict_mean(model::SomeSupervisedModel, fitresult, Xnew) =
    mean.(predict(model, fitresult, Xnew))
MLJBase.predict_median(model::SomeSupervisedModel, fitresult, Xnew) =
    median.(predict(model, fitresult, Xnew))</code></pre><p>Required, if model is to be registered (findable by general users):</p><pre><code class="language-julia">MLJBase.load_path(::Type{&lt;:SomeSupervisedModel})    = &quot;&quot;
MLJBase.package_name(::Type{&lt;:SomeSupervisedModel}) = &quot;Unknown&quot;
MLJBase.package_uuid(::Type{&lt;:SomeSupervisedModel}) = &quot;Unknown&quot;</code></pre><p>Recommended, to constrain the form of input data passed to fit and predict:</p><pre><code class="language-julia">MLJBase.input_scitype_union(::Type{&lt;:SomeSupervisedModel}) = Union{Missing,Found}</code></pre><p>Recommended, to constrain the form of target data passed to fit (and compulsory if target is multivariate/sequential):</p><pre><code class="language-julia">MLJBase.target_scitype_union(::Type{&lt;:SomeSupervisedModel}) = Union{Found,NTuple{&lt;:Found}}</code></pre><p>Optional but recommended:</p><pre><code class="language-julia">MLJBase.package_url(::Type{&lt;:SomeSupervisedModel})  = &quot;Unknown&quot;
MLJBase.is_pure_julia(::Type{&lt;:SomeSupervisedModel}) = false</code></pre><h4><a class="nav-anchor" id="The-form-of-data-for-fitting-and-predicting-1" href="#The-form-of-data-for-fitting-and-predicting-1">The form of data for fitting and predicting</a></h4><p>The inputs <code>X</code> and <code>Xnew</code> for <code>fit</code> and <code>predict</code> are always tables, unless one defines</p><pre><code class="language-julia">MLJBase.input_is_multivariate(::Type{&lt;:SomeSupervisedModel}) = false</code></pre><p>The target <code>y</code> is always an <code>AbstractVector</code> (see the discussion in <a href="../#Getting-Started-1">Getting Started</a>). For multivariate or sequence-valued targets, a <code>target_scitype_union</code> declaration is required. This is discussed under <a href="#Trait-declarations-1">Trait declarations</a> below, which also describes how to constrain the element types of data.</p><h5><a class="nav-anchor" id="Additional-type-coercions-1" href="#Additional-type-coercions-1">Additional type coercions</a></h5><p>If the core algorithm being wrapped requires data in a different or more specific form, then <code>fit</code> will need to coerce the table into the form desired (and the same coercions applied to <code>X</code> will have to be repeated for <code>Xnew</code> in <code>predict</code>). To assist with common cases, MLJ provides the convenience method <code>MLJBase.matrix</code>. <code>MLJBase.matrix(Xtable)</code> has type <code>Matrix{T}</code> where <code>T</code> is the tightest common type of elements of <code>Xtable</code>, and <code>Xtable</code> is any table.</p><p>Other auxiliary methods provided by MLJBase for handling tabular data are: <code>selectrows</code>, <code>selectcols</code>, <code>select</code> and <code>schema</code> (for extracting the size, names and eltypes of a table). See <a href="#Convenience-methods-1">Convenience methods</a> below for details.</p><h5><a class="nav-anchor" id="Important-convention-1" href="#Important-convention-1">Important convention</a></h5><p>It is to be understood that the columns of the table <code>X</code> correspond to features and the rows to patterns.</p><h4><a class="nav-anchor" id="The-fit-method-1" href="#The-fit-method-1">The fit method</a></h4><p>A compulsory <code>fit</code> method returns three objects:</p><pre><code class="language-julia">MLJBase.fit(model::SomeSupervisedModel, verbosity::Int, X, y) -&gt; fitresult, cache, report</code></pre><p>Note: The <code>Int</code> typing of <code>verbosity</code> cannot be omitted.</p><ol><li><p><code>fitresult::R</code> is the fitresult in the sense above (which becomes an  argument for <code>predict</code> discussed below).</p></li><li><p><code>report</code> is a (possibly empty) <code>NamedTuple</code>, for example,  <code>report=(deviance=..., dof_residual=..., stderror=..., vcov=...)</code>.  Any training-related statistics, such as internal estimates of the  generalization error, and feature rankings, should be returned in  the <code>report</code> tuple. How, or if, these are generated should be  controlled by hyperparameters (the fields of <code>model</code>). Fitted  parameters, such as the coefficients of a linear model, do not go  in the report as they will be extractable from <code>fitresult</code> (and  accessible to MLJ through the <code>fitted_params</code> method described below).</p></li></ol><p>3.	The value of <code>cache</code> can be <code>nothing</code>, unless one is also defining       an <code>update</code> method (see below). The Julia type of <code>cache</code> is not       presently restricted.</p><p>It is not necessary for <code>fit</code> to provide dimension checks or to call <code>clean!</code> on the model; MLJ will carry out such checks.</p><p>The method <code>fit</code> should never alter hyperparameter values. If the package is able to suggest better hyperparameters, as a byproduct of training, return these in the report field.</p><p>One should test that actual fitresults have the type declared in the model <code>mutable struct</code> declaration. To help with this, <code>MLJBase.fitresult_type(m)</code> returns the declared type, for any supervised model (or model type) <code>m</code>.</p><p>The <code>verbosity</code> level (0 for silent) is for passing to learning algorithm itself. A <code>fit</code> method wrapping such an algorithm should generally avoid doing any of its own logging.</p><h4><a class="nav-anchor" id="The-fitted_params-method-1" href="#The-fitted_params-method-1">The fitted_params method</a></h4><p>A <code>fitted_params</code> method may be optionally overloaded. It&#39;s purpose is to provide MLJ access to a user-friendly representation of the learned parameters of the model (as opposed to the hyperparameters). They must be extractable from <code>fitresult</code>.</p><pre><code class="language-julia">MLJBase.fitted_params(model::SomeSupervisedModel, fitresult) -&gt; friendly_fitresult::NamedTuple</code></pre><p>For a linear model, for example, one might declare something like <code>friendly_fitresult=(coefs=[...], bias=...)</code>.</p><p>The fallback is to return <code>(fitresult=fitresult,)</code>.</p><h4><a class="nav-anchor" id="The-predict-method-1" href="#The-predict-method-1">The predict method</a></h4><p>A compulsory <code>predict</code> method has the form</p><pre><code class="language-julia">MLJBase.predict(model::SomeSupervisedModel, fitresult, Xnew) -&gt; yhat</code></pre><p>Here <code>Xnew</code> will be have the same form as the <code>X</code> passed to <code>fit</code>.</p><h5><a class="nav-anchor" id="Prediction-types-for-deterministic-responses.-1" href="#Prediction-types-for-deterministic-responses.-1">Prediction types for deterministic responses.</a></h5><p>In the case of <code>Deterministic</code> models, <code>yhat</code> should be an <code>AbstractVector</code> (commonly a plain <code>Vector</code>) with the same element type as the target <code>y</code> passed to the <code>fit</code> method (see above). Any <code>CategoricalValue</code> or <code>CategoricalString</code> appearing in <code>yhat</code> <strong>must have the same levels in its the pool as was present in the elements of the target <code>y</code> presented in training</strong>, even if not all levels appear in the training data or prediction itself. For example, in the univariate target case, this means <code>MLJ.classes(yhat[i]) = MLJ.classes(y[j])</code> for all admissible <code>i</code> and <code>j</code>. (The method <code>classes</code> is described under <a href="#Convenience-methods-1">Convenience methods</a> below).</p><p>Unfortunately, code not written with the preservation of categorical levels in mind poses special problems. To help with this, MLJBase provides three utility methods: <code>int</code> (for converting a <code>CategoricalValue</code> or <code>CategoricalString</code> into an integer, the ordering of these integers being consistent with that of the pool), <code>decoder</code> (for constructing a callable object that decodes the integers back into <code>CategoricalValue</code>/<code>CategoricalString</code> objects), and <code>classes</code>, for extracting the complete pool from a single value. Refer to <a href="#Convenience-methods-1">Convenience methods</a> below for important details.</p><p>Note that a decoder created during <code>fit</code> may need to be bundled with <code>fitresult</code> to make it available to <code>predict</code> during re-encoding. So, for example, if the core algorithm being wrapped by <code>fit</code> expects a nominal target <code>yint</code> of type <code>Vector{&lt;:Integer}</code> then a <code>fit</code> method may look something like this:</p><pre><code class="language-julia">function MLJBase.fit(model::SomeSupervisedModel, verbosity, X, y)
    yint = MLJBase.int(y) 
    a_target_element = y[1]                    # a CategoricalValue/String
	decode = MLJBase.decoder(a_target_element) # can be called on integers
	
    core_fitresult = SomePackage.fit(X, yint, verbosity=verbosity)

    fitresult = (decode, core_fitresult)
    cache = nothing
    report = nothing
    return fitresult, cache, report
end</code></pre><p>while a corresponding deterministic <code>predict</code> operation might look like this:</p><pre><code class="language-julia">function MLJBase.predict(model::SomeSupervisedModel, fitresult, Xnew)
    decode, core_fitresult = fitresult
    yhat = SomePackage.predict(core_fitresult, Xnew)
    return decode.(yhat)  # or decode(yhat) also works
end</code></pre><p>For a concrete example, refer to the <a href="https://github.com/alan-turing-institute/MLJModels.jl/blob/master/src/ScikitLearn.jl">code</a> for <code>SVMClassifier</code>.</p><p>Of course, if you are coding a learning algorithm from scratch, rather than wrapping an existing one, these extra measures may be unnecessary.</p><h5><a class="nav-anchor" id="Prediction-types-for-probabilistic-responses-1" href="#Prediction-types-for-probabilistic-responses-1">Prediction types for probabilistic responses</a></h5><p>In the case of <code>Probabilistic</code> models with univariate targets, <code>yhat</code> must be a <code>Vector</code> whose elements are distributions (one distribution per row of <code>Xnew</code>).</p><p>Presently, a <em>distribution</em> is any object <code>d</code> for which <code>MLJBase.isdistribution(::d) = true</code>, which includes all objects of type <code>Distributions.Distribution</code> from the package Distributions.jl. The declaration <code>MLJBase.isdistribution(::d) = true</code> implies that at least <code>Base.rand(d)</code> is implemented, but the rest of this API is still a work-in-progress.</p><p>Use the distribution <code>MLJBase.UnivariateFinite</code> for <code>Probabilistic</code> models predicting a target with <code>Finite</code> scitype (classifiers). In this case each element of the training target <code>y</code> is a <code>CategoricalValue</code> or <code>CategoricalString</code>, as in this contrived example:</p><pre><code class="language-julia">using CategoricalArrays
y = identity.(categorical([:yes, :no, :no, :maybe, :maybe]))</code></pre><p>Note that, as in this case, we cannot assume <code>y</code> is a <code>CategoricalVector</code>, and we rely on elements for pool information (if we need it); this is accessible using the convenience method <code>MLJ.classes</code>:</p><pre><code class="language-julia">julia&gt; yes = y[1]
julia&gt; levels = MLJBase.classes(yes)
3-element Array{CategoricalValue{Symbol,UInt32},1}:
 :maybe
 :no
 :yes</code></pre><p>Now supposing that, for some new input pattern, the elements <code>yes = y[1]</code> and <code>no = y[2]</code> are to be assigned respective probabilities of 0.2 and 0.8. Then the corresponding distribution <code>d</code> is constructed as follows:</p><pre><code class="language-julia">julia&gt; d = MLJBase.UnivariateFinite([yes, no], [0.2, 0.8])
UnivariateFinite{CategoricalValue{Symbol,UInt32},Float64}(Dict(:yes=&gt;0.2,:maybe=&gt;0.0,:no=&gt;0.8))

julia&gt; pdf(d, yes)
0.2

julia&gt; maybe = y[4]; pdf(d, maybe)
0.0</code></pre><p>Alternatively, a dictionary can be passed to the constructor. </p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.UnivariateFinite" href="#MLJBase.UnivariateFinite"><code>MLJBase.UnivariateFinite</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">UnivariateFinite(levels, p)</code></pre><p>A discrete univariate distribution whose finite support is the elements of the vector <code>levels</code>, and whose corresponding probabilities are elements of the vector <code>p</code>, which must sum to one.</p><p>In the special case that <code>levels</code> has type <code>AbstractVector{L}</code> where <code>L &lt;: CategoricalValue</code> or <code>L &lt;: CategoricalString</code> (for example <code>levels</code> is a <code>CategoricalVector</code>) the constructor adds the unobserved classes (from the common pool) with probability zero.</p><pre><code class="language-none">UnivariateFinite(prob_given_level)</code></pre><p>A discrete univariate distribution whose finite support is the set of keys of the provided dictionary, <code>prob_given_level</code>. The dictionary values specify the corresponding probabilities, which must be nonnegative and sum to one. </p><p>In the special case that <code>keys(prob_given_level)</code> has type <code>AbstractVector{L}</code> where <code>L &lt;: CategoricalValue</code> or <code>L &lt;: CategoricalString</code> (for example it is a <code>CategoricalVector</code>) the constructor adds the unobserved classes from the common pool with probability zero.</p><pre><code class="language-none">levels(d::UnivariateFinite)</code></pre><p>Return the levels of <code>d</code>.</p><pre><code class="language-julia">d = UnivariateFinite([&quot;yes&quot;, &quot;no&quot;, &quot;maybe&quot;], [0.1, 0.2, 0.7])
pdf(d, &quot;no&quot;) # 0.2
mode(d) # &quot;maybe&quot;
rand(d, 5) # [&quot;maybe&quot;, &quot;no&quot;, &quot;maybe&quot;, &quot;maybe&quot;, &quot;no&quot;]
d = fit(UnivariateFinite, [&quot;maybe&quot;, &quot;no&quot;, &quot;maybe&quot;, &quot;yes&quot;])
pdf(d, &quot;maybe&quot;) ≈ 0.5 # true
levels(d) # [&quot;yes&quot;, &quot;no&quot;, &quot;maybe&quot;]</code></pre><p>If the element type of <code>v</code> is a <code>CategoricalValue</code> or <code>CategoricalString</code>, then <code>fit(UnivariateFinite, v)</code> assigns a probability of zero to unobserved classes from the common pool.</p><p>See also classes</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/1bbdb3dc66f5e751725cc998812488459cbc2be0/src/distributions.jl#L37-L82">source</a></section><h4><a class="nav-anchor" id="Trait-declarations-1" href="#Trait-declarations-1">Trait declarations</a></h4><p>Two trait functions allow the implementer to restrict the types of elements appearing in the inputs <code>X</code>, and <code>Xnew</code> passed to <code>fit</code> and <code>predict</code>, and the elements appearing in the training target <code>y</code>. The MLJ task interface also uses these traits to match models to tasks. So if they are omitted (and your model is registered) then a general user may attempt to use your model with inappropriately typed data.</p><p>The trait functions <code>input_scitype_union</code> and <code>target_scitype_union</code> take scientific data types as values (see <a href="../#Getting-Started-1">Getting Started</a> for scitype basics). These types are organized in the following hierarchy:</p><p><img src="../scitypes.png" alt/></p><p>For example,  to ensure that elements of <code>X</code> presented to the <code>DecisionTreeClassifier</code> <code>fit</code> method all have <code>Continuous</code> scitype (and hence <code>AbstractFloat</code> machine type), one declares</p><pre><code class="language-julia">MLJBase.input_scitype_union(::Type{&lt;:DecisionTreeClassifier}) = MLJBase.Continuous</code></pre><p>For, in general, MLJ will never call <code>fit(model::SomeSuperivsedModel, verbosity, X, y)</code> unless <code>Union{scitypes(X)...} &lt;: inputs_scitype_union(SomeSupervisedModel)</code> holds. (See <a href="#Convenience-methods-1">Convenience methods</a> below for more on the <code>scitypes</code> and related <code>scitype_union</code> methods.)</p><p>Similarly, one declares</p><pre><code class="language-julia">MLJBase.target_scitype_union(::Type{&lt;:DecisionTreeClassifier}) = MLJBase.Finite</code></pre><p>to ensure that all elements of the target <code>y</code> (which is always an <code>AbstractVector</code>) have <code>Finite</code> scitype (and hence <code>CategoricalValue</code> or <code>CategoricalString</code> machine type). This is because, in the general case, MLJ guarantees that <code>scitype_union(y) &lt;: target_scitype_union(SomeSupervisedModel)</code>.</p><h5><a class="nav-anchor" id="Multivariate-targets-1" href="#Multivariate-targets-1">Multivariate targets</a></h5><p>The above remarks continue to hold unchanged for the case multivariate targets.  In this case the elements of the <code>AbstractVector</code> <code>y</code> are now tuples. If, for example, you declare</p><pre><code class="language-julia">target_scitype_union(SomeSupervisedModel) = Tuple{Continuous,Count}</code></pre><p>then each element of <code>y</code> will be a tuple of type <code>Tuple{AbstractFloat,Integer}</code>. For predicting variable length sequences of, say, binary values, use</p><pre><code class="language-julia">target_scitype_union(SomeSupervisedModel) = NTuple{&lt;:Multiclass{2}}</code></pre><p>The trait functions controlling the form of data are summarized as follows:</p><table><tr><th style="text-align: right">method</th><th style="text-align: right">return type</th><th style="text-align: right">declarable return values</th><th style="text-align: right">default value</th></tr><tr><td style="text-align: right"><code>input_is_multivariate</code></td><td style="text-align: right"><code>Bool</code></td><td style="text-align: right"><code>true</code> or <code>false</code></td><td style="text-align: right"><code>true</code></td></tr><tr><td style="text-align: right"><code>input_scitype_union</code></td><td style="text-align: right"><code>DataType</code></td><td style="text-align: right">subtype of <code>Union{Missing,Found}</code></td><td style="text-align: right"><code>Union{Missing,Found}</code></td></tr><tr><td style="text-align: right"><code>target_scitype_union</code></td><td style="text-align: right"><code>DataType</code></td><td style="text-align: right">subtype of <code>Found</code> or tuple of such types</td><td style="text-align: right"><code>Union{Found,NTuple{&lt;:Found}}</code></td></tr></table><p>Additional trait functions tell MLJ&#39;s <code>@load</code> macro how to find your model if it is registered, and provide other self-explanatory metadata about the model:</p><table><tr><th style="text-align: right">method</th><th style="text-align: right">return type</th><th style="text-align: right">declarable return values</th><th style="text-align: right">default value</th></tr><tr><td style="text-align: right"><code>load_path</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr><tr><td style="text-align: right"><code>package_name</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr><tr><td style="text-align: right"><code>package_uuid</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr><tr><td style="text-align: right"><code>package_url</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr><tr><td style="text-align: right"><code>is_pure_julia</code></td><td style="text-align: right"><code>Bool</code></td><td style="text-align: right"><code>true</code> or <code>false</code></td><td style="text-align: right"><code>false</code></td></tr></table><p>Here is the complete list of trait function declarations for <code>DecistionTreeClassifier</code>  (<a href="https://github.com/alan-turing-institute/MLJModels.jl/blob/master/src/DecisionTree.jl">source</a>):</p><pre><code class="language-julia">MLJBase.input_is_multivariate(::Type{&lt;:DecisionTreeClassifier}) = true
MLJBase.input_scitype_union(::Type{&lt;:DecisionTreeClassifier}) = MLJBase.Continuous
MLJBase.target_scitype_union(::Type{&lt;:DecisionTreeClassifier}) = MLJBase.Finite
MLJBase.load_path(::Type{&lt;:DecisionTreeClassifier}) = &quot;MLJModels.DecisionTree_.DecisionTreeClassifier&quot; 
MLJBase.package_name(::Type{&lt;:DecisionTreeClassifier}) = &quot;DecisionTree&quot;
MLJBase.package_uuid(::Type{&lt;:DecisionTreeClassifier}) = &quot;7806a523-6efd-50cb-b5f6-3fa6f1930dbb&quot;
MLJBase.package_url(::Type{&lt;:DecisionTreeClassifier}) = &quot;https://github.com/bensadeghi/DecisionTree.jl&quot;
MLJBase.is_pure_julia(::Type{&lt;:DecisionTreeClassifier}) = true</code></pre><p>You can test all your declarations of traits by calling <code>info(SomeModel)</code>.</p><h4><a class="nav-anchor" id="Iterative-models-and-the-update!-method-1" href="#Iterative-models-and-the-update!-method-1">Iterative models and the update! method</a></h4><p>An <code>update</code> method may be optionally overloaded to enable a call by MLJ to retrain a model (on the same training data) to avoid repeating computations unnecessarily.</p><pre><code class="language-julia">MLJBase.update(model::SomeSupervisedModel, verbosity, old_fitresult, old_cache, X, y) -&gt; fitresult, cache, report</code></pre><p>If an MLJ <code>Machine</code> is being <code>fit!</code> and it is not the first time, then <code>update</code> is called instead of <code>fit</code>, unless the machine <code>fit!</code> has been called with a new <code>rows</code> keyword argument. However, <code>MLJBase</code> defines a fallback for <code>update</code> which just calls <code>fit</code>. For context, see <a href="../internals/">MLJ Internals</a>.</p><p>Learning networks wrapped as models constitute one use-case (see <a href="../learning_networks/#Learning-Networks-1">Learning Networks</a>): One would like each component model to be retrained only when hyperparameter changes &quot;upstream&quot; make this necessary. In this case MLJ provides a fallback (specifically, the fallback is for any subtype of <code>SupervisedNetwork = Union{DeterministicNetwork,ProbabilisticNetwork}</code>). A second more generally relevant use-case is iterative models, where calls to increase the number of iterations only restarts the iterative procedure if other hyperparameters have also changed. For an example, see the MLJ <a href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/src/ensembles.jl">ensemble code</a>.</p><p>In the event that the argument <code>fitresult</code> (returned by a preceding call to <code>fit</code>) is not sufficient for performing an update, the author can arrange for <code>fit</code> to output in its <code>cache</code> return value any additional information required, as this is also passed as an argument to the <code>update</code> method.</p><h3><a class="nav-anchor" id="Unsupervised-models-1" href="#Unsupervised-models-1">Unsupervised models</a></h3><p>TODO</p><ul><li><code>transform</code> should return a table unless <code>output_is_multivariate</code> is</li></ul><p>set <code>false</code>. Convenience method: <code>table</code> (for materializing an <code>AbstractMatrix</code>, or named tuple of vectors, as a table matching a given prototype)</p><ul><li><p>instead of <code>target_scitype_union</code> have <code>output_scitype_union</code></p></li><li><p><code>input_is_multivariate</code> and <code>input_scitype_union</code> are the same </p></li></ul><h3><a class="nav-anchor" id="Convenience-methods-1" href="#Convenience-methods-1">Convenience methods</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.int" href="#MLJBase.int"><code>MLJBase.int</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>int(x)</p><p>The positional integer of the <code>CategoricalString</code> or <code>CategoricalValue</code> <code>x</code>, in the ordering defined by the pool of <code>x</code>. The type of <code>int(x)</code> is the refrence type of <code>x</code>.</p><p>Not to be confused with <code>x.ref</code>, which is unchanged by reordering of the pool of <code>x</code>, but has the same type.</p><pre><code class="language-none">int(X::CategoricalArray)
int(W::Array{&lt;:CategoricalString})
int(W::Array{&lt;:CategoricalValue})</code></pre><p>Broadcasted versions of <code>int</code>.</p><pre><code class="language-none">julia&gt; v = categorical([:c, :b, :c, :a])
julia&gt; levels(v)
3-element Array{Symbol,1}:
 :a
 :b
 :c
julia&gt; int(v)
4-element Array{UInt32,1}:
 0x00000003
 0x00000002
 0x00000003
 0x00000001</code></pre><p>See also: decoder</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/1bbdb3dc66f5e751725cc998812488459cbc2be0/src/data.jl#L66-L96">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.classes" href="#MLJBase.classes"><code>MLJBase.classes</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">classes(x)</code></pre><p>All the categorical values in the same pool as <code>x</code> (including <code>x</code>), returned as a list, with an ordering consistent with the pool. Here <code>x</code> has <code>CategoricalValue</code> or <code>CategoricalString</code> type, and <code>classes(x)</code> is a vector of the same eltype. </p><p>Not to be confused with the levels of <code>x.pool</code> which have a different type. In particular, while <code>x in classes(x)</code> is always true, <code>x in x.pool.levels</code> is not true.</p><pre><code class="language-none">julia&gt; v = categorical([:c, :b, :c, :a])
julia&gt; levels(v)
3-element Array{Symbol,1}:
 :a
 :b
 :c
julia&gt; classes(v[4])
3-element Array{CategoricalValue{Symbol,UInt32},1}:
 :a
 :b
 :c</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/1bbdb3dc66f5e751725cc998812488459cbc2be0/src/data.jl#L34-L58">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.decoder" href="#MLJBase.decoder"><code>MLJBase.decoder</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">d = decoder(x)</code></pre><p>A callable object for decoding the integer representation of a <code>CategoricalString</code> or <code>CategoricalValue</code> sharing the same pool as <code>x</code>. (Here <code>x</code> is of one of these two types.) Specifically, one has <code>d(int(y)) == y</code> for all <code>y in classes(x)</code>. One can also call <code>d</code> on integer arrays, in which case <code>d</code> is broadcast over all elements.</p><pre><code class="language-none">julia&gt; v = categorical([:c, :b, :c, :a])
julia&gt; int(v)
4-element Array{UInt32,1}:
 0x00000003
 0x00000002
 0x00000003
 0x00000001
julia&gt; d = decoder(v[3])
julia&gt; d(int(v)) == v
true</code></pre><p>See also: int, classes</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/1bbdb3dc66f5e751725cc998812488459cbc2be0/src/data.jl#L106-L128">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.matrix" href="#MLJBase.matrix"><code>MLJBase.matrix</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">MLJBase.matrix(X)</code></pre><p>Convert a table source <code>X</code> into an <code>Matrix</code>; or, if <code>X</code> is a <code>AbstractMatrix</code>, return <code>X</code>. Optimized for column-based sources.</p><p>If instead X is a sparse table, then a <code>SparseMatrixCSC</code> object is returned. The integer relabelling of column names follows the lexicographic ordering (as indicated by <code>schema(X).names</code>).</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/1bbdb3dc66f5e751725cc998812488459cbc2be0/src/data.jl#L179-L189">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.table" href="#MLJBase.table"><code>MLJBase.table</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">MLJBase.table(cols; prototype=cols)</code></pre><p>Convert a named tuple of vectors <code>cols</code>, into a table. The table type returned is the &quot;preferred sink type&quot; for <code>prototype</code> (see the Tables.jl documentation). </p><pre><code class="language-none">MLJBase.table(X::AbstractMatrix; names=nothing, prototype=nothing)</code></pre><p>Convert an abstract matrix <code>X</code> into a table with <code>names</code> (a tuple of symbols) as column names, or with labels <code>(:x1, :x2, ..., :xn)</code> where <code>n=size(X, 2)</code>, if <code>names</code> is not specified.  If prototype=nothing, then a named tuple of vectors is returned.</p><p>Equivalent to <code>table(cols, prototype=prototype)</code> where <code>cols</code> is the named tuple of columns of <code>X</code>, with <code>keys(cols) = names</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/1bbdb3dc66f5e751725cc998812488459cbc2be0/src/data.jl#L214-L231">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.select" href="#MLJBase.select"><code>MLJBase.select</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">select(X, r, c)</code></pre><p>Select element of a table or sparse table at row <code>r</code> and column <code>c</code>. In the case of sparse data where the key <code>(r, c)</code>, zero or <code>missing</code> is returned, depending on the value type.</p><p>See also: selectrows, selectcols</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/1bbdb3dc66f5e751725cc998812488459cbc2be0/src/data.jl#L276-L285">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.selectrows" href="#MLJBase.selectrows"><code>MLJBase.selectrows</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">selectrows(X, r)</code></pre><p>Select single or multiple rows from any table, sparse table, or abstract vector <code>X</code>.  If <code>X</code> is tabular, the object returned is a table of the preferred sink type of <code>typeof(X)</code>, even a single row is selected.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/1bbdb3dc66f5e751725cc998812488459cbc2be0/src/data.jl#L251-L259">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.selectcols" href="#MLJBase.selectcols"><code>MLJBase.selectcols</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">selectcols(X, c)</code></pre><p>Select single or multiple columns from any table or sparse table <code>X</code>. If <code>c</code> is an abstract vector of integers or symbols, then the object returned is a table of the preferred sink type of <code>typeof(X)</code>. If <code>c</code> is a <em>single</em> integer or column, then a <code>Vector</code> or <code>CategoricalVector</code> is returned.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/1bbdb3dc66f5e751725cc998812488459cbc2be0/src/data.jl#L263-L272">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.schema" href="#MLJBase.schema"><code>MLJBase.schema</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">schema(X)</code></pre><p>Returns a struct with properties <code>names</code>, <code>types</code> with the obvious meanings. Here <code>X</code> is any table or sparse table.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/1bbdb3dc66f5e751725cc998812488459cbc2be0/src/data.jl#L289-L295">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.nrows" href="#MLJBase.nrows"><code>MLJBase.nrows</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">nrows(X)</code></pre><p>Return the number of rows in a table, sparse table, or abstract vector.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/1bbdb3dc66f5e751725cc998812488459cbc2be0/src/data.jl#L299-L304">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.scitype" href="#MLJBase.scitype"><code>MLJBase.scitype</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">scitype(x)</code></pre><p>Return the scientific type for scalar values that object <code>x</code> can represent. If <code>x</code> is a tuple, then <code>Tuple{scitype.(x)...}</code> is returned. </p><pre><code class="language-none">julia&gt; scitype(4.5)
Continous

julia&gt; scitype(&quot;book&quot;)
Unknown

julia&gt; scitype((1, 4.5))
Tuple{Count,Continuous}

julia&gt; using CategoricalArrays
julia&gt; v = categorical([:m, :f, :f])
julia&gt; scitype(v[1])
Multiclass{2}</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/1bbdb3dc66f5e751725cc998812488459cbc2be0/src/scitypes.jl#L18-L38">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.scitype_union" href="#MLJBase.scitype_union"><code>MLJBase.scitype_union</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">scitype_union(A)</code></pre><p>Return the type union, over all elements <code>x</code> generated by the iterable <code>A</code>, of <code>scitype(x)</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/1bbdb3dc66f5e751725cc998812488459cbc2be0/src/scitypes.jl#L50-L56">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.scitypes" href="#MLJBase.scitypes"><code>MLJBase.scitypes</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">scitypes(X)</code></pre><p>Returns a named tuple keyed on the column names of the table <code>X</code> with values the corresponding scitype unions over a column&#39;s entries.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/1bbdb3dc66f5e751725cc998812488459cbc2be0/src/scitypes.jl#L59-L65">source</a></section><h3><a class="nav-anchor" id="Where-to-place-code-implementing-new-models-1" href="#Where-to-place-code-implementing-new-models-1">Where to place code implementing new models</a></h3><p>Note that different packages can implement models having the same name without causing conflicts, although an MLJ user cannot simultaneously <em>load</em> two such models.</p><p>There are two options for making a new model implementation available to all MLJ users:</p><ol><li><p><strong>Native implementations</strong> (preferred option). The implementation code lives in the same package that contains the learning algorithms implementing the interface. In this case, it is sufficient to open an issue at <a href="https://github.com/alan-turing-institute/MLJRegistry.jl">MLJRegistry</a> requesting the package to be registered with MLJ. Registering a package allows the MLJ user to access its models&#39; metadata and to selectively load them.</p></li><li><p><strong>External implementations</strong> (short-term alternative). The model implementation code is necessarily separate from the package <code>SomePkg</code> defining the learning algorithm being wrapped. In this case, the recommended procedure is to include the implementation code at <a href="https://github.com/alan-turing-institute/MLJModels.jl/tree/master/src">MLJModels/src</a> via a pull-request, and test code at <a href="https://github.com/alan-turing-institute/MLJModels.jl/tree/master/test">MLJModels/test</a>. Assuming <code>SomePkg</code> is the only package imported by the implementation code, one needs to: (i) register <code>SomePkg</code> at MLJRegistry as explained above; and (ii) add a corresponding <code>@require</code> line in the PR to <a href="https://github.com/alan-turing-institute/MLJModels.jl/tree/master/src/MLJModels.jl">MLJModels/src/MLJModels.jl</a> to enable lazy-loading of that package by MLJ (following the pattern of existing additions). If other packages must be imported, add them to the MLJModels project file after checking they are not already there. If it is really necessary, packages can be also added to Project.toml for testing purposes.</p></li></ol><p>Additionally, one needs to ensure that the implementation code defines the <code>package_name</code> and <code>load_path</code> model traits appropriately, so that <code>MLJ</code>&#39;s <code>@load</code> macro can find the necessary code (see <a href="https://github.com/alan-turing-institute/MLJModels.jl/tree/master/src">MLJModels/src</a> for examples). The <code>@load</code> command can only be tested after registration. If changes are made, lodge an issue at <a href="https://github.com/alan-turing-institute/MLJRegistry.jl">MLJRegistry</a> to make the changes available to MLJ.  </p><footer><hr/><a class="previous" href="../simple_user_defined_models/"><span class="direction">Previous</span><span class="title">Simple User Defined Models</span></a><a class="next" href="../internals/"><span class="direction">Next</span><span class="title">Internals</span></a></footer></article></body></html>
