<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Adding Models for General Use · MLJ</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MLJ</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Getting Started</a></li><li><a class="toctext" href="../working_with_tasks/">Working with Tasks</a></li><li><a class="toctext" href="../built_in_transformers/">Built-in Transformers</a></li><li><a class="toctext" href="../learning_networks/">Learning Networks</a></li><li><a class="toctext" href="../simple_user_defined_models/">Simple User Defined Models</a></li><li class="current"><a class="toctext" href>Adding Models for General Use</a><ul class="internal"></ul></li><li><a class="toctext" href="../internals/">Internals</a></li><li><a class="toctext" href="../glossary/">Glossary</a></li><li><a class="toctext" href="../api/">API</a></li><li><a class="toctext" href="../frequently_asked_questions/">FAQ</a></li><li><a class="toctext" href="../NEWS/">MLJ News</a></li><li><a class="toctext" href="../julia_blogpost/">Julia BlogPost</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Adding Models for General Use</a></li></ul><a class="edit-page" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/adding_models_for_general_use.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Adding Models for General Use</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Adding-New-Models-1" href="#Adding-New-Models-1">Adding New Models</a></h1><p>This guide outlines in detail the specification of the MLJ model interface and provides guidelines for implementing the interface for models intended for general use. For sample implementations, see <a href="https://github.com/alan-turing-institute/MLJModels.jl/tree/master/src">MLJModels/src</a>.</p><p>The machine learning tools provided by MLJ can be applied to the models in any package that imports the package <a href="https://github.com/alan-turing-institute/MLJBase.jl">MLJBase</a> and implements the API defined there, as outlined below. For a quick-and-dirty implementation of user-defined models see <a href="../simple_user_defined_models/">Simple User Defined Models</a>.  To make new models available to all MLJ users, see <a href="#Where-to-place-code-implementing-new-models-1">Where to place code implementing new models</a>.</p><p>It is assumed the reader has read <a href="../">Getting Started</a>. To implement the API described here, some familiarity with the following packages is also helpful:</p><ul><li><p><a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a> (for probabilistic predictions)</p></li><li><p><a href="https://github.com/JuliaData/CategoricalArrays.jl">CategoricalArrays.jl</a> (essential if you are implementing a model handling data of <code>Multiclass</code> or <code>FiniteOrderedFactor</code> scitype)</p></li><li><p><a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> (if you&#39;re algorithm needs input data in a novel format).</p></li></ul><p>In MLJ, the basic interface exposed to the user, built atop the model interface described here, is the <em>machine interface</em>. After a first reading of this document, the reader may wish to refer to <a href="../internals/">MLJ Internals</a> for context.</p><h3><a class="nav-anchor" id="Overview-1" href="#Overview-1">Overview</a></h3><p>A <em>model</em> is an object storing hyperparameters associated with some machine learning algorithm.  In MLJ, hyperparameters include configuration parameters, like the number of threads, and special instructions, such as &quot;compute feature rankings&quot;, which may or may not affect the final learning outcome.  However, the logging level (<code>verbosity</code> below) is excluded.</p><p>The name of the Julia type associated with a model indicates the associated algorithm (e.g., <code>DecisionTreeClassifier</code>). The outcome of training a learning algorithm is called a <em>fit-result</em>. For ordinary multilinear regression, for example, this would be the coefficients and intercept. For a general supervised model, it is the (generally minimal) information needed to make new predictions.</p><p>The ultimate supertype of all models is <code>MLJBase.Model</code>, which has two abstract subtypes:</p><pre><code class="language-julia">abstract type Supervised{R} &lt;: Model end
abstract type Unsupervised &lt;: Model end</code></pre><p>Here the parameter <code>R</code> refers to a fit-result type. By declaring a model to be a subtype of <code>MLJBase.Supervised{R}</code> you guarantee the fit-result to be of type <code>R</code> and, if <code>R</code> is concrete, one may improve the performance of homogeneous ensembles of the model (as defined by the built-in MLJ <code>EnsembleModel</code> wrapper). There is no abstract type for fit-results because these types are generally declared outside of MLJBase.</p><blockquote><p>WIP: The necessity to declare the fitresult type <code>R</code> may disappear in the future (issue #93).</p></blockquote><p><code>Supervised</code> models are further divided according to whether they are able to furnish probabilistic predictions of the target(s) (which they will do so by default) or directly predict &quot;point&quot; estimates, for each new input pattern:</p><pre><code class="language-julia">abstract type Probabilistic{R} &lt;: Supervised{R} end
abstract type Deterministic{R} &lt;: Supervised{R} end</code></pre><p>Further division of model types is realized through <a href="#Trait-declarations-1">Trait declarations</a>.</p><p>Associated with every concrete subtype of <code>Model</code> there must be a <code>fit</code> method, which implements the associated algorithm to produce the fit-result. Additionally, every <code>Supervised</code> model has a <code>predict</code> method, while <code>Unsupervised</code> models must have a <code>transform</code> method. More generally, methods such as these, that are dispatched on a model instance and a fit-result (plus other data), are called <em>operations</em>. <code>Probabilistic</code> supervised models optionally implement a <code>predict_mode</code> operation (in the case of classifiers) or a <code>predict_mean</code> and/or <code>predict_median</code> operations (in the case of regressors) overriding obvious fallbacks provided by <code>MLJBase</code>. <code>Unsupervised</code> models may implement an <code>inverse_transform</code> operation.</p><h3><a class="nav-anchor" id="New-model-type-declarations-and-optional-clean!-method-1" href="#New-model-type-declarations-and-optional-clean!-method-1">New model type declarations and optional clean! method</a></h3><p>Here is an example of a concrete supervised model type declaration, made after defining an appropriate fit-result type (an optional step):</p><pre><code class="language-julia">import MLJ

struct LinearFitResult{F&lt;:AbstractFloat} &lt;: MLJBase.MLJType
    coefficients::Vector{F}
    bias::F
end

mutable struct RidgeRegressor{F} &lt;: MLJBase.Deterministic{LinearFitResult{F}}
    target_type::Type{F}
    lambda::Float64
end</code></pre><p><strong>Note.</strong> Model fields may be of any type except <code>NamedTuple</code>.  (This is because named tuples are used to represented the nested hyperparameters  of composite models (models that have other models as fields.)</p><p>Models (which are mutable) should not be given internal constructors. It is recommended that they be given an external lazy keyword constructor of the same name. This constructor defines default values for every field, and optionally corrects invalid field values by calling a <code>clean!</code> method (whose fallback returns an empty message string):</p><pre><code class="language-julia">function MLJ.clean!(model::RidgeRegressor)
    warning = &quot;&quot;
    if model.lambda &lt; 0
        warning *= &quot;Need lambda ≥ 0. Resetting lambda=0. &quot;
        model.lambda = 0
    end
    return warning
end

# keyword constructor
function RidgeRegressor(; target_type=Float64, lambda=0.0)

    model = RidgeRegressor(target_type, lambda)

    message = MLJBase.clean!(model)
    isempty(message) || @warn message

    return model
    
end</code></pre><h3><a class="nav-anchor" id="Supervised-models-1" href="#Supervised-models-1">Supervised models</a></h3><p>Below we describe the compulsory and optional methods to be specified for each concrete type <code>SomeSupervisedModelType{R} &lt;: MLJBase.Supervised{R}</code>. </p><h4><a class="nav-anchor" id="The-form-of-data-for-fitting-and-predicting-1" href="#The-form-of-data-for-fitting-and-predicting-1">The form of data for fitting and predicting</a></h4><p>In every circumstance, the argument <code>X</code> passed to the <code>fit</code> method described below, and the argument <code>Xnew</code> of the <code>predict</code> method, will be some table supporting the <a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> API. The interface implementer can control the scientific type of data appearing in <code>X</code> with an appropriate <code>input_scitype_union</code> declaration (see <a href="#Trait-declarations-1">Trait declarations</a>, as <code>Union{scitypes(X)...} &lt;: input_scitype_union(SomeSupervisedModel)</code> will always hold. See <a href="#Convenience-methods-1">Convenience methods</a> below for the definition of <code>scitypes</code>. If the core algorithm requires data in a different or more specific form, then <code>fit</code> will need to coerce the table into the form desired. To this end, MLJ provides the convenience method <code>MLJBase.matrix</code>; <code>MLJBase.matrix(Xtable)</code> has type <code>Matrix{T}</code> where <code>T</code> is the tightest common type of elements of <code>Xtable</code>, and <code>Xtable</code> is any table.</p><blockquote><p>Tables.jl has recently added a <code>matrix</code> method as well.</p></blockquote><p>Other convenience methods provided by MLJBase for handling tabular data are: <code>selectrows</code>, <code>selectcols</code>, <code>select</code>, <code>schema</code> (for extracting the size, names and eltypes of a table) and <code>table</code> (for materializing an abstract matrix, or named tuple of vectors, as a table matching a given prototype). See <a href="#Convenience-methods-1">Convenience methods</a> below for details.</p><p>Note that generally the same type coercions applied to <code>X</code> by <code>fit</code> will need to be applied by <code>predict</code> to <code>Xnew</code>. </p><p><strong>Important convention</strong> It is to be understood that the columns of the table <code>X</code> correspond to features and the rows to patterns.</p><p>For univariate targets, <code>y</code> is always a <code>Vector</code> or <code>CategoricalVector</code>, according to the value of the trait:</p><table><tr><th style="text-align: right"><code>target_scitype_union(SomeSupervisedModelType)</code></th><th style="text-align: right">type of <code>y</code></th><th style="text-align: right">a supertype of <code>eltype(y)</code></th></tr><tr><td style="text-align: right"><code>Continuous</code></td><td style="text-align: right"><code>Vector</code></td><td style="text-align: right"><code>Real</code></td></tr><tr><td style="text-align: right"><code>&lt;: Multiclass</code></td><td style="text-align: right"><code>CategoricalVector</code></td><td style="text-align: right"><code>Union{CategoricalString, CategoricalValue}</code></td></tr><tr><td style="text-align: right"><code>&lt;: FiniteOrderedFactor</code></td><td style="text-align: right"><code>CategoricalVector</code></td><td style="text-align: right"><code>Union{CategoricalString, CategoricalValue}</code></td></tr><tr><td style="text-align: right"><code>Count</code></td><td style="text-align: right"><code>Vector</code></td><td style="text-align: right"><code>Integer</code></td></tr></table><p>The form of the target data <code>y</code> passed to <code>fit</code> is constrained by the <code>target_scitype_union</code> trait declaration as <code>scitype_union(y) &lt;: target_scitype_union(SomeSupervisedModelType)</code> will always hold. See See <a href="#Convenience-methods-1">Convenience methods</a> for the definition of <code>scitype_union</code>.</p><p>So, for example, if your model is a binary classifier, you declare</p><pre><code class="language-julia">target_scitype_union(SomeSupervisedModelType)=Multiclass{2}</code></pre><p>If it can predict any number of classes, you might instead declare</p><pre><code class="language-julia">target_scitype_union(SomeSupervisedModelType)=Union{Multiclass, FiniteOrderedFactor}</code></pre><p>See also the table in <a href="../">Getting Started</a>.</p><p>In the case of a multivariate target, in which case <code>y</code> is a vector of tuples, the same constraint <code>scitype_union(y) &lt;: target_scitype_union(SomeSupervisedModelType)</code> holds. For example, if you declare <code>target_scitype_union(SomeSupervisedModelType) = Tuple{Continuous,Count}</code>, then each element of <code>y</code> will be a tuple of type <code>Tuple{Real,Integer}</code>.</p><h4><a class="nav-anchor" id="The-fit-method-1" href="#The-fit-method-1">The fit method</a></h4><p>A compulsory <code>fit</code> method returns three objects:</p><pre><code class="language-julia">MLJBase.fit(model::SomeSupervisedModelType, verbosity::Int, X, y) -&gt; fitresult, cache, report</code></pre><p>Note: The <code>Int</code> typing of <code>verbosity</code> cannot be omitted.</p><ol><li><p><code>fitresult::R</code> is the fit-result in the sense above (which becomes an  argument for <code>predict</code> discussed below).</p></li><li><p><code>report</code> is a (possibly empty) <code>NamedTuple</code>, for example,  <code>report=(deviance=..., dof_residual=..., stderror=..., vcov=...)</code>.  Any training-related statistics, such as internal estimates of the  generalization error, and feature rankings, should be returned in  the <code>report</code> tuple. How, or if, these are generated should be  controlled by hyperparameters (the fields of <code>model</code>). Fitted  parameters, such as the coefficients of a linear model, do not go  in the report as they will be extractable from <code>fitresult</code> (and  accessible to MLJ through the <code>fitted_params</code> method, see below).</p></li></ol><p>3.	The value of <code>cache</code> can be <code>nothing</code>, unless one is also defining       an <code>update</code> method (see below). The Julia type of <code>cache</code> is not       presently restricted.</p><p>It is not necessary for <code>fit</code> to provide dimension checks or to call <code>clean!</code> on the model; MLJ will carry out such checks.</p><p>The method <code>fit</code> should never alter hyperparameter values. If the package is able to suggest better hyperparameters, as a byproduct of training, return these in the report field.</p><p>One should test that actual fit-results have the type declared in the model <code>mutable struct</code> declaration. To help with this, <code>MLJBase.fitresult_type(m)</code> returns the declared type, for any supervised model (or model type) <code>m</code>.</p><p>The <code>verbosity</code> level (0 for silent) is for passing to learning algorithm itself. A <code>fit</code> method wrapping such an algorithm should generally avoid doing any of its own logging.</p><h4><a class="nav-anchor" id="The-fitted_params-method-1" href="#The-fitted_params-method-1">The fitted_params method</a></h4><p>A <code>fitted_params</code> method may be optionally overloaded. It&#39;s purpose is to provide MLJ accesss to a user-friendly representation of the learned parameters of the model (as opposed to the hyperparameters). They must be extractable from <code>fitresult</code>.</p><pre><code class="language-julia">MLJBase.fitted_params(model::SomeSupervisedModelType, fitresult) -&gt; friendly_fitresult::NamedTuple</code></pre><p>For a linear model, for example, one might declare something like <code>friendly_fitresult=(coefs=[...], bias=...)</code>.</p><p>The fallback is to return <code>(fitresult=fitresult,)</code>.</p><h4><a class="nav-anchor" id="The-predict-method-1" href="#The-predict-method-1">The predict method</a></h4><p>A compulsory <code>predict</code> method has the form</p><pre><code class="language-julia">MLJBase.predict(model::SomeSupervisedModelType, fitresult, Xnew) -&gt; yhat</code></pre><p>Here <code>Xnew</code> is an any table whose entries satisfy the same scitype constraints as discussed for <code>X</code> above.</p><p><strong>Prediction types for deterministic responses.</strong> In the case of <code>Deterministic</code> models, <code>yhat</code> must have the same form as the target <code>y</code> passed to the <code>fit</code> method (see above discussion on the form of data for fitting), with one exception: If predicting a <code>Count</code>, the prediction may be <code>Continuous</code>. For all models predicting a <code>Multiclass</code> or <code>FiniteOrderedFactor</code>, the categorical vectors returned by <code>predict</code> <strong>must have the levels in the categorical pool of the target data presented in training</strong>, even if not all levels appear in the training data or prediction itself. That is, we must have <code>levels(yhat) == levels(y)</code>.</p><p>Unfortunately, code not written with the preservation of categorical levels in mind poses special problems. To help with this, MLJ provides a utility <code>CategoricalDecoder</code> which can decode a <code>CategoricalArray</code> into a plain array, and re-encode a prediction with the original levels intact. The <code>CategoricalDecoder</code> object created during <code>fit</code> will need to be bundled with <code>fitresult</code> to make it available to <code>predict</code> during re-encoding. </p><p>So, for example, if the core algorithm being wrapped by <code>fit</code> expects a nominal target <code>yint</code> of type <code>Vector{Int64}</code> then a <code>fit</code> method may look something like this:</p><pre><code class="language-julia">function MLJBase.fit(model::SomeSupervisedModelType, verbosity, X, y)
    decoder = MLJBase.CategoricalDecoder(y, Int64)
    yint = transform(decoder, y)
    core_fitresult = SomePackage.fit(X, yint, verbosity=verbosity)
    fitresult = (decoder, core_fitresult)
    cache = nothing
    report = nothing
    return fitresult, cache, report
end</code></pre><p>while a corresponding deterministic <code>predict</code> operation might look like this:</p><pre><code class="language-julia">function MLJBase.predict(model::SomeSupervisedModelType, fitresult, Xnew)
    decoder, core_fitresult = fitresult
    yhat = SomePackage.predict(core_fitresult, Xnew)
    return inverse_transform(decoder, yhat)
end</code></pre><p>Query <code>?MLJBase.DecodeCategorical</code> for more information.</p><p>If you are coding a learning algorithm from scratch, rather than wrapping an existing one, conversions may be unnecessary. It may suffice to record the pool of <code>y</code> and bundle that with the fitresult for <code>predict</code> to append to the levels of its categorical output.</p><p><strong>Prediction types for probabilistic responses.</strong> In the case of <code>Probabilistic</code> models with univariate targets, <code>yhat</code> must be a <code>Vector</code> whose elements are distributions (one distribution per row of <code>Xnew</code>).</p><p>A <em>distribution</em> is any instance of a subtype of <code>Distributions.Distribution</code> from the package Distributions.jl, or any instance of the additional types <code>UnivariateNominal</code> and <code>MultivariateNominal</code> defined in MLJBase.jl (or any other type <code>D</code> you define for which <code>MLJBase.isdistribution(::D) = true</code>, meaning <code>Base.rand</code> and <code>Distributions.pdf</code> are implemented, as well <code>Distributions.mean</code>/<code>Distribution.median</code> or <code>Distributions.mode</code>).</p><p>Use <code>UnivariateNominal</code> for <code>Probabilistic</code> models predicting <code>Multiclass</code> or <code>FiniteOrderedFactor</code> targets. For example, suppose <code>levels(y)=[&quot;yes&quot;, &quot;no&quot;, &quot;maybe&quot;]</code> and set <code>L=levels(y)</code>. Then, if the predicted probabilities for some input pattern are <code>[0.1, 0.7, 0.2]</code>, respectively, then the prediction returned for that pattern will be <code>UnivariateNominal(L, [0.1, 0.7, 0.2])</code>. Query <code>?UnivariateNominal</code> for more information.</p><p>The <code>predict</code> method will need access to all levels in the pool of the target variable presented <code>y</code> presented for training, which consequently need to be encoded in the <code>fitresult</code> returned by <code>fit</code>. If a <code>CategoricalDecoder</code> object, <code>decoder</code>, has been bundled in <code>fitresult</code>, as in the deterministic example above, then the levels are given by <code>levels(decoder)</code>. Levels not observed in the training data  (i.e., only in its pool) should be assigned probability zero.</p><h4><a class="nav-anchor" id="Trait-declarations-1" href="#Trait-declarations-1">Trait declarations</a></h4><p>There are a number of recommended trait declarations for each model mutable structure <code>SomeSupervisedModelType &lt;: Supervised</code> you define. Basic fitting, resampling and tuning in MLJ does not require these traits but some advanced MLJ meta-algorithms may require them now, or in the future. In particular, MLJ&#39;s <code>models(::Task)</code> method (matching models to user-specified tasks) can only identify models having a complete set of trait declarations. A full set of declarations is shown below for the <code>DecisionTreeClassifier</code> type (defined in the submodule DecisionTree_ of MLJModels):</p><pre><code class="language-julia">MLJBase.load_path(::Type{&lt;:DecisionTreeClassifier}) = &quot;MLJModels.DecisionTree_.DecisionTreeClassifier&quot; 
MLJBase.package_name(::Type{&lt;:DecisionTreeClassifier}) = &quot;DecisionTree&quot;
MLJBase.package_uuid(::Type{&lt;:DecisionTreeClassifier}) = &quot;7806a523-6efd-50cb-b5f6-3fa6f1930dbb&quot;
MLJBase.package_url(::Type{&lt;:DecisionTreeClassifier}) = &quot;https://github.com/bensadeghi/DecisionTree.jl&quot;
MLJBase.is_pure_julia(::Type{&lt;:DecisionTreeClassifier}) = true
MLJBase.input_is_multivariate(::Type{&lt;:DecisionTreeClassifier}) = true
MLJBase.input_scitype_union(::Type{&lt;:DecisionTreeClassifier}) = MLJBase.Continuous
MLJBase.target_scitype_union(::Type{&lt;:DecisionTreeClassifier}) = MLJBase.Multiclass</code></pre><p>Note that models predicting multivariate targets will need to need to have <code>target_scitype_union</code> return an appropriate <code>Tuple</code> type. </p><p>For an explanation of <code>Found</code> and <code>Other</code> in the table below, see <a href="../">Scientific Types</a>.</p><table><tr><th style="text-align: right">method</th><th style="text-align: right">return type</th><th style="text-align: right">declarable return values</th><th style="text-align: right">default value</th></tr><tr><td style="text-align: right"><code>target_scitype_union</code></td><td style="text-align: right"><code>DataType</code></td><td style="text-align: right">subtype of <code>Found</code> or tuple of such types</td><td style="text-align: right"><code>Union{Found,NTuple{&lt;:Found}}</code></td></tr><tr><td style="text-align: right"><code>input_scitype_union</code></td><td style="text-align: right"><code>DataType</code></td><td style="text-align: right">subtype of <code>Union{Missing,Found}</code></td><td style="text-align: right"><code>Union{Missing,Found}</code></td></tr><tr><td style="text-align: right"><code>input_is_multivariate</code></td><td style="text-align: right"><code>Bool</code></td><td style="text-align: right"><code>true</code> or <code>false</code></td><td style="text-align: right"><code>true</code></td></tr><tr><td style="text-align: right"><code>is_pure_julia</code></td><td style="text-align: right"><code>Bool</code></td><td style="text-align: right"><code>true</code> or <code>false</code></td><td style="text-align: right"><code>false</code></td></tr><tr><td style="text-align: right"><code>load_path</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr><tr><td style="text-align: right"><code>package_name</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr><tr><td style="text-align: right"><code>package_uuid</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr><tr><td style="text-align: right"><code>package_url</code></td><td style="text-align: right"><code>String</code></td><td style="text-align: right">unrestricted</td><td style="text-align: right">&quot;unknown&quot;</td></tr></table><p>You can test declarations of traits by calling <code>info(SomeModelType)</code>.</p><h4><a class="nav-anchor" id="The-update!-method-1" href="#The-update!-method-1">The update! method</a></h4><p>An <code>update</code> method may be optionally overloaded to enable a call by MLJ to retrain a model (on the same training data) to avoid repeating computations unnecessarily.</p><pre><code class="language-julia">MLJBase.update(model::SomeSupervisedModelType, verbosity, old_fitresult, old_cache, X, y) -&gt; fitresult, cache, report</code></pre><p>If an MLJ <code>Machine</code> is being <code>fit!</code> and it is not the first time, then <code>update</code> is called instead of <code>fit</code> unless <code>fit!</code> has been called with new rows. However, <code>MLJBase</code> defines a fallback for <code>update</code> which just calls <code>fit</code>. For context, see <a href="../internals/">MLJ Internals</a>. </p><p>Learning networks wrapped as models constitute one use-case: One would like each component model to be retrained only when hyperparameter changes &quot;upstream&quot; make this necessary. In this case MLJ provides a fallback (specifically, the fallback is for any subtype of <code>Supervised{Node}</code>). A second important use-case is iterative models, where calls to increase the number of iterations only restarts the iterative procedure if other hyperparameters have also changed. For an example, see <code>builtins/Ensembles.jl</code>.</p><p>In the event that the argument <code>fitresult</code> (returned by a preceding call to <code>fit</code>) is not sufficient for performing an update, the author can arrange for <code>fit</code> to output in its <code>cache</code> return value any additional information required, as this is also passed as an argument to the <code>update</code> method.</p><h4><a class="nav-anchor" id="Multivariate-models-1" href="#Multivariate-models-1">Multivariate models</a></h4><p>TODO</p><h3><a class="nav-anchor" id="Unsupervised-models-1" href="#Unsupervised-models-1">Unsupervised models</a></h3><p>TODO</p><h3><a class="nav-anchor" id="Convenience-methods-1" href="#Convenience-methods-1">Convenience methods</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.CategoricalDecoder" href="#MLJBase.CategoricalDecoder"><code>MLJBase.CategoricalDecoder</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-julia">CategoricalDecoder(C::CategoricalArray)
CategoricalDecoder(C::CategoricalArray, eltype, start_at_zero=false)</code></pre><p>Construct a decoder for transforming a <code>CategoricalArray{T}</code> object into an ordinary array, and for re-encoding similar arrays back into a <code>CategoricalArray{T}</code> object having the same <code>pool</code> (and, in particular, the same levels) as <code>C</code>. If <code>eltype</code> is not specified then the element type of the transformed array is <code>T</code>. Otherwise, the element type is <code>eltype</code> and the elements are conversions to <code>eltype</code> of the internal (unsigned integer) <code>ref</code>s of the <code>CategoricalArray</code>, shifted backwards by one if <code>start_at_zero=false</code>. One must have <code>eltype &lt;: Real</code>.</p><p>If <code>eltype = Bool</code>, then <code>start_at_zero</code> is ignored.</p><pre><code class="language-none">transform(decoder::CategoricalDecoder, C::CategoricalArray)</code></pre><p>Transform <code>C</code> into an ordinary <code>Array</code>.</p><pre><code class="language-none">inverse_transform(decoder::CategoricalDecoder, A::Array)</code></pre><p>Transform an array <code>A</code> suitably compatible with <code>decoder</code> into a <code>CategoricalArray</code> having the same <code>pool</code> as <code>C</code>.</p><pre><code class="language-none">levels(decoder::CategoricalDecoder)
levels_seen(decoder::CategoricaDecoder)</code></pre><p>Return, respectively, all levels in pool of the categorical vector <code>C</code> used to construct <code>decoder</code> (ie, <code>levels(C)</code>), and just those levels explicitly appearing as entries of <code>C</code> (ie, <code>unique(C)</code>).</p><p><strong>Example</strong></p><pre><code class="language-none">julia&gt; using CategoricalArrays
julia&gt; C = categorical([&quot;a&quot; &quot;b&quot;; &quot;a&quot; &quot;c&quot;])
2×2 CategoricalArray{String,2,UInt32}:
 &quot;a&quot;  &quot;b&quot;
 &quot;a&quot;  &quot;c&quot;

julia&gt; decoder = MLJBase.CategoricalDecoder(C, eltype=Float64);
julia&gt; A = transform(decoder, C)
2×2 Array{Float64,2}:
 1.0  2.0
 1.0  3.0

julia&gt; inverse_transform(decoder, A[1:1,:])
1×2 CategoricalArray{String,2,UInt32}:
 &quot;a&quot;  &quot;b&quot;

julia&gt; levels(ans)
3-element Array{String,1}:
 &quot;a&quot;
 &quot;b&quot;
 &quot;c&quot;</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/4fdd13ccf04e2e2e20b93a0054cb08b12e9c4062/src/data.jl#L31-L89">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.matrix" href="#MLJBase.matrix"><code>MLJBase.matrix</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">MLJBase.matrix(X)</code></pre><p>Convert a table source <code>X</code> into an <code>Matrix</code>; or, if <code>X</code> is a <code>AbstractMatrix</code>, return <code>X</code>. Optimized for column-based sources.</p><p>If instead X is a sparse table, then a <code>SparseMatrixCSC</code> object is returned. The integer relabelling of column names follows the lexicographic ordering (as indicated by <code>schema(X).names</code>).</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/4fdd13ccf04e2e2e20b93a0054cb08b12e9c4062/src/data.jl#L195-L205">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.table" href="#MLJBase.table"><code>MLJBase.table</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">MLJBase.table(cols; prototype=cols)</code></pre><p>Convert a named tuple of vectors <code>cols</code>, into a table. The table type returned is the &quot;preferred sink type&quot; for <code>prototype</code> (see the Tables.jl documentation). </p><pre><code class="language-none">MLJBase.table(X::AbstractMatrix; names=nothing, prototype=nothing)</code></pre><p>Convert an abstract matrix <code>X</code> into a table with <code>names</code> (a tuple of symbols) as column names, or with labels <code>(:x1, :x2, ..., :xn)</code> where <code>n=size(X, 2)</code>, if <code>names</code> is not specified.  If prototype=nothing, then a named tuple of vectors is returned.</p><p>Equivalent to <code>table(cols, prototype=prototype)</code> where <code>cols</code> is the named tuple of columns of <code>X</code>, with <code>keys(cols) = names</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/4fdd13ccf04e2e2e20b93a0054cb08b12e9c4062/src/data.jl#L228-L245">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.select" href="#MLJBase.select"><code>MLJBase.select</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">select(X, r, c)</code></pre><p>Select element of a table or sparse table at row <code>r</code> and column <code>c</code>. In the case of sparse data where the key <code>(r, c)</code>, zero or <code>missing</code> is returned, depending on the value type.</p><p>See also: selectrows, selectcols</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/4fdd13ccf04e2e2e20b93a0054cb08b12e9c4062/src/data.jl#L290-L299">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.selectrows" href="#MLJBase.selectrows"><code>MLJBase.selectrows</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">selectrows(X, r)</code></pre><p>Select single or multiple rows from any table, sparse table, or abstract vector <code>X</code>.  If <code>X</code> is tabular, the object returned is a table of the preferred sink type of <code>typeof(X)</code>, even a single row is selected.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/4fdd13ccf04e2e2e20b93a0054cb08b12e9c4062/src/data.jl#L265-L273">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.selectcols" href="#MLJBase.selectcols"><code>MLJBase.selectcols</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">selectcols(X, c)</code></pre><p>Select single or multiple columns from any table or sparse table <code>X</code>. If <code>c</code> is an abstract vector of integers or symbols, then the object returned is a table of the preferred sink type of <code>typeof(X)</code>. If <code>c</code> is a <em>single</em> integer or column, then a <code>Vector</code> or <code>CategoricalVector</code> is returned.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/4fdd13ccf04e2e2e20b93a0054cb08b12e9c4062/src/data.jl#L277-L286">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.schema" href="#MLJBase.schema"><code>MLJBase.schema</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">schema(X)</code></pre><p>Returns a struct with properties <code>names</code>, <code>types</code> with the obvious meanings. Here <code>X</code> is any table or sparse table.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/4fdd13ccf04e2e2e20b93a0054cb08b12e9c4062/src/data.jl#L303-L309">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.nrows" href="#MLJBase.nrows"><code>MLJBase.nrows</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">nrows(X)</code></pre><p>Return the number of rows in a table, sparse table, or abstract vector.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/4fdd13ccf04e2e2e20b93a0054cb08b12e9c4062/src/data.jl#L313-L318">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.scitype" href="#MLJBase.scitype"><code>MLJBase.scitype</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">scitype(x)</code></pre><p>Return the scientific type for scalar values that object <code>x</code> can represent. If <code>x</code> is a tuple, then <code>Tuple{scitype.(x)...}</code> is returned. </p><pre><code class="language-none">julia&gt; scitype(4.5)
Continous

julia&gt; scitype(&quot;book&quot;)
Unknown

julia&gt; scitype((1, 4.5))
Tuple{Count,Continuous}

julia&gt; using CategoricalArrays
julia&gt; v = categorical([:m, :f, :f])
julia&gt; scitype(v[1])
Multiclass{2}</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/4fdd13ccf04e2e2e20b93a0054cb08b12e9c4062/src/scitypes.jl#L18-L38">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.scitype_union" href="#MLJBase.scitype_union"><code>MLJBase.scitype_union</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">scitype_union(A)</code></pre><p>Return the type union, over all elements <code>x</code> generated by the iterable <code>A</code>, of <code>scitype(x)</code>.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/4fdd13ccf04e2e2e20b93a0054cb08b12e9c4062/src/scitypes.jl#L50-L56">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="MLJBase.scitypes" href="#MLJBase.scitypes"><code>MLJBase.scitypes</code></a> — <span class="docstring-category">Function</span>.</div><div><div><pre><code class="language-julia">scitypes(X)</code></pre><p>Returns a named tuple keyed on the column names of the table <code>X</code> with values the corresponding scitype unions over a column&#39;s entries.</p></div></div><a class="source-link" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/4fdd13ccf04e2e2e20b93a0054cb08b12e9c4062/src/scitypes.jl#L59-L65">source</a></section><h3><a class="nav-anchor" id="Where-to-place-code-implementing-new-models-1" href="#Where-to-place-code-implementing-new-models-1">Where to place code implementing new models</a></h3><p>Note that different packages can implement models having the same name without causing conflicts, although an MLJ user cannot simultaneously <em>load</em> two such models.</p><p>There are two options for making a new model implementation available to all MLJ users:</p><ol><li><p><strong>Native implementations</strong> (preferred option). The implementation code lives in the same package that contains the learning algorithms implementing the interface. In this case, it is sufficient to open an issue at <a href="https://github.com/alan-turing-institute/MLJRegistry.jl">MLJRegistry</a> requesting the package to be registered with MLJ. Registering a package allows the MLJ user to access its models&#39; metadata and to selectively load them.</p></li><li><p><strong>External implementations</strong> (short-term alternative). The model implementation code is necessarily separate from the package <code>SomePkg</code> defining the learning algorithm being wrapped. In this case, the recommended procedure is to include the implementation code at <a href="https://github.com/alan-turing-institute/MLJModels.jl/tree/master/src">MLJModels/src</a> via a pull-request, and test code at <a href="https://github.com/alan-turing-institute/MLJModels.jl/tree/master/test">MLJModels/test</a>. Assuming <code>SomePkg</code> is the only package imported by the implementation code, one needs to: (i) register <code>SomePkg</code> at MLJRegistry as explained above; and (ii) add a corresponding <code>@require</code> line in the PR to <a href="https://github.com/alan-turing-institute/MLJModels.jl/tree/master/src/MLJModels.jl">MLJModels/src/MLJModels.jl</a> to enable lazy-loading of that package by MLJ (following the pattern of existing additions). If other packages must be imported, add them to the MLJModels project file after checking they are not already there. If it is really necessary, packages can be also added to Project.toml for testing purposes.</p></li></ol><p>Additionally, one needs to ensure that the implementation code defines the <code>package_name</code> and <code>load_path</code> model traits appropriately, so that <code>MLJ</code>&#39;s <code>@load</code> macro can find the necessary code (see <a href="https://github.com/alan-turing-institute/MLJModels.jl/tree/master/src">MLJModels/src</a> for examples). The <code>@load</code> command can only be tested after registration. If changes are made, lodge an issue at <a href="https://github.com/alan-turing-institute/MLJRegistry.jl">MLJRegistry</a> to make the changes available to MLJ.  </p><footer><hr/><a class="previous" href="../simple_user_defined_models/"><span class="direction">Previous</span><span class="title">Simple User Defined Models</span></a><a class="next" href="../internals/"><span class="direction">Next</span><span class="title">Internals</span></a></footer></article></body></html>
