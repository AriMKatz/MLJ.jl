<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Getting Started · MLJ</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>MLJ</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li class="current"><a class="toctext" href>Getting Started</a><ul class="internal"></ul></li><li><a class="toctext" href="common_mlj_workflows/">Common MLJ Workflows</a></li><li><a class="toctext" href="model_search/">Model Search</a></li><li><a class="toctext" href="machines/">Machines</a></li><li><a class="toctext" href="evaluating_model_performance/">Evaluating Model Performance</a></li><li><a class="toctext" href="performance_measures/">Performance Measures</a></li><li><a class="toctext" href="tuning_models/">Tuning Models</a></li><li><a class="toctext" href="built_in_transformers/">Built-in Transformers</a></li><li><a class="toctext" href="composing_models/">Composing Models</a></li><li><a class="toctext" href="homogeneous_ensembles/">Homogeneous Ensembles</a></li><li><a class="toctext" href="simple_user_defined_models/">Simple User Defined Models</a></li><li><a class="toctext" href="adding_models_for_general_use/">Adding Models for General Use</a></li><li><a class="toctext" href="benchmarking/">Benchmarking</a></li><li><a class="toctext" href="working_with_tasks/">Working with Tasks</a></li><li><a class="toctext" href="internals/">Internals</a></li><li><a class="toctext" href="glossary/">Glossary</a></li><li><a class="toctext" href="api/">API</a></li><li><a class="toctext" href="mlj_cheatsheet/">MLJ Cheatsheet</a></li><li><a class="toctext" href="NEWS/">MLJ News</a></li><li><a class="toctext" href="frequently_asked_questions/">FAQ</a></li><li><a class="toctext" href="julia_blogpost/">Julia BlogPost</a></li><li><a class="toctext" href="acceleration_and_parallelism/">Acceleration and Parallelism</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Getting Started</a></li></ul><a class="edit-page" href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/docs/src/index.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Getting Started</span><a class="fa fa-bars" href="#"></a></div></header><h3><a class="nav-anchor" id="[Installation](https://github.com/alan-turing-institute/MLJ.jl/blob/master/README.md)-[Cheatsheet](mlj_cheatsheet.md)-[Workflows](common_mlj_workflows.md)-1" href="#[Installation](https://github.com/alan-turing-institute/MLJ.jl/blob/master/README.md)-[Cheatsheet](mlj_cheatsheet.md)-[Workflows](common_mlj_workflows.md)-1"><a href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/README.md">Installation</a> | <a href="mlj_cheatsheet/">Cheatsheet</a> | <a href="common_mlj_workflows/">Workflows</a></a></h3><h1><a class="nav-anchor" id="Getting-Started-1" href="#Getting-Started-1">Getting Started</a></h1><div></div><h3><a class="nav-anchor" id="Choosing-and-evaluating-a-model-1" href="#Choosing-and-evaluating-a-model-1">Choosing and evaluating a model</a></h3><p>To load some demonstration data, add <a href="https://github.com/JuliaStats/RDatasets.jl">RDatasets</a> to your load path and enter</p><pre><code class="language-julia-repl">julia&gt; using RDatasets

julia&gt; iris = dataset(&quot;datasets&quot;, &quot;iris&quot;); # a DataFrame</code></pre><p>and then split the data into input and target parts:</p><pre><code class="language-julia-repl">julia&gt; using MLJ

julia&gt; y, X = unpack(iris, ==(:Species), colname -&gt; true);

julia&gt; first(X, 3) |&gt; pretty
┌─────────────┬────────────┬─────────────┬────────────┐
│ SepalLength │ SepalWidth │ PetalLength │ PetalWidth │
│ Float64     │ Float64    │ Float64     │ Float64    │
│ Continuous  │ Continuous │ Continuous  │ Continuous │
├─────────────┼────────────┼─────────────┼────────────┤
│ 5.1         │ 3.5        │ 1.4         │ 0.2        │
│ 4.9         │ 3.0        │ 1.4         │ 0.2        │
│ 4.7         │ 3.2        │ 1.3         │ 0.2        │
└─────────────┴────────────┴─────────────┴────────────┘</code></pre><p>To list all models available in MLJ&#39;s <a href="model_search/">model registry</a>:</p><pre><code class="language-julia-repl">julia&gt; models()
107-element Array{NamedTuple,1}:
 (name = ARDRegressor, package_name = ScikitLearn, ... )
 (name = AdaBoostClassifier, package_name = ScikitLearn, ... )
 (name = AdaBoostRegressor, package_name = ScikitLearn, ... )
 (name = BaggingClassifier, package_name = ScikitLearn, ... )
 (name = BaggingRegressor, package_name = ScikitLearn, ... )
 (name = BayesianLDA, package_name = MultivariateStats, ... )
 (name = BayesianRidgeRegressor, package_name = ScikitLearn, ... )
 (name = BernoulliNBClassifier, package_name = ScikitLearn, ... )
 (name = ComplementNBClassifier, package_name = ScikitLearn, ... )
 (name = ConstantClassifier, package_name = MLJModels, ... )
 ⋮
 (name = Standardizer, package_name = MLJModels, ... )
 (name = StaticTransformer, package_name = MLJModels, ... )
 (name = TheilSenRegressor, package_name = ScikitLearn, ... )
 (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... )
 (name = UnivariateDiscretizer, package_name = MLJModels, ... )
 (name = UnivariateStandardizer, package_name = MLJModels, ... )
 (name = XGBoostClassifier, package_name = XGBoost, ... )
 (name = XGBoostCount, package_name = XGBoost, ... )
 (name = XGBoostRegressor, package_name = XGBoost, ... )</code></pre><p>In MLJ a <em>model</em> is a struct storing the hyperparameters of the learning algorithm indicated by the struct name.  </p><p>Assuming the DecisionTree.jl package is in your load path, we can use <code>@load</code> to load the code defining the <code>DecisionTreeClassifier</code> model type. This macro also returns an instance, with default hyperparameters. </p><p>Drop the <code>verbosity=1</code> declaration for silent loading:</p><pre><code class="language-julia-repl">julia&gt; tree_model = @load DecisionTreeClassifier verbosity=1
import MLJModels ✔
import DecisionTree ✔
import MLJModels.DecisionTree_.DecisionTreeClassifier ✔
MLJModels.DecisionTree_.DecisionTreeClassifier(pruning_purity = 1.0,
                                               max_depth = -1,
                                               min_samples_leaf = 1,
                                               min_samples_split = 2,
                                               min_purity_increase = 0.0,
                                               n_subfeatures = 0,
                                               display_depth = 5,
                                               post_prune = false,
                                               merge_purity_threshold = 0.9,
                                               pdf_smoothing = 0.05,) @ 5…65</code></pre><p><em>Important:</em> DecisionTree.jl and most other packages implementing machine learning algorithms for use in MLJ are not MLJ dependencies. If such a package is not in your load path you will receive an error explaining how to add the package to your current environment.</p><p>Once loaded, a model can be evaluated with the <code>evaluate</code> method:</p><pre><code class="language-julia-repl">julia&gt; evaluate(tree_model, X, y,
                resampling=CV(shuffle=true), measure=cross_entropy, verbosity=0)
(measure = MLJBase.CrossEntropy[cross_entropy],
 measurement = [4.017951863385837],
 per_fold = Array{Float64,1}[[4.127134385045093, 4.127134385045093, 3.963360602556209, 3.963360602556209, 3.963360602556209, 3.963360602556209]],
 per_observation = Array{Array{Float64,1},1}[[[4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092  …  4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092], [4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092  …  4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092], [4.127134385045092, 4.127134385045092, 0.032789822822990956, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092  …  4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092], [4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092  …  4.127134385045092, 4.127134385045092, 0.032789822822990956, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092], [4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092  …  4.127134385045092, 4.127134385045092, 0.032789822822990956, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092], [4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092  …  4.127134385045092, 0.032789822822990956, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092]]],)</code></pre><p>Evaluating against multiple performance measures is also possible. See <a href="evaluating_model_performance/">Evaluating Model Performance</a> for details.</p><h3><a class="nav-anchor" id="Fit-and-predict-1" href="#Fit-and-predict-1">Fit and predict</a></h3><p>To illustrate MLJ&#39;s fit and predict interface, let&#39;s perform the above evaluations by hand.</p><p>Wrapping the model in data creates a <em>machine</em> which will store training outcomes:</p><pre><code class="language-julia-repl">julia&gt; tree = machine(tree_model, X, y)
Machine{DecisionTreeClassifier} @ 1…40</code></pre><p>Training and testing on a hold-out set:</p><pre><code class="language-julia-repl">julia&gt; train, test = partition(eachindex(y), 0.7, shuffle=true); # 70:30 split

julia&gt; fit!(tree, rows=train);
[ Info: Training Machine{DecisionTreeClassifier} @ 1…40.

julia&gt; yhat = predict(tree, X[test,:]);

julia&gt; yhat[3:5]
3-element Array{UnivariateFinite{String,UInt8,Float64},1}:
 UnivariateFinite(setosa=&gt;0.0161, versicolor=&gt;0.0161, virginica=&gt;0.968)
 UnivariateFinite(setosa=&gt;0.968, versicolor=&gt;0.0161, virginica=&gt;0.0161)
 UnivariateFinite(setosa=&gt;0.0161, versicolor=&gt;0.0161, virginica=&gt;0.968)

julia&gt; cross_entropy(yhat, y[test]) |&gt; mean
2.5803819948722984</code></pre><p>Notice that <code>yhat</code> is a vector of <code>Distribution</code> objects (because DecisionTreeClassifier makes probabilistic predictions). The methods of the <a href="https://github.com/JuliaStats/Distributions.jl">Distributions</a> package can be applied to such distributions:</p><pre><code class="language-julia-repl">julia&gt; broadcast(pdf, yhat[3:5], &quot;virginica&quot;) # predicted probabilities of virginica
3-element Array{Float64,1}:
 0.9677419354838709
 0.01612903225806452
 0.9677419354838709

julia&gt; mode.(yhat[3:5])
3-element CategoricalArrays.CategoricalArray{String,1,UInt8}:
 &quot;virginica&quot;
 &quot;setosa&quot;
 &quot;virginica&quot;</code></pre><p>Or, one can explicitly get modes by using <code>predict_mode</code> instead of <code>predict</code>:</p><pre><code class="language-julia-repl">julia&gt; predict_mode(tree, rows=test[3:5])
3-element CategoricalArrays.CategoricalArray{String,1,UInt8}:
 &quot;virginica&quot;
 &quot;setosa&quot;
 &quot;virginica&quot;</code></pre><p>Unsupervised models have a <code>transform</code> method instead of <code>predict</code>, and may optionally implement an <code>inverse_transform</code> method:</p><pre><code class="language-julia-repl">julia&gt; v = [1, 2, 3, 4]
4-element Array{Int64,1}:
 1
 2
 3
 4

julia&gt; stand_model = UnivariateStandardizer()
UnivariateStandardizer() @ 1…23

julia&gt; stand = machine(stand_model, v)
Machine{UnivariateStandardizer} @ 1…38

julia&gt; fit!(stand)
[ Info: Training Machine{UnivariateStandardizer} @ 1…38.
Machine{UnivariateStandardizer} @ 1…38

julia&gt; w = transform(stand, v)
4-element Array{Float64,1}:
 -1.161895003862225
 -0.3872983346207417
  0.3872983346207417
  1.161895003862225

julia&gt; inverse_transform(stand, w)
4-element Array{Float64,1}:
 1.0
 2.0
 3.0
 4.0</code></pre><p><a href="machines/">Machines</a> have an internal state which allows them to avoid redundant calculations when retrained, in certain conditions - for example when increasing the number of trees in a random forest, or the number of epochs in a neural network. The machine building syntax also anticipates a more general syntax for composing multiple models, as explained in <a href="composing_models/">Composing Models</a>.</p><p>There is a version of <code>evaluate</code> for machines as well as models. An exclamation point is added to the method name because machines are generally mutated when trained:</p><pre><code class="language-julia-repl">julia&gt; evaluate!(tree, resampling=Holdout(fraction_train=0.5, shuffle=true),
                       measure=cross_entropy,
                       verbosity=0)
(measure = MLJBase.CrossEntropy[cross_entropy],
 measurement = [3.908769341726582],
 per_fold = Array{Float64,1}[[3.908769341726582]],
 per_observation = Array{Array{Float64,1},1}[[[4.127134385045092, 4.127134385045092, 0.032789822822990956, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092  …  4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092, 4.127134385045092]]],)</code></pre><p>Changing a hyperparameter and re-evaluating:</p><pre><code class="language-julia-repl">julia&gt; tree_model.max_depth = 3
3

julia&gt; evaluate!(tree, resampling=Holdout(fraction_train=0.5, shuffle=true),
                 measure=cross_entropy,
                 verbosity=0)
(measure = MLJBase.CrossEntropy[cross_entropy],
 measurement = [0.25023646501669833],
 per_fold = Array{Float64,1}[[0.25023646501669833]],
 per_observation = Array{Array{Float64,1},1}[[[0.03278982282299073, 1.1151415906193203, 0.032789822822990956, 0.032789822822990956, 0.03278982282299073, 0.03278982282299073, 0.03278982282299073, 0.032789822822990956, 0.032789822822990956, 0.03278982282299073  …  0.03278982282299073, 0.03278982282299073, 0.032789822822990956, 0.03278982282299073, 0.03278982282299073, 0.03278982282299073, 0.03278982282299073, 0.03278982282299073, 0.03278982282299073, 0.03278982282299073]]],)</code></pre><h3><a class="nav-anchor" id="Next-steps-1" href="#Next-steps-1">Next steps</a></h3><p>To learn a little more about what MLJ can do, take the MLJ <a href="https://github.com/alan-turing-institute/MLJ.jl/blob/master/examples/tour/tour.ipynb">tour</a> or browse <a href="common_mlj_workflows/">Common MLJ Workflows</a>, returning to the manual as needed. <em>Read at least the remainder of this page before considering serious use of MLJ.</em></p><h3><a class="nav-anchor" id="Prerequisites-1" href="#Prerequisites-1">Prerequisites</a></h3><p>MLJ assumes some familiarity with the <code>CategoricalValue</code> and <code>CategoricalString</code> types from <a href="https://github.com/JuliaData/CategoricalArrays.jl">CategoricalArrays.jl</a>, used here for representing categorical data. For probabilistic predictors, a basic acquaintance with <a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a> is also assumed.</p><h3><a class="nav-anchor" id="Data-containers-and-scientific-types-1" href="#Data-containers-and-scientific-types-1">Data containers and scientific types</a></h3><p>The MLJ user should acquaint themselves with some basic assumptions about the form of data expected by MLJ, as outlined below. </p><pre><code class="language-none">machine(model::Supervised, X, y) 
machine(model::Unsupervised, X)</code></pre><p>Each supervised model in MLJ declares the permitted <em>scientific type</em> of the inputs <code>X</code> and targets <code>y</code> that can be bound to it in the first constructor above, rather than specifying specific machine types (such as <code>Array{Float32, 2}</code>). Similar remarks apply to the input <code>X</code> of an unsupervised model.</p><p>Scientific types are julia types defined in the package <a href="https://github.com/alan-turing-institute/ScientificTypes.jl">ScientificTypes.jl</a>, which also defines the convention used here (and there called <em>mlj</em>) for assigning a specific scientific type (interpretation) to each julia object (see the <code>scitype</code> examples below).</p><p>The basic &quot;scalar&quot; scientific types are <code>Continuous</code>, <code>Multiclass{N}</code>, <code>OrderedFactor{N}</code> and <code>Count</code>. Be sure you read <a href="#Container-element-types-1">Container element types</a> below to be guarantee your scalar data is interpreted correctly. Tools exist to coerce the data to have the appropriate scientfic type; see <a href="https://github.com/alan-turing-institute/ScientificTypes.jl">ScientificTypes.jl</a> or run <code>?coerce</code> for details.</p><p>Additionally, most data containers - such as tuples, vectors, matrices and tables - have a scientific type.</p><p><img src="scitypes.png" alt/></p><p><em>Figure 1. Part of the scientific type heirarchy in</em> ScientificTypes.jl.</p><pre><code class="language-julia-repl">julia&gt; scitype(4.6)
Continuous

julia&gt; scitype(42)
Count

julia&gt; x1 = categorical([&quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;maybe&quot;]);

julia&gt; scitype(x1)
AbstractArray{Multiclass{3},1}

julia&gt; X = (x1=x1, x2=rand(4), x3=rand(4))  # a &quot;column table&quot;
(x1 = CategoricalArrays.CategoricalString{UInt32}[&quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;maybe&quot;],
 x2 = [0.5342469059503165, 0.11965800173553998, 0.6492704730877039, 0.26457782120152595],
 x3 = [0.41141422946632034, 0.8332540412522127, 0.5256041162569622, 0.1556474952268594],)

julia&gt; scitype(X)
Table{Union{AbstractArray{Continuous,1}, AbstractArray{Multiclass{3},1}}}</code></pre><h4><a class="nav-anchor" id="Tabular-data-1" href="#Tabular-data-1">Tabular data</a></h4><p>All data containers compatible with the <a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> interface (which includes all source formats listed <a href="https://github.com/queryverse/IterableTables.jl">here</a>) have the scientific type <code>Table{K}</code>, where <code>K</code> depends on the scientific types of the columns, which can be individually inspected using <code>schema</code>:</p><pre><code class="language-julia-repl">julia&gt; schema(X)
(names = (:x1, :x2, :x3),
 types = (CategoricalArrays.CategoricalString{UInt32}, Float64, Float64),
 scitypes = (Multiclass{3}, Continuous, Continuous),
 nrows = 4,)</code></pre><h4><a class="nav-anchor" id="Inputs-1" href="#Inputs-1">Inputs</a></h4><p>Since an MLJ model only specifies the scientific type of data, if that type is <code>Table</code> - which is the case for the majority of MLJ models - then any <a href="https://github.com/JuliaData/Tables.jl">Tables.jl</a> format is permitted. However, the Tables.jl API excludes matrices. If <code>Xmatrix</code> is a matrix, convert it to a column table using <code>X = MLJ.table(Xmatrix)</code>.</p><p>Specifically, the requirement for an arbitrary model&#39;s input is <code>scitype(X) &lt;: input_scitype(model)</code>.</p><h4><a class="nav-anchor" id="Targets-1" href="#Targets-1">Targets</a></h4><p>The target <code>y</code> expected by MLJ models is generally an <code>AbstractVector</code>. A multivariate target <code>y</code> will generally be table. </p><p>Specifically, the type requirement for a model target is <code>scitype(y) &lt;: target_scitype(model)</code>.</p><h4><a class="nav-anchor" id="Querying-a-model-for-acceptable-data-types-1" href="#Querying-a-model-for-acceptable-data-types-1">Querying a model for acceptable data types</a></h4><p>Given a model instance, one can inspect the admissible scientific types of its input and target by querying the scientific type of the model itself:</p><div></div><pre><code class="language-">julia&gt; tree = DecisionTreeClassifier();
julia&gt; scitype(tree)
(input_scitype = ScientificTypes.Table{#s13} where #s13&lt;:(AbstractArray{#s12,1} where #s12&lt;:Continuous),
 target_scitype = AbstractArray{#s21,1} where #s21&lt;:Finite,
 is_probabilistic = true,)</code></pre><p>This does not work if relevant model code has not been loaded. In that case one can extract this information from the model type&#39;s registry entry, using <code>info</code>:</p><pre><code class="language-julia-repl">julia&gt; info(&quot;DecisionTreeClassifier&quot;)
Decision Tree Classifier.
→ based on [DecisionTree](https://github.com/bensadeghi/DecisionTree.jl).
→ do `@load DecisionTreeClassifier pkg=&quot;DecisionTree&quot;` to use the model.
→ do `?DecisionTreeClassifier` for documentation.
(name = &quot;DecisionTreeClassifier&quot;,
 package_name = &quot;DecisionTree&quot;,
 is_supervised = true,
 docstring = &quot;Decision Tree Classifier.\n→ based on [DecisionTree](https://github.com/bensadeghi/DecisionTree.jl).\n→ do `@load DecisionTreeClassifier pkg=\&quot;DecisionTree\&quot;` to use the model.\n→ do `?DecisionTreeClassifier` for documentation.&quot;,
 hyperparameter_types = [&quot;Float64&quot;, &quot;Int64&quot;, &quot;Int64&quot;, &quot;Int64&quot;, &quot;Float64&quot;, &quot;Int64&quot;, &quot;Int64&quot;, &quot;Bool&quot;, &quot;Float64&quot;, &quot;Float64&quot;],
 hyperparameters = Symbol[:pruning_purity, :max_depth, :min_samples_leaf, :min_samples_split, :min_purity_increase, :n_subfeatures, :display_depth, :post_prune, :merge_purity_threshold, :pdf_smoothing],
 implemented_methods = Symbol[:fit, :predict, :fitted_params],
 is_pure_julia = true,
 is_wrapper = false,
 load_path = &quot;MLJModels.DecisionTree_.DecisionTreeClassifier&quot;,
 package_license = &quot;MIT&quot;,
 package_url = &quot;https://github.com/bensadeghi/DecisionTree.jl&quot;,
 package_uuid = &quot;7806a523-6efd-50cb-b5f6-3fa6f1930dbb&quot;,
 prediction_type = :probabilistic,
 supports_weights = false,
 input_scitype = Table{_s13} where _s13&lt;:(AbstractArray{_s12,1} where _s12&lt;:Continuous),
 target_scitype = AbstractArray{_s673,1} where _s673&lt;:Finite,)</code></pre><h4><a class="nav-anchor" id="Container-element-types-1" href="#Container-element-types-1">Container element types</a></h4><p>Models in MLJ will always apply the <em>mlj</em> convention described in <a href="https://github.com/alan-turing-institute/ScientificTypes.jl">ScientificTypes.jl</a> to decide how to interpret the elements of your container types. Here are the key aspects of that convention:</p><ul><li><p>Any <code>AbstractFloat</code> is interpreted as <code>Continuous</code>.</p></li><li><p>Any <code>Integer</code> is interpreted as <code>Count</code>. </p></li><li><p>Any <code>CategoricalValue</code> or <code>CategoricalString</code>, <code>x</code>, is interpreted as <code>Multiclass</code> or <code>OrderedFactor</code>, depending on the value of <code>x.pool.ordered</code>.</p></li><li><p><code>String</code>s and <code>Char</code>s are <em>not</em> interpreted as <code>Finite</code>; they have <code>Unknown</code> scitype. Coerce vectors of strings or characters to <code>CategoricalVector</code>s if they represent <code>Multiclass</code> or <code>OrderedFactor</code> data. Do <code>?coerce</code> and <code>?unpack</code> to learn how. </p></li><li><p>In particular, <em>integers</em> (including <code>Bool</code>s) <em>cannot be used to represent categorical data.</em></p></li></ul><footer><hr/><a class="next" href="common_mlj_workflows/"><span class="direction">Next</span><span class="title">Common MLJ Workflows</span></a></footer></article></body></html>
