{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A tour of MLJ\n",
    "\n",
    "### Models, machines, basic training and testing\n",
    "\n",
    "Let's load data and define train and test rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/ayush99/.julia/compiled/v1.1/MLJ/rAU56.ji for MLJ [add582a8-e3ab-11e8-2d5e-e98b27df1bc7]\n",
      "└ @ Base loading.jl:1184\n"
     ]
    }
   ],
   "source": [
    "using MLJ\n",
    "using DataFrames, Statistics\n",
    "\n",
    "Xraw = rand(300,3)\n",
    "y = exp(Xraw[:,1] - Xraw[:,2] - 2Xraw[:,3] + 0.1*rand(300))\n",
    "X = DataFrame(Xraw)\n",
    "\n",
    "train, test = partition(eachindex(y), 0.70); # 70:30 split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *model* is a container for hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNRegressor(K = 10,\n",
       "             metric = MLJ.KNN.euclidean,\n",
       "             kernel = MLJ.KNN.reciprocal,)\u001b[34m @ 1…33\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model=KNNRegressor(K=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping the model in data creates a *machine* which will store training outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{KNNRegressor} @ 7…12\u001b[39m\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = machine(knn_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the training rows and evaluating on the test rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{KNNRegressor} @ 7…12\u001b[39m.\n",
      "└ @ MLJ /home/ayush99/.julia/packages/MLJ/LrTgW/src/machines.jl:102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.056181310146433745"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(knn, rows=train)\n",
    "yhat = predict(knn, X[test,:])\n",
    "rms(yhat, y[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we could have skipped the train/test definitions and evaluated one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Evaluating using a holdout set. \n",
      "│ fraction_train=0.7 \n",
      "│ shuffle=false \n",
      "│ measure=MLJ.rms \n",
      "│ operation=StatsBase.predict \n",
      "│ Resampling from all rows. \n",
      "└ @ MLJ /home/ayush99/.julia/packages/MLJ/LrTgW/src/resampling.jl:91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.056181310146433745"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate!(knn, resampling=Holdout(fraction_train=0.7), measure=rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our machine/model constructions and associateed fit/predict syntax anticipates a powerful extension for building networks of learners described later. Changing a hyperparameter and re-evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Evaluating using a holdout set. \n",
      "│ fraction_train=0.7 \n",
      "│ shuffle=false \n",
      "│ measure=MLJ.rms \n",
      "│ operation=StatsBase.predict \n",
      "│ Resampling from all rows. \n",
      "└ @ MLJ /home/ayush99/.julia/packages/MLJ/LrTgW/src/resampling.jl:91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07634238032999312"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model.K = 20\n",
    "evaluate!(knn, resampling=Holdout(fraction_train=0.7))  # `default_measure(knn) == rms` so `measure` kwarg can be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneous ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a bagged ensemble model for 20 K-nearest neighbour regressors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJ.DeterministicEnsembleModel(atom = \u001b[34mKNNRegressor{} @ 1…33\u001b[39m,\n",
       "                               weights = Float64[],\n",
       "                               bagging_fraction = 0.8,\n",
       "                               rng = MersenneTwister(UInt32[0x9aabcac1, 0x23fe85ec, 0xdb9c6bcc, 0xba9f2529]),\n",
       "                               n = 20,\n",
       "                               parallel = true,\n",
       "                               out_of_bag_measure = Any[],)\u001b[34m @ 7…29\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model = EnsembleModel(atom=knn_model, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLJ.DeterministicEnsembleModel(atom = KNNRegressor(K = 20,\n",
      "                                                   metric = MLJ.KNN.euclidean,\n",
      "                                                   kernel = MLJ.KNN.reciprocal,),\n",
      "                               weights = Float64[],\n",
      "                               bagging_fraction = 0.8,\n",
      "                               rng = MersenneTwister(UInt32[0x9aabcac1, 0x23fe85ec, 0xdb9c6bcc, 0xba9f2529]),\n",
      "                               n = 20,\n",
      "                               parallel = true,\n",
      "                               out_of_bag_measure = Any[],)\u001b[34m @ 7…29\u001b[39m"
     ]
    }
   ],
   "source": [
    "@more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be trained and tested the same as any other model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Evaluating using cross-validation. \n",
      "│ nfolds=6. \n",
      "│ shuffle=false \n",
      "│ measure=MLJ.rms \n",
      "│ operation=StatsBase.predict \n",
      "│ Resampling from all rows. \n",
      "└ @ MLJ /home/ayush99/.julia/packages/MLJ/LrTgW/src/resampling.jl:135\n",
      "\u001b[33mCross-validating: 100%[=========================] Time: 0:00:03\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6-element Array{Float64,1}:\n",
       " 0.12482743153258992\n",
       " 0.07508734859631627\n",
       " 0.15502281894894274\n",
       " 0.10822915364474125\n",
       " 0.07549563178337156\n",
       " 0.0791335239392    "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble = machine(ensemble_model, X, y)\n",
    "estimates = evaluate!(ensemble, resampling=CV())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10296598474086031"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematic tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simultaneously tune the ensemble's `bagging_fraction` and the K-nearest neighbour hyperparameter `K`. Since one of our models is a field of the other, we have nested hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(atom = (K = 20,\n",
       "         metric = MLJ.KNN.euclidean,\n",
       "         kernel = MLJ.KNN.reciprocal,),\n",
       " weights = Float64[],\n",
       " bagging_fraction = 0.8,\n",
       " rng = MersenneTwister(UInt32[0x9aabcac1, 0x23fe85ec, 0xdb9c6bcc, 0xba9f2529]),\n",
       " n = 20,\n",
       " parallel = true,\n",
       " out_of_bag_measure = Any[],)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(ensemble_model) # extract model hyperparameters as nested named tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a tuning grid, we construct ranges for the two parameters and collate these ranges following the same pattern above (omitting parameters that don't change):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(atom = (K = \u001b[34mNumericRange{K} @ 1…84\u001b[39m,),\n",
       " bagging_fraction = \u001b[34mNumericRange{bagging_fraction} @ 1…67\u001b[39m,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_range = range(ensemble_model, :bagging_fraction, lower= 0.5, upper=1.0, scale = :linear)\n",
    "K_range = range(knn_model, :K, lower=1, upper=100, scale=:log10)\n",
    "nested_ranges = (atom = (K = K_range,), \n",
    "                 bagging_fraction = B_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we choose a tuning strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grid(resolution = 12,\n",
       "     parallel = true,)\u001b[34m @ 1…44\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning = Grid(resolution=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a resampling strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Holdout(fraction_train = 0.8,\n",
       "        shuffle = false,)\u001b[34m @ 6…63\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampling = Holdout(fraction_train=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define a new model which wraps the these strategies around our ensemble model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJ.DeterministicTunedModel(model = \u001b[34mDeterministicEnsembleModel{KNNRegressor} @ 7…29\u001b[39m,\n",
       "                            tuning = \u001b[34mGrid{} @ 1…44\u001b[39m,\n",
       "                            resampling = \u001b[34mHoldout{} @ 6…63\u001b[39m,\n",
       "                            measure = nothing,\n",
       "                            operation = StatsBase.predict,\n",
       "                            nested_ranges = (atom = (K = \u001b[34mNumericRange{K} @ 1…84\u001b[39m,), bagging_fraction = \u001b[34mNumericRange{bagging_fraction} @ 1…67\u001b[39m),\n",
       "                            minimize = true,\n",
       "                            full_report = true,\n",
       "                            train_best = true,)\u001b[34m @ 1…72\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_ensemble_model = TunedModel(model=ensemble_model, \n",
    "    tuning=tuning, resampling=resampling, nested_ranges=nested_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the corresponding machine tunes the underlying model (in this case an ensemble) and retrains on all supplied data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: No measure specified. Using measure=MLJ.rms. \n",
      "└ @ MLJ /home/ayush99/.julia/packages/MLJ/LrTgW/src/machines.jl:91\n",
      "┌ Info: Training \u001b[34mMachine{DeterministicTunedModel} @ 8…04\u001b[39m.\n",
      "└ @ MLJ /home/ayush99/.julia/packages/MLJ/LrTgW/src/machines.jl:102\n",
      "\u001b[33mIterating over a 132-point grid: 100%[=========================] Time: 0:00:04\u001b[39m\n",
      "┌ Info: Training best model on all supplied data.\n",
      "└ @ MLJ /home/ayush99/.julia/packages/MLJ/LrTgW/src/tuning.jl:200\n"
     ]
    }
   ],
   "source": [
    "tuned_ensemble = machine(tuned_ensemble_model, X[train,:], y[train])\n",
    "fit!(tuned_ensemble);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each fitted machine, one may inspect a user-friendly version of the fitted parameters (as opposed to the hyperparameters stored in its model). In the current case this is the best ensemble model (trained on all available data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(best_model = \u001b[34mDeterministicEnsembleModel{KNNRegressor} @ 8…96\u001b[39m,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_params(tuned_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(atom = (K = 2,\n",
       "         metric = MLJ.KNN.euclidean,\n",
       "         kernel = MLJ.KNN.reciprocal,),\n",
       " weights = Float64[],\n",
       " bagging_fraction = 0.5454545454545454,\n",
       " rng = MersenneTwister(UInt32[0x9aabcac1, 0x23fe85ec, 0xdb9c6bcc, 0xba9f2529]),\n",
       " n = 20,\n",
       " parallel = true,\n",
       " out_of_bag_measure = Any[],)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.best_model |> params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `report` method gives more detail on the tuning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(parameter_names = [\"atom.K\" \"bagging_fraction\"],\n",
       " parameter_scales = Symbol[:log10 :linear],\n",
       " parameter_values = Any[1 0.5; 2 0.5; … ; 66 1.0; 100 1.0],\n",
       " measurements = [0.10563, 0.0848966, 0.0984288, 0.0997608, 0.118416, 0.124203, 0.151223, 0.187434, 0.218704, 0.266869  …  0.124131, 0.0831967, 0.073591, 0.0800684, 0.100143, 0.119439, 0.12947, 0.155378, 0.188244, 0.231607],\n",
       " best_measurement = 0.06895626545206299,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(tuned_ensemble) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or for a plot of tuning results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "pyplot()\n",
    "plot(tuned_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = predict(tuned_ensemble, X[test,:])\n",
    "rms(yhat, y[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, using all the data, get cross-validation estimates, with cv-tuning on each fold complement (nested resampling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_ensemble = machine(tuned_ensemble_model, X, y)\n",
    "estimates = evaluate!(tuned_ensemble, resampling=CV(nfolds=4), verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLJ has a flexible interface for building networks from multiple machine learning elements, whose complexity extend beyond linear \"pipelines\", and with a minimal of added abstraction.\n",
    "\n",
    "In MLJ, a *learning network* is a graph whose nodes apply an operation, such as `predict` or `transform`, using a fixed machine (requiring training) - or which, alternatively, applies a regular (untrained) mathematical operation to its input(s). In practice, a learning network works with *fixed* sources for its training/evaluation data, but can be built and tested in stages. By contrast, an *exported learning network* is a learning network exported as a stand-alone, re-usable `Model` object, to which all the MLJ `Model`  meta-algorthims can be applied (ensembling, systematic tuning, etc). \n",
    "\n",
    "As we shall see, exporting a learning network as a reusable model, is very easy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple learning network\n",
    "\n",
    "![](wrapped_ridge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram above depicts a learning network which standardises the input data, `X`, learns an optimal Box-Cox transformation for the target, `y`, predicts new targets using ridge regression, and then inverse-transforms those predictions (for later comparison with the original test data). The machines are labelled yellow. \n",
    "\n",
    "To implement the network, we begin by loading all data needed for training and evaluation into *source nodes*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = source(X)\n",
    "ys = source(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label nodes according to their outputs in the diagram. Notice that the nodes `z` and `yhat` use the same machine `box` for different operations. \n",
    "\n",
    "To construct the `W` node we first need to define the machine `stand` that it will use to transform inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_model = Standardizer()\n",
    "stand = machine(stand_model, Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `Xs` is a node, instead of concrete data, we can call `transform` on the machine without first training it, and the result is the new node `W`, instead of concrete transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = transform(stand, Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get actual transformed data we *call* the node appropriately, which will require we first train the node. Training a node, rather than a machine, triggers training of *all* necessary machines in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(W, rows=train)\n",
    "W()          # transform all data\n",
    "W(rows=test) # transform only test data\n",
    "W(X[3:4,:])  # transform any data, new or old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can think of `W` (and the other nodes we will define) as \"dynamic data\": `W` is *data*, in the sense that  it an be called (\"indexed\") on rows, but *dynamic*, in the sense the result depends on the outcome of training events. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other nodes of our network are defined similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_model = UnivariateBoxCoxTransformer()  # for making data look normally-distributed\n",
    "box = machine(box_model, ys)\n",
    "z = transform(box, ys)\n",
    "\n",
    "@load RidgeRegressor\n",
    "\n",
    "ridge_model = RidgeRegressor(lambda=0.1)\n",
    "ridge =machine(ridge_model, W, z)\n",
    "zhat = predict(ridge, W)\n",
    "\n",
    "yhat = inverse_transform(box, zhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to train and evaluate the completed network. Notice that the standardizer, `stand`, is *not* retrained, as MLJ remembers that it was trained earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(yhat, rows=train)\n",
    "rms(y[test], yhat(rows=test)) # evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat(X[3:4,:])  # predict on new or old data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change hyperparameters and retrain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model.lambda = 0.01\n",
    "fit!(yhat, rows=train) \n",
    "rms(y[test], yhat(rows=test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Notable feature.** The machine, `ridge::NodalMachine{RidgeRegressor}`, is retrained, because its underlying model has been mutated. However, since the outcome of this training has no effect on the training inputs of the machines `stand` and `box`, these transformers are left untouched. (During construction, each node and machine in a learning network determines and records all machines on which it depends.) This behaviour, which extends to exported learning networks, means we can tune our wrapped regressor without re-computing transformations each time the hyperparameter is changed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting a learning network as a composite model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export a learning network:\n",
    "- Define a new `mutable struct` model type.\n",
    "- Wrap the learning network code in a model `fit` method.\n",
    "\n",
    "All learning networks that make determinisic (or, probabilistic) predictions export as models of subtype `DeterministicNetwork` (respectively, `ProbabilisticNetwork`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct WrappedRidge <: DeterministicNetwork\n",
    "    ridge_model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now satisfied that our wrapped Ridge Regression learning network works, we simply cut and paste its defining code into a `fit` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function MLJ.fit(model::WrappedRidge, X, y)\n",
    "    Xs = source(X)\n",
    "    ys = source(y)\n",
    "\n",
    "    stand_model = Standardizer()\n",
    "    stand = machine(stand_model, Xs)\n",
    "    W = transform(stand, Xs)\n",
    "\n",
    "    box_model = UnivariateBoxCoxTransformer()  # for making data look normally-distributed\n",
    "    box = machine(box_model, ys)\n",
    "    z = transform(box, ys)\n",
    "\n",
    "    ridge_model = model.ridge_model ###\n",
    "    ridge =machine(ridge_model, W, z)\n",
    "    zhat = predict(ridge, W)\n",
    "\n",
    "    yhat = inverse_transform(box, zhat)\n",
    "    fit!(yhat, verbosity=0)\n",
    "    \n",
    "    return yhat\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line marked `###`, where the new exported model's hyperparameter `ridge_model` is spliced into the network, is the only modification.\n",
    "\n",
    "This completes the export process.\n",
    "\n",
    "> **What's going on here?** MLJ's machine interface is built atop a more primitive *[model](adding_models_for_general_use.md)* interface, implemented for each algorithm. Each supervised model type (eg, `RidgeRegressor`) requires model `fit` and `predict` methods, which are called by the corresponding machine `fit!` and `predict` methods. We don't need to define a  model `predict` method here because MLJ provides a fallback which simply calls the node returned by `fit` on the data supplied: `MLJ.predict(model::Supervised{Node}, Xnew) = yhat(Xnew)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now let's wrap our composite model as a tuned model and evaluate on the Boston dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = load_boston()\n",
    "X, y = task()\n",
    "train, test = partition(eachindex(y), 0.7)\n",
    "wrapped_model = WrappedRidge(ridge_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params(wrapped_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_ranges = (ridge_model = (lambda = range(ridge_model, :lambda, lower=0.1, upper=100.0, scale=:log10),),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_wrapped_model = TunedModel(model=wrapped_model, tuning=Grid(resolution=20),\n",
    "resampling=CV(), measure=rms, nested_ranges=nested_ranges);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_wrapped = machine(tuned_wrapped_model, X, y)\n",
    "evaluate!(tuned_wrapped, resampling=Holdout(fraction_train=0.7), measure=rms, verbosity=0) |> mean  # nested resampling estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
