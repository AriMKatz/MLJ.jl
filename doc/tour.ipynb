{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A tour of MLJ\n",
    "\n",
    "### Models, machines, basic training and testing\n",
    "\n",
    "Let's load data and define train and test rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ\n",
    "using DataFrames, Statistics\n",
    "\n",
    "Xraw = rand(300,3)\n",
    "y = exp(Xraw[:,1] - Xraw[:,2] - 2Xraw[:,3] + 0.1*rand(300))\n",
    "X = DataFrame(Xraw)\n",
    "\n",
    "train, test = partition(eachindex(y), 0.70); # 70:30 split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *model* is a container for hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mKNNRegressor{Float64} @ 1…89\u001b[22m: \n",
       "target_type             =>   Float64\n",
       "K                       =>   10\n",
       "metric                  =>   euclidean (generic function with 1 method)\n",
       "kernel                  =>   reciprocal (generic function with 1 method)\n",
       "\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model=KNNRegressor(K=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping the model in data creates a *machine* which will store training outcomes (called *fit-results*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mMachine{KNNRegressor{Float64}} @ 1…86\u001b[22m: \n",
       "model                   =>   \u001b[0m\u001b[1mKNNRegressor{Float64} @ 1…89\u001b[22m\n",
       "fitresult               =>   (undefined)\n",
       "cache                   =>   (undefined)\n",
       "args                    =>   (omitted Tuple{DataFrame,Array{Float64,1}} of length 2)\n",
       "report                  =>   (undefined)\n",
       "rows                    =>   (undefined)\n",
       "\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = machine(knn_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the training rows and evaluating on the test rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[0m\u001b[1mMachine{KNNRegressor{Float64}} @ 1…86\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:93\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09078144921694162"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(knn, rows=train)\n",
    "yhat = predict(knn, X[test,:])\n",
    "rms(y[test], yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we could have skipped the train/test definitions and evaluated one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09078144921694162"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate!(knn, resampling=Holdout(fraction_train=0.7), measure=rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our machine/model constructions and associateed fit/predict syntax anticipates a powerful extension for building networks of learners described later. Changing a hyperparameter and re-evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11375481529099939"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model.K = 20\n",
    "evaluate!(knn, resampling=Holdout(fraction_train=0.7))  # `default_measure(knn) == rms` so `measure` kwarg can be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneous ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a bagged ensemble model for 20 K-nearest neighbour regressors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mDeterministicEnsembleModel @ 1…12\u001b[22m: \n",
       "atom                    =>   \u001b[0m\u001b[1mKNNRegressor{Float64} @ 1…89\u001b[22m\n",
       "weights                 =>   0-element Array{Float64,1}\n",
       "bagging_fraction        =>   0.8\n",
       "rng_seed                =>   0\n",
       "n                       =>   20\n",
       "parallel                =>   true\n",
       "\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model = EnsembleModel(atom=knn_model, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# \u001b[0m\u001b[1mDeterministicEnsembleModel @ 1…12\u001b[22m: \n",
      "atom                    =>   \u001b[0m\u001b[1mKNNRegressor{Float64} @ 1…89\u001b[22m\n",
      "weights                 =>   0-element Array{Float64,1}\n",
      "bagging_fraction        =>   0.8\n",
      "rng_seed                =>   0\n",
      "n                       =>   20\n",
      "parallel                =>   true\n",
      "\n",
      "## \u001b[0m\u001b[1mKNNRegressor{Float64} @ 1…89\u001b[22m: \n",
      "target_type             =>   Float64\n",
      "K                       =>   20\n",
      "metric                  =>   euclidean (generic function with 1 method)\n",
      "kernel                  =>   reciprocal (generic function with 1 method)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be trained and tested the same as any other model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mCross-validating: 100%[=========================] Time: 0:00:06\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6-element Array{Float64,1}:\n",
       " 0.11224997911116968\n",
       " 0.08307949451639975\n",
       " 0.04953104214674491\n",
       " 0.16412111968261445\n",
       " 0.06412673500541309\n",
       " 0.14816801971581123"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble = machine(ensemble_model, X, y)\n",
    "estimates = evaluate!(ensemble, resampling=CV())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10354606502969217, 0.04610119519434663)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mean(estimates), std(estimates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematic tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simultaneously tune the ensemble's `bagging_fraction` and the K-nearest neighbour hyperparameter `K`. Since one of our models is a field of the other, we have nested hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(atom = (target_type = Float64, K = 20, metric = MLJ.KNN.euclidean, kernel = MLJ.KNN.reciprocal), weights = Float64[], bagging_fraction = 0.8, rng_seed = 0, n = 20, parallel = true)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(ensemble_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a tuning grid, we construct ranges for the two parameters and collate these ranges following the same pattern above (omitting parameters that don't change):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(atom = (K = \u001b[0m\u001b[1mNumericRange @ 1…30\u001b[22m,), bagging_fraction = \u001b[0m\u001b[1mNumericRange @ 1…09\u001b[22m)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_range = range(ensemble_model, :bagging_fraction, lower= 0.5, upper=1.0, scale = :linear)\n",
    "K_range = range(knn_model, :K, lower=1, upper=100, scale=:log10)\n",
    "nested_ranges = (atom = (K = K_range,), bagging_fraction = B_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we choose a tuning strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mGrid @ 4…12\u001b[22m: \n",
       "resolution              =>   12\n",
       "parallel                =>   true\n",
       "\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning = Grid(resolution=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a resampling strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mHoldout @ 3…32\u001b[22m: \n",
       "fraction_train          =>   0.8\n",
       "shuffle                 =>   false\n",
       "\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampling = Holdout(fraction_train=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define a new model which wraps the these strategies around our ensemble model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mDeterministicTunedModel @ 1…86\u001b[22m: \n",
       "model                   =>   \u001b[0m\u001b[1mDeterministicEnsembleModel @ 1…12\u001b[22m\n",
       "tuning                  =>   \u001b[0m\u001b[1mGrid @ 4…12\u001b[22m\n",
       "resampling              =>   \u001b[0m\u001b[1mHoldout @ 3…32\u001b[22m\n",
       "measure                 =>   nothing\n",
       "operation               =>   predict (generic function with 19 methods)\n",
       "nested_ranges           =>   (omitted NamedTuple{(:atom, :bagging_fraction),Tuple{NamedTuple{(:K,),Tuple{MLJ.NumericRange{Int64,Symbol}}},MLJ.NumericRange{Float64,Symbol}}})\n",
       "full_report             =>   true\n",
       "\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_ensemble_model = TunedModel(model=ensemble_model, \n",
    "    tuning=tuning, resampling=resampling, nested_ranges=nested_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the corresponding machine tunes the underlying model (in this case an ensemble) and retrains on all supplied data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: No measure specified. Using measure=MLJ.rms. \n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:82\n",
      "┌ Info: Training \u001b[0m\u001b[1mMachine{MLJ.DeterministicTunedMo…} @ 1…23\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:93\n",
      "\u001b[33mSearching a 132-point grid for best model: 100%[=========================] Time: 0:00:07\u001b[39m\n",
      "┌ Info: Training best model on all supplied data.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/tuning.jl:142\n"
     ]
    }
   ],
   "source": [
    "tuned_ensemble = machine(tuned_ensemble_model, X[train,:], y[train])\n",
    "fit!(tuned_ensemble);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each fitted machine, one may access the fitted parameters (as opposed to the hyperparameters stored in its model). In the current case the \"fitted parameter\" is the best ensemble model (trained on all available data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(best_model = \u001b[0m\u001b[1mDeterministicEnsembleModel @ 1…20\u001b[22m,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = fitted_params(tuned_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(fp.best_model).bagging_fraction = 0.5909090909090909\n",
      "((fp.best_model).atom).K = 1\n"
     ]
    }
   ],
   "source": [
    "@show fp.best_model.bagging_fraction\n",
    "@show fp.best_model.atom.K;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `report` method gives more detail on the tuning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Tuple{Symbol,Any},1}:\n",
       " (:parameter_names, [\"atom.K\" \"bagging_fraction\"])                                                                                                                                                                            \n",
       " (:parameter_scales, Symbol[:log10 :linear])                                                                                                                                                                                  \n",
       " (:parameter_values, Any[1 0.5; 2 0.5; … ; 66 1.0; 100 1.0])                                                                                                                                                                  \n",
       " (:measurements, [0.0896036, 0.115431, 0.128693, 0.154714, 0.196045, 0.222209, 0.246999, 0.273775, 0.31375, 0.358156  …  0.0999239, 0.105842, 0.111224, 0.129869, 0.151254, 0.194264, 0.224528, 0.254061, 0.287962, 0.327163])\n",
       " (:best_measurement, 0.0786385513959987)                                                                                                                                                                                      "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=report(tuned_ensemble)           # named tuple\n",
    "zip(keys(r), values(r)) |> collect # for better viewing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08409339267106415"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = predict(tuned_ensemble, X[test,:])\n",
    "rms(yhat, y[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, using all the data, get cross-validation estimates, with cv-tuning on each fold complement (nested resampling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mCross-validating:  20%[=====>                   ]  ETA: 0:00:00\u001b[39m┌ Info: Training \u001b[0m\u001b[1mMachine{MLJ.DeterministicTunedMo…} @ 1…48\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:93\n",
      "\u001b[33mSearching a 132-point grid for best model: 100%[=========================] Time: 0:00:06\u001b[39m\n",
      "┌ Info: Training best model on all supplied data.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/tuning.jl:142\n",
      "\u001b[33mCross-validating:  40%[==========>              ]  ETA: 0:00:09\u001b[39m┌ Info: Training \u001b[0m\u001b[1mMachine{MLJ.DeterministicTunedMo…} @ 1…48\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:93\n",
      "\u001b[33mSearching a 132-point grid for best model: 100%[=========================] Time: 0:00:06\u001b[39m\n",
      "┌ Info: Training best model on all supplied data.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/tuning.jl:142\n",
      "\u001b[33mCross-validating:  60%[===============>         ]  ETA: 0:00:08\u001b[39m┌ Info: Training \u001b[0m\u001b[1mMachine{MLJ.DeterministicTunedMo…} @ 1…48\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:93\n",
      "\u001b[33mSearching a 132-point grid for best model: 100%[=========================] Time: 0:00:06\u001b[39m\n",
      "┌ Info: Training best model on all supplied data.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/tuning.jl:142\n",
      "\u001b[33mCross-validating:  80%[====================>    ]  ETA: 0:00:05\u001b[39m┌ Info: Training \u001b[0m\u001b[1mMachine{MLJ.DeterministicTunedMo…} @ 1…48\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:93\n",
      "\u001b[33mSearching a 132-point grid for best model: 100%[=========================] Time: 0:00:06\u001b[39m\n",
      "┌ Info: Training best model on all supplied data.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/tuning.jl:142\n",
      "\u001b[33mCross-validating: 100%[=========================] Time: 0:00:25\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.08096082109810439\n",
       " 0.07800556218551255\n",
       " 0.07327732650564481\n",
       " 0.08309441293645088"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_ensemble = machine(tuned_ensemble_model, X, y)\n",
    "evaluate!(tuned_ensemble, resampling=CV(nfolds=4), verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLJ has a flexible interface for building networks from multiple machine learning elements, whose complexity extend beyond linear \"pipelines\", and with a minimal of added abstraction.\n",
    "\n",
    "In MLJ, a *learning network* is a graph whose nodes apply an operation, such as `predict` or `transform`, using a fixed machine (requiring training) - or which, alternatively, applies a regular (untrained) mathematical operation to its input(s). In practice, a learning network works with *fixed* sources for its training/evaluation data, but can be built and tested in stages. By contrast, an *exported learning network* is a learning network exported as a stand-alone, re-usable `Model` object, to which all the MLJ `Model`  meta-algorthims can be applied (ensembling, systematic tuning, etc). \n",
    "\n",
    "As we shall see, exporting a learning network as a reusable model, is very easy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple learning network\n",
    "\n",
    "![](wrapped_ridge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram above depicts a learning network which standardises the input data, `X`, learns an optimal Box-Cox transformation for the target, `y`, predicts new targets using ridge regression, and then inverse-transforms those predictions (for later comparison with the original test data). The machines are labelled yellow. \n",
    "\n",
    "To implement the network, we begin by loading all data needed for training and evaluation into *source nodes*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1mSource @ 1…99\u001b[22m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs = source(X)\n",
    "ys = source(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label nodes according to their outputs in the diagram. Notice that the nodes `z` and `yhat` use the same machine `box` for different operations. \n",
    "\n",
    "To construct the `W` node we first need to define the machine `stand` that it will use to transform inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1mNodalMachine @ 1…82\u001b[22m = machine(\u001b[0m\u001b[1mStandardizer @ 1…00\u001b[22m, \u001b[0m\u001b[1m8…05\u001b[22m)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stand_model = Standardizer()\n",
    "stand = machine(stand_model, Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `Xs` is a node, instead of concrete data, we can call `transform` on the machine without first training it, and the result is the new node `W`, instead of concrete transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1mNode @ 1…46\u001b[22m = transform(\u001b[0m\u001b[1m1…82\u001b[22m, \u001b[0m\u001b[1m8…05\u001b[22m)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = transform(stand, Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get actual transformed data we *call* the node appropriately, which will require we first train the node. Training a node, rather than a machine, triggers training of *all* necessary machines in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[0m\u001b[1mNodalMachine{Standardizer} @ 1…82\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:93\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(x1 = [0.376715, 0.561488], x2 = [-0.213696, -0.555213], x3 = [-0.787951, 1.2842])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(W, rows=train)\n",
    "W()          # transform all data\n",
    "W(rows=test) # transform only test data\n",
    "W(X[3:4,:])  # transform any data, new or old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can think of `W` (and the other nodes we will define) as \"dynamic data\": `W` is *data*, in the sense that  it an be called (\"indexed\") on rows, but *dynamic*, in the sense the result depends on the outcome of training events. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other nodes of our network are defined similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1mNode @ 1…00\u001b[22m = inverse_transform(\u001b[0m\u001b[1m2…26\u001b[22m, predict(\u001b[0m\u001b[1m5…34\u001b[22m, transform(\u001b[0m\u001b[1m1…82\u001b[22m, \u001b[0m\u001b[1m8…05\u001b[22m)))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_model = UnivariateBoxCoxTransformer()  # for making data look normally-distributed\n",
    "box = machine(box_model, ys)\n",
    "z = transform(box, ys)\n",
    "\n",
    "ridge_model = RidgeRegressor(lambda=0.1)\n",
    "ridge =machine(ridge_model, W, z)\n",
    "zhat = predict(ridge, W)\n",
    "\n",
    "yhat = inverse_transform(box, zhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to train and evaluate the completed network. Notice that the standardizer, `stand`, is *not* retrained, as MLJ remembers that it was trained earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Not retraining \u001b[0m\u001b[1mNodalMachine{Standardizer} @ 1…82\u001b[22m. It is up-to-date.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:201\n",
      "┌ Info: Training \u001b[0m\u001b[1mNodalMachine{UnivariateBoxCoxTransfor…} @ 2…26\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:93\n",
      "┌ Info: Training \u001b[0m\u001b[1mNodalMachine{RidgeRegressor{Float64}} @ 5…34\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:93\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.020077532519835517"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(yhat, rows=train)\n",
    "rms(y[test], yhat(rows=test)) # evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.6976639470833352\n",
       " 0.244671334466375 "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat(X[3:4,:])  # predict on new or old data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change hyperparameters and retrain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Not retraining \u001b[0m\u001b[1mNodalMachine{UnivariateBoxCoxTransfor…} @ 2…26\u001b[22m. It is up-to-date.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:201\n",
      "┌ Info: Not retraining \u001b[0m\u001b[1mNodalMachine{Standardizer} @ 1…82\u001b[22m. It is up-to-date.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:201\n",
      "┌ Info: Updating \u001b[0m\u001b[1mNodalMachine{RidgeRegressor{Float64}} @ 5…34\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02008343668851138"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model.lambda = 0.01\n",
    "fit!(yhat, rows=train) \n",
    "rms(y[test], yhat(rows=test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Notable feature.** The machine, `ridge::NodalMachine{RidgeRegressor}`, is retrained, because its underlying model has been mutated. However, since the outcome of this training has no effect on the training inputs of the machines `stand` and `box`, these transformers are left untouched. (During construction, each node and machine in a learning network determines and records all machines on which it depends.) This behaviour, which extends to exported learning networks, means we can tune our wrapped regressor without re-computing transformations each time the hyperparameter is changed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting a learning network as a composite model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export a learning network:\n",
    "- Define a new `mutable struct` model type.\n",
    "- Wrap the learning network code in a model `fit` method.\n",
    "\n",
    "All learning networks that make determinisic (or, probabilistic) predictions export as models of subtype `Deterministic{Node}` (respectively, `Probabilistic{Node}`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct WrappedRidge <: Deterministic{Node}\n",
    "    ridge_model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now satisfied that our wrapped Ridge Regression learning network works, we simply cut and paste its defining code into a `fit` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "function MLJ.fit(model::WrappedRidge, X, y)\n",
    "    Xs = source(X)\n",
    "    ys = source(y)\n",
    "\n",
    "    stand_model = Standardizer()\n",
    "    stand = machine(stand_model, Xs)\n",
    "    W = transform(stand, Xs)\n",
    "\n",
    "    box_model = UnivariateBoxCoxTransformer()  # for making data look normally-distributed\n",
    "    box = machine(box_model, ys)\n",
    "    z = transform(box, ys)\n",
    "\n",
    "    ridge_model = model.ridge_model ###\n",
    "    ridge =machine(ridge_model, W, z)\n",
    "    zhat = predict(ridge, W)\n",
    "\n",
    "    yhat = inverse_transform(box, zhat)\n",
    "    fit!(yhat, verbosity=0)\n",
    "    \n",
    "    return yhat\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line marked `###`, where the new exported model's hyperparameter `ridge_model` is spliced into the network, is the only modification.\n",
    "\n",
    "This completes the export process.\n",
    "\n",
    "> **What's going on here?** MLJ's machine interface is built atop a more primitive *[model](adding_new_models.md)* interface, implemented for each algorithm. Each supervised model type (eg, `RidgeRegressor`) requires model `fit` and `predict` methods, which are called by the corresponding machine `fit!` and `predict` methods. We don't need to define a  model `predict` method here because MLJ provides a fallback which simply calls the node returned by `fit` on the data supplied: `MLJ.predict(model::Supervised{Node}, Xnew) = yhat(Xnew)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now let's wrap our composite model as a tuned model and evaluate on the Boston dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mWrappedRidge @ 1…44\u001b[22m: \n",
       "ridge_model             =>   \u001b[0m\u001b[1mRidgeRegressor{Float64} @ 1…83\u001b[22m\n",
       "\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = load_boston()\n",
    "X, y = task()\n",
    "train, test = partition(eachindex(y), 0.7)\n",
    "wrapped_model = WrappedRidge(ridge_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ridge_model = (target_type = Float64, lambda = 0.01),)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(wrapped_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ridge_model = (lambda = \u001b[0m\u001b[1mNumericRange @ 3…60\u001b[22m,),)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_ranges = (ridge_model = (lambda = range(ridge_model, :lambda, lower=0.1, upper=100.0, scale=:log10),),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_wrapped_model = TunedModel(model=wrapped_model, tuning=Grid(resolution=20),\n",
    "resampling=CV(), measure=rms, nested_ranges=nested_ranges);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.88236977264247"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_wrapped = machine(tuned_wrapped_model, X, y)\n",
    "evaluate!(tuned_wrapped, resampling=Holdout(fraction_train=0.7), measure=rms, verbosity=0) |> mean  # nested resampling estimate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
