{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A tour of MLJ\n",
    "\n",
    "### Models, machines, basic training and testing\n",
    "\n",
    "Let's load data and define train and test rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/anthony/.julia/compiled/v1.0/MLJ/rAU56.ji for MLJ [add582a8-e3ab-11e8-2d5e-e98b27df1bc7]\n",
      "└ @ Base loading.jl:1190\n"
     ]
    }
   ],
   "source": [
    "using MLJ\n",
    "using DataFrames\n",
    "\n",
    "Xraw = rand(300,3)\n",
    "y = exp(Xraw[:,1] - Xraw[:,2] - 2Xraw[:,3] + 0.1*rand(300))\n",
    "X = DataFrame(Xraw)\n",
    "\n",
    "train, test = partition(eachindex(y), 0.70); # 70:10:10 split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *model* is a container for hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mKNNRegressor @ 1…44\u001b[22m: \n",
       "K                       =>   10\n",
       "metric                  =>   euclidean (generic function with 1 method)\n",
       "kernel                  =>   reciprocal (generic function with 1 method)\n",
       "\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model=KNNRegressor(K=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping the model in data creates a *machine* which will store training outcomes (called *fit-results*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mMachine{KNNRegressor} @ 1…37\u001b[22m: \n",
       "model                   =>   \u001b[0m\u001b[1mKNNRegressor @ 1…44\u001b[22m\n",
       "fitresult               =>   (undefined)\n",
       "cache                   =>   (undefined)\n",
       "args                    =>   (omitted Tuple{DataFrame,Array{Float64,1}} of length 2)\n",
       "report                  =>   empty Dict{Symbol,Any}\n",
       "rows                    =>   (undefined)\n",
       "\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = machine(knn_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the training rows and evaluating on the test rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[0m\u001b[1mMachine{KNNRegressor} @ 1…37\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08218224634391742"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(knn, rows=train)\n",
    "yhat = predict(knn, X[test,:])\n",
    "rms(y[test], yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our machine/model constructs and associateed fit/predict syntax anticipates a powerful extension for building networks of learners described later. Changing a hyperparameter and re-evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[0m\u001b[1mMachine{KNNRegressor} @ 1…37\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08990830966536938"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model.K = 20\n",
    "fit!(knn)\n",
    "yhat = predict(knn, X[test,:])\n",
    "rms(y[test], yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneous ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a bagged ensemble model for 20 K-nearest neighbour regressors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mDeterministicEnsembleModel @ 1…71\u001b[22m: \n",
       "atom                    =>   \u001b[0m\u001b[1mKNNRegressor @ 1…44\u001b[22m\n",
       "weights                 =>   0-element Array{Float64,1}\n",
       "bagging_fraction        =>   0.8\n",
       "rng_seed                =>   0\n",
       "n                       =>   20\n",
       "parallel                =>   true\n",
       "\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model = EnsembleModel(atom=knn_model, n=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# \u001b[0m\u001b[1mDeterministicEnsembleModel @ 1…71\u001b[22m: \n",
      "atom                    =>   \u001b[0m\u001b[1mKNNRegressor @ 1…44\u001b[22m\n",
      "weights                 =>   0-element Array{Float64,1}\n",
      "bagging_fraction        =>   0.8\n",
      "rng_seed                =>   0\n",
      "n                       =>   20\n",
      "parallel                =>   true\n",
      "\n",
      "## \u001b[0m\u001b[1mKNNRegressor @ 1…44\u001b[22m: \n",
      "K                       =>   20\n",
      "metric                  =>   euclidean (generic function with 1 method)\n",
      "kernel                  =>   reciprocal (generic function with 1 method)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be trained and tested the same as any other model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[0m\u001b[1mMachine{DeterministicEnsembleMod…} @ 9…88\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:69\n",
      "\u001b[33mTraining ensemble:  10%[=====>                                            ]  ETA: 0:00:05\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mTraining ensemble: 100%[==================================================] Time: 0:00:01\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11899555394742675"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble = machine(ensemble_model, X, y)\n",
    "fit!(ensemble, rows=train)\n",
    "yhat = predict(ensemble, X[test, :])\n",
    "rms(y[test], yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematic tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simultaneously tune the ensemble's `bagging_fraction` and the K-nearest neighbour hyperparameter `K`. Since one of these models is a field of the other, we have nested hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params(:atom => Params(:K => 20, :metric => MLJ.KNN.euclidean, :kernel => MLJ.KNN.reciprocal), :weights => Float64[], :bagging_fraction => 0.8, :rng_seed => 0, :n => 20, :parallel => true)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(ensemble_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a tuning grid, we construct ranges for the two parameters and collate these ranges following the same pattern above (omitting parameters that don't change):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params(:atom => Params(:K => \u001b[0m\u001b[1mNumericRange @ 1…54\u001b[22m), :bagging_fraction => \u001b[0m\u001b[1mNumericRange @ 1…62\u001b[22m)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_range = range(ensemble_model, :bagging_fraction, lower= 0.5, upper=1.0, scale = :linear)\n",
    "K_range = range(knn_model, :K, lower=1, upper=100, scale=:log10)\n",
    "param_ranges = Params(:atom => Params(:K => K_range), :bagging_fraction => B_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we choose a tuning strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mGrid @ 1…58\u001b[22m: \n",
       "resolution              =>   12\n",
       "parallel                =>   true\n",
       "\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning = Grid(resolution=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a resampling strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mHoldout @ 1…26\u001b[22m: \n",
       "fraction_train          =>   0.8\n",
       "\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampling = Holdout(fraction_train=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define a new model which wraps the these strategies around our ensemble model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mTunedModel @ 1…63\u001b[22m: \n",
       "model                   =>   \u001b[0m\u001b[1mDeterministicEnsembleModel @ 1…71\u001b[22m\n",
       "tuning_strategy         =>   \u001b[0m\u001b[1mGrid @ 1…58\u001b[22m\n",
       "resampling_strategy     =>   \u001b[0m\u001b[1mHoldout @ 1…26\u001b[22m\n",
       "measure                 =>   rms (generic function with 5 methods)\n",
       "operation               =>   predict (generic function with 19 methods)\n",
       "param_ranges            =>   Params(:atom => Params(:K => \u001b[0m\u001b[1mNumericRange @ 1…54\u001b[22m), :bagging_fraction => \u001b[0m\u001b[1mNumericRange @ 1…62\u001b[22m)\n",
       "report_measurements     =>   true\n",
       "\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_ensemble_model = TunedModel(model=ensemble_model, \n",
    "    tuning_strategy=tuning, resampling_strategy=resampling, param_ranges=param_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the corresponding machine tunes the underlying model (in this case an ensemble) and retrains on all supplied data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[0m\u001b[1mMachine{TunedModel{Grid,Determin…} @ 2…65\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:69\n",
      "\u001b[33mSearching a 132-point grid for best model: 100%[==================================================] Time: 0:00:04\u001b[39m\n",
      "┌ Info: Training best model on all supplied data.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/tuning.jl:106\n"
     ]
    }
   ],
   "source": [
    "tuned_ensemble = machine(tuned_ensemble_model, X[train,:], y[train])\n",
    "fit!(tuned_ensemble);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the best model by looking at the machine's `report` field (every machine has one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol,Any} with 4 entries:\n",
       "  :measurements     => [0.0789623, 0.0932894, 0.0998844, 0.0998019, 0.1081, 0.1…\n",
       "  :models           => DeterministicEnsembleModel{Tuple{Array{Float64,2},Array{…\n",
       "  :best_model       => \u001b[0m\u001b[1mDeterministicEnsembleModel @ 8…06\u001b[22m\n",
       "  :best_measurement => 0.0750575"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_ensemble.report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model.bagging_fraction = 0.5454545454545454\n",
      "(best_model.atom).K = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = tuned_ensemble.report[:best_model]\n",
    "@show best_model.bagging_fraction\n",
    "@show best_model.atom.K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07611929792774337"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = predict(tuned_ensemble, X[test,:])\n",
    "rms(yhat, y[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLJ has a flexible interface for building networks from multiple machine learning elements, whose complexity extend beyond linear \"pipelines\", and with a minimal of added abstraction.\n",
    "\n",
    "In MLJ, a *learning network* is a graph whose nodes apply an operation, such as `predict` or `transform`, using a fixed machine (requiring training) - or which, alternatively, applies a regular (untrained) mathematical operation to its input(s). In practice, a learning network works with *fixed* sources for its training/evaluation data, but can be built and tested in stages. By contrast, an *exported learning network* is a learning network exported as a stand-alone, re-usable `Model` object, to which all the MLJ `Model`  meta-algorthims can be applied (ensembling, systematic tuning, etc). \n",
    "\n",
    "As we shall see, exporting a learning network as a reusable model, is very easy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple learning network\n",
    "\n",
    "![](wrapped_ridge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram above depicts a learning network which standardises the input data, `X`, learns an optimal Box-Cox transformation for the target, `y`, predicts new targets using ridge regression, and then inverse-transforms those predictions (for later comparison with the original test data). The machines are labelled yellow. \n",
    "\n",
    "To implement the network, we begin by loading all data needed for training and evaluation into *source nodes*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1mSource @ 6…01\u001b[22m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs = source(X)\n",
    "ys = source(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label nodes according to their outputs in the diagram. Notice that the nodes `z` and `yhat` use the same machine `box` for different operations. \n",
    "\n",
    "To construct the `W` node we first need to define the machine `stand` that it will use to transform inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1mNodalMachine @ 1…56\u001b[22m = machine(\u001b[0m\u001b[1mStandardizer @ 4…47\u001b[22m, \u001b[0m\u001b[1m1…70\u001b[22m)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stand_model = Standardizer()\n",
    "stand = machine(stand_model, Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `Xs` is a node, instead of concrete data, we can call `transform` on the machine without first training it, and the result is the new node `W`, instead of concrete transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1mNode @ 8…17\u001b[22m = transform(\u001b[0m\u001b[1m1…56\u001b[22m, \u001b[0m\u001b[1m1…70\u001b[22m)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = transform(stand, Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get actual transformed data we *call* the node appropriately, which will require we first train the node. Training a node, rather than a machine, triggers training of *all* necessary machines in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[0m\u001b[1mNodalMachine{Standardizer} @ 1…56\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:69\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>x1</th><th>x2</th><th>x3</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>2 rows × 3 columns</p><tr><th>1</th><td>-0.972987</td><td>0.500143</td><td>1.25454</td></tr><tr><th>2</th><td>0.689341</td><td>1.47493</td><td>0.865731</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& x1 & x2 & x3\\\\\n",
       "\t\\hline\n",
       "\t1 & -0.972987 & 0.500143 & 1.25454 \\\\\n",
       "\t2 & 0.689341 & 1.47493 & 0.865731 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "2×3 DataFrame\n",
       "│ Row │ x1        │ x2       │ x3       │\n",
       "│     │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mFloat64\u001b[39m  │ \u001b[90mFloat64\u001b[39m  │\n",
       "├─────┼───────────┼──────────┼──────────┤\n",
       "│ 1   │ -0.972987 │ 0.500143 │ 1.25454  │\n",
       "│ 2   │ 0.689341  │ 1.47493  │ 0.865731 │"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(W, rows=train)\n",
    "W()          # transform all data\n",
    "W(rows=test) # transform only test data\n",
    "W(X[3:4,:])  # transform any data, new or old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can think of `W` (and the other nodes we will define) as \"dynamic data\": `W` is *data*, in the sense that  it an be called (\"indexed\") on rows, but *dynamic*, in the sense the result depends on the outcome of training events. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other nodes of our network are defined similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1mNode @ 7…53\u001b[22m = inverse_transform(\u001b[0m\u001b[1m2…01\u001b[22m, predict(\u001b[0m\u001b[1m1…23\u001b[22m, transform(\u001b[0m\u001b[1m1…56\u001b[22m, \u001b[0m\u001b[1m1…70\u001b[22m)))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_model = UnivariateBoxCoxTransformer()  # for making data look normally-distributed\n",
    "box = machine(box_model, ys)\n",
    "z = transform(box, ys)\n",
    "\n",
    "ridge_model = RidgeRegressor(lambda=0.1)\n",
    "ridge =machine(ridge_model, W, z)\n",
    "zhat = predict(ridge, W)\n",
    "\n",
    "yhat = inverse_transform(box, zhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to train and evaluate the completed network. Notice that the standardizer, `stand`, is *not* retrained, as MLJ remembers that it was trained earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Not retraining \u001b[0m\u001b[1mNodalMachine{Standardizer} @ 1…56\u001b[22m. It is up-to-date.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:201\n",
      "┌ Info: Training \u001b[0m\u001b[1mNodalMachine{UnivariateBoxCoxTransfor…} @ 2…01\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:69\n",
      "┌ Info: Training \u001b[0m\u001b[1mNodalMachine{RidgeRegressor} @ 1…23\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04010491751037469"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(yhat, rows=train)\n",
    "rms(y[test], yhat(rows=test)) # evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.13061764410844875\n",
       " 0.19069150780607036"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat(X[3:4,:])  # predict on new or old data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change hypeparameters and retrain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Not retraining \u001b[0m\u001b[1mNodalMachine{UnivariateBoxCoxTransfor…} @ 2…01\u001b[22m. It is up-to-date.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:201\n",
      "┌ Info: Not retraining \u001b[0m\u001b[1mNodalMachine{Standardizer} @ 1…56\u001b[22m. It is up-to-date.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/networks.jl:201\n",
      "┌ Info: Updating \u001b[0m\u001b[1mNodalMachine{RidgeRegressor} @ 1…23\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:73\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04037074481178468"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model.lambda = 0.01\n",
    "fit!(yhat, rows=train) \n",
    "rms(y[test], yhat(rows=test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Notable feature.** The machine, `ridge::NodalMachine{RidgeRegressor}`, is retrained, because its underlying model has been mutated. However, since the outcome of this training has no effect on the training inputs of the machines `stand` and `box`, these transformers are left untouched. (During construction, each node and machine in a learning network determines and records all machines on which it depends.) This behaviour, which extends to exported learning networks, means we can tune our wrapped regressor without re-computing transformations each time the hyperparameter is changed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting a learning network as a composite model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export a learning network:\n",
    "- Define a new `mutable struct` model type.\n",
    "- Wrap the learning network code in a model `fit` method.\n",
    "\n",
    "All learning networks that make determinisic (or, probabilistic) predictions export as models of subtype `Deterministic{Node}` (respectively, `Probabilistic{Node}`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct WrappedRidge <: Deterministic{Node}\n",
    "    ridge_model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now satisfied that our wrapped Ridge Regression learning network works, we simply cut and paste its defining code into a `fit` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "function MLJ.fit(model::WrappedRidge, X, y)\n",
    "    Xs = source(X)\n",
    "    ys = source(y)\n",
    "\n",
    "    stand_model = Standardizer()\n",
    "    stand = machine(stand_model, Xs)\n",
    "    W = transform(stand, Xs)\n",
    "\n",
    "    box_model = UnivariateBoxCoxTransformer()  # for making data look normally-distributed\n",
    "    box = machine(box_model, ys)\n",
    "    z = transform(box, ys)\n",
    "\n",
    "    ridge_model = model.ridge_model ###\n",
    "    ridge =machine(ridge_model, W, z)\n",
    "    zhat = predict(ridge, W)\n",
    "\n",
    "    yhat = inverse_transform(box, zhat)\n",
    "    fit!(yhat, verbosity=0)\n",
    "    \n",
    "    return yhat\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line marked `###`, where the new exported model's hyperparameter `ridge_model` is spliced into the network, is the only modification.\n",
    "\n",
    "This completes the export process.\n",
    "\n",
    "> **What's going on here?** MLJ's machine interface is built atop a more primitive *[model](adding_new_models.md)* interface, implemented for each algorithm. Each supervised model type (eg, `RidgeRegressor`) requires model `fit` and `predict` methods, which are called by the corresponding machine `fit!` and `predict` methods. We don't need to define a  model `predict` method here because MLJ provides a fallback which simply calls the node returned by `fit` on the data supplied: `MLJ.predict(model::Supervised{Node}, yhat, Xnew) = yhat(Xnew)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now tune our wrapped ridge model on the Boston dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mWrappedRidge @ 1…01\u001b[22m: \n",
       "ridge_model             =>   \u001b[0m\u001b[1mRidgeRegressor @ 1…17\u001b[22m\n",
       "\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = X_and_y(load_boston())\n",
    "train, test = partition(eachindex(y), 0.7)\n",
    "wrapped_model = WrappedRidge(ridge_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params(:ridge_model => Params(:lambda => 0.01))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(wrapped_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params(:ridge_model => Params(:lambda => \u001b[0m\u001b[1mNumericRange @ 1…07\u001b[22m))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_ranges = Params(:ridge_model => Params(:lambda => range(ridge_model, :lambda, lower=0.1, upper=100.0, scale=:log10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# \u001b[0m\u001b[1mTunedModel @ 9…20\u001b[22m: \n",
       "model                   =>   \u001b[0m\u001b[1mWrappedRidge @ 1…01\u001b[22m\n",
       "tuning_strategy         =>   \u001b[0m\u001b[1mGrid @ 1…45\u001b[22m\n",
       "resampling_strategy     =>   \u001b[0m\u001b[1mHoldout @ 6…36\u001b[22m\n",
       "measure                 =>   rms (generic function with 5 methods)\n",
       "operation               =>   predict (generic function with 19 methods)\n",
       "param_ranges            =>   Params(:ridge_model => Params(:lambda => \u001b[0m\u001b[1mNumericRange @ 1…07\u001b[22m))\n",
       "report_measurements     =>   true\n",
       "\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_wrapped_model = TunedModel(model=wrapped_model, tuning_strategy=Grid(resolution=100), \n",
    "    param_ranges=param_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[0m\u001b[1mMachine{TunedModel{Grid,WrappedR…} @ 7…80\u001b[22m.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/machines.jl:69\n",
      "\u001b[33mSearching a 100-point grid for best model: 100%[==================================================] Time: 0:00:02\u001b[39m\n",
      "┌ Info: Training best model on all supplied data.\n",
      "└ @ MLJ /Users/anthony/Dropbox/Julia7/MLJ/src/tuning.jl:106\n"
     ]
    }
   ],
   "source": [
    "tuned_wrapped = machine(tuned_wrapped_model, X, y)\n",
    "fit!(tuned_wrapped, rows=train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((tuned_wrapped.report[:best_model]).ridge_model).lambda = 23.101297000831593\n",
      "tuned_wrapped.report[:best_measurement] = 2.331822749246328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.331822749246328"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show tuned_wrapped.report[:best_model].ridge_model.lambda\n",
    "@show tuned_wrapped.report[:best_measurement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
