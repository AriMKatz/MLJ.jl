# Adding new models to MLJ

This guide outlines the specification for the lowest level of the MLJ
application interface. It is is guide for those adding new models by:
(i) Writing glue code for lazily loaded external packages (the main
case); and (ii) Declaring built-in models in include files.

A checklist for adding models is given at the end, and a template for
adding supervised models from external packages is at
["src/interfaces/DecisionTree.jl"](../src/interfaces/DecisionTree.jl)


<!-- ### MLJ types -->

<!-- Every type introduced the core MLJ package should be a subtype of: -->

<!-- ``` -->
<!-- abstract type MLJType end -->
<!-- ``` -->

<!-- The Julia `show` method is informatively overloaded for this -->
<!-- type. Variable bindings declared with `@constant` "register" the -->
<!-- binding, which is reflected in the output of `show`. -->


## Models

A *model* is an object storing hyperparameters associated with some
machine learning algorithm, where "learning algorithm" is to be
broadly interpreted.  In MLJ, hyperparameters include configuration
parameters, like number of threads, which may not affect the final
learning outcome.  However, the logging level, `verbosity`, is
excluded.

The name of the Julia type associated with a model indicates the
associated algorithm (e.g., `DecisionTreeClassifier`). The ultimate
supertype of all models is:

````julia
abstract type Model <: MLJType end 
````

Models are divided into `Supervised` and `Unsupervised` subtypes. 

````julia
abstract type Supervised{R} <: Model end
abstract type Unsupervised <: Model end
````

Here the parameter `R` refers to the fit-result type (concrete if
possible); see below.

In addition to basic learning algorithms are "meta-algorithms" like
estimating performance by cross-validation, and hyperparameter tuning,
which have also hyperparameters (e.g., number of folds for
cross-validation) and so have associated models.

Associated with every concrete subtype of `Model` there must be a
`fit` method, which implements the associated algorithm to produce a
*fit-result* (see below). At least one method dispatched on model
instances and a fit-result (typically `predict` for supervised
algorithms) must be provided. Such methods, here called *operations*,
can have one of the following names: `predict`, `predict_proba`,
`transform`, `inverse_transform`, `se`, or `evaluate`.

The outcome of training a learning algorithm is here called a
*fit-result*. For a linear model, for example, this would be the
coefficients and intercept.  There is no abstract type for fit-results
because these types are generally declared in external
packages. However, in MLJ the abstract supervised model type is
parameterized by the fit-result type `R`, for efficient implementation
of large ensembles of models of uniform type.


## Package interfaces (glue code)

Note: Most of the following remarks also apply to built-in learning
algorithms (i.e., not defined in external packages) and presently
located in "src/builtins/". In particular "src/transforms.jl" will
contain a number of common preprocessing transformations. External
package interfaces go in "src/interfaces/".

Every package interface should live inside a submodule for namespace
hygiene (see the template at
"src/interfaces/DecisionTree.jl"). Ideally, package interfaces should
export no `struct` outside of the new model types they define, and
import only abstract types. All "structural" design should be
restricted to the MLJ core to prevent rewriting glue code when there
are design changes.

### New model type declarations

Here is an example of a concrete model type declaration:

````julia

R = Tuple{Matrix{Float64},Vector{Float64}}

mutable struct KNNRegressor{M,K} <: Regressor{R}
    K::Int          
    metric::M
    kernel::K
end

````

Models (which are mutable) should never have internally defined
constructors but should always be given an external lazy keyword
constructor of the same name that defines default values and checks
their validity, by calling an optional `clean!` method (see below).


### Supervised models

Below we describe the methods associated with some concrete type
`ConcreteModel{R} <: Supervised{R}`. These are what go into package
interfaces, together with model declarations.


#### Compulsory methods

````julia
fitresult, cache, report =  fit(model::ConcreteModel, verbosity::Int, rows, X, y)
````

Here `fitresult::R` is the fit-result in the sense above. Any
training-related statistics, such as internal estimates of the
generalization error, and feature rankings (controlled by model
hyperparameters) should be returned in the `report` object. This is
either a `Dict{Symbol,Any}` object, or `nothing` if there is nothing
to report. So for example, `fit` might declare
`report[:feature_importances]=...`.  Reports get merged with those
generated by previous calls to `fit` at higher levels of the MLJ
interface. The value of `cache` can be `nothing` unless one is also
defining an `update` method (see below). The Julia type of `cache` is
not presently restricted.

The types of the training data `X` and `y` should be whatever is
required by the package for the training algorithm and declared in the
`fit` type signature for safety.  It is understood that `fit` only
uses `rows` for training. Checks not specific to the package (e.g.,
dimension matching checks) should be left to higher levels of the
interface to avoid code duplication.

The method `fit` should initially call `clean!` on `model` and issue
the returned warning indicating the changes to `model`. The `clean!`
method has a trivial fall-back (which needs to be imported from MLJ)
but can be extended (see below, and the template). This is the only
time `fit` should alter hyperparameter values. If the package is able
to suggest better hyperparameters, as a byproduct of training, return
these in the report field.

The `verbosity` level (0 for silent) is for passing to the fit method
of the external package. The `fit` method should generally avoid doing
its own logging to avoid duplication at higher levels of the
interface.

````julia
yhat = predict(model::ConcreteModel, fitresult, Xnew)
````

Here `Xnew` is understood to be of the same type as `X` in the `fit`
method. (So to get a prediction on a single pattern, a user may need
to suitably wrap the pattern before passing to `predict` - as a
single-row `DataFrame`, for example - and suitably unwrap
`yhat`, which must have the same type as `y` in the `fit`
method.)

**Metadata** Methods encoding certain model type metadata must be
provided for the `Task` interface. For example, in the
`DecisionTreeClassifier`, metadata is declared as follows:

````julia
properties(::Type{DecisionTreeClassifier}) = [MultiClass(), Numeric()]
operations(::Type{DecisionTreeClassifier}) = [:predict]
type_of_X(::Type{DecisionTreeClassifier}) = Array{Float64,2}
type_of_y(::Type{DecisionTreeClassifier}) = Vector
````

For a list of properties run `subtypes(Property)`. For explanation of
a property, query the doc string, e.g., run `?MulitClass`.


#### Optional methods

**Binary classifiers.** A model with the `Classifier()` property can
implement a `predict_proba` method to predict probabilities instead of
labels, and will have the same type signature as `predict` for its
inputs. It should return a single `Float64` probability per input
pattern.

**Multilabel classifiers.** A model with the `MultiClass()` property
can also implement a `predict_proba` method, but its return value
`yhat` must be an `Array{Float64}` object of size `(nrows, k - 1)`
where `nrows` is the number of input patterns (the number of rows of
the input data `Xnew`) and `k` is the number of labels. In addition, a
method `labels` must be implemented such that `labels(fitresult)` is a
the vector of the unique labels, ordered such that the probabilities in
the `j`th column of `yhat` correspond to the `j`th label (`j < k`).

````julia
message::String = clean!(model::Supervised)
````

Checks and corrects for invalid fields (hyperparameters), returning a
warning `message`. Should only throw an exception as a last resort.

````julia
    fitresult, cache, report = 
	    update(model::ConcreteModel, verbosity, old_fitresult, old_cache, X, y) 
````

A package interface author may overload an `update` method to enable a
call to retrain a model at higher levels of the API (on the same
training data) to avoid repeating computations unnecessarily.
Composite models (subtypes of `Supervised{LearningNode}`) constitute
one use-case (component models are only retrained when new
hyperparameter values make this necessary) and in this case MLJ
provides a fallback. A second important use-case is iterative models,
where calls to increase the number of iterations only restarts the
iterative procedure if other hyperparameters have also changed. For an
example see `builtins/Ensembles.jl`.

In the event that the argument `fitresult` (returned by a preceding
call to `fit`) is not sufficient for performing an update, the author
can arrange for `fit` to output in its `cache` return value any
additional information required, as this is also passed as an argument
to the `update` method.


##  Checklist for new adding models 

At present the following checklist is just for supervised models in
lazily loaded external packages.

- Copy and edit file
["src/interfaces/DecisionTree.jl"](../src/interfaces/DecisionTree.jl)
which is annotated for use as a template. Give your new file a name
identical to the package name, including ".jl" extension, such as
"DecisionTree.jl". Put this file in "src/interfaces/".

- Register your package for lazy loading with MLJ by finding out the
UUID of the package and adding an appropriate line to the `__init__`
method at the end of "src/MLJ.jl". It will look something like this:

````julia
function __init__()
   @load_interface DecisionTree "7806a523-6efd-50cb-b5f6-3fa6f1930dbb" lazy=true
   @load_interface NewExternalPackage "893749-98374-9234-91324-1324-9134-98" lazy=true
end
````

With `lazy=true`, your glue code only gets loaded by the MLJ user
after they run `import NewExternalPackage`. For testing in your local
MLJ fork, you may want to set `lazy=false` but to use `Revise` you
will also need to move the `@load_interface` line out outside of the
`__init__` function. 

- Write self-contained test-code for the methods defined in your glue
code, in a file with an identical name, but placed in "test/" (as in
["test/DecisionTree.jl"](../test/DecisionTree.jl)). This
code should be wrapped in a module to prevent namespace conflicts with
other test code. For a module name, just prepend "Test", as in
"TestDecisionTree". See "test/DecisionTree.jl" for an example. 

- Add a line to ["test/runtests.jl"](../test/runtests.jl) to
`include` your test file, for the purpose of testing MLJ core and all
currently supported packages, including yours. You can Test your code
by running `test MLJ` from the Julia interactive package manager. You
will need to `Pkg.dev` your local MLJ fork first. To test your code in
isolation, locally edit "test/runtest.jl" appropriately.


